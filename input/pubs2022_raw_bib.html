<h1>pubs2022.bib</h1><a name="aziz2022planningoverviews"></a><pre>
@article{<a href="pubs2022_raw.html#aziz2022planningoverviews">aziz2022planningoverviews</a>,
  author = {Aziz, N and Stockman, T and Stewart, R},
  journal = {ACM Transactions on Accessible Computing},
  month = {Apr},
  publisher = {Association for Computing Machinery (ACM)},
  title = {Planning Your Journey in Audio: Design and Evaluation of Auditory Route Overviews},
  year = {2022},
  doi = {10.1145/3531529},
  issn = {1936-7236},
  day = {27},
  publicationstatus = {published}
}
</pre>

<a name="metatlaaudiohapticworkstations"></a><pre>
@article{<a href="pubs2022_raw.html#metatlaaudiohapticworkstations">metatlaaudiohapticworkstations</a>,
  author = {Metatla, O and Martin, F and Parkinson, A and Bryan-Kinns, N and Stockman, T and Tanaka, A},
  journal = {Journal on Multimodal User Interfaces},
  pages = {1--12},
  title = {Audio-haptic interfaces for digital audio workstations},
  url = {<a href="http://dx.doi.org/10.1007/s12193-016-0217-8">http://dx.doi.org/10.1007/s12193-016-0217-8</a>},
  year = {},
  doi = {10.1007/s12193-016-0217-8},
  issn = {1783-8738},
  publicationstatus = {published}
}
</pre>

<a name="preniqi2022morelyrics"></a><pre>
@misc{<a href="pubs2022_raw.html#preniqi2022morelyrics">preniqi2022morelyrics</a>,
  author = {Preniqi, V and Kalimeri, K and Saitis, C},
  month = {Sep},
  title = {"More Than Words": Linking Music Preferences and Moral Values Through  Lyrics},
  year = {2022},
  day = {2}
}
</pre>

<a name="hayes2022disembodiedsynthesis1"></a><pre>
@misc{<a href="pubs2022_raw.html#hayes2022disembodiedsynthesis1">hayes2022disembodiedsynthesis1</a>,
  author = {Hayes, B and Saitis, C and Fazekas, G},
  month = {Aug},
  title = {Disembodied Timbres: a Study on Semantically Prompted FM Synthesis},
  year = {2022},
  doi = {10.31234/osf.io/ksw5j},
  day = {5}
}
</pre>

<a name="hayes2022disembodiedsynthesis2"></a><pre>
@article{<a href="pubs2022_raw.html#hayes2022disembodiedsynthesis2">hayes2022disembodiedsynthesis2</a>,
  author = {Hayes, B and Saitis, C and Fazekas, G},
  journal = {Journal of the Audio Engineering Society},
  month = {May},
  number = {5},
  pages = {373--391},
  publisher = {Audio Engineering Society},
  title = {Disembodied Timbres: A Study on Semantically Prompted FM Synthesis},
  volume = {70},
  year = {2022},
  abstract = {Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalize to electronic sounds, nor is it obvious how these relate to the creation of such sounds. This work presents an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. A novel experimental paradigm, in which experienced sound designers responded to semantic prompts by programming a synthesizer, was applied, and semantic ratings on the sounds they created were provided. Exploratory factor analysis revealed a five-dimensional semantic space. The first two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. The results suggest that further inquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile and that this could benefit research into auditory perception and cognition and synthesis control and audio engineering.},
  doi = {10.17743/jaes.2022.0006},
  issn = {0004-7554},
  day = {11},
  publicationstatus = {published}
}
</pre>

<a name="delgado2022deepclassification"></a><pre>
@misc{<a href="pubs2022_raw.html#delgado2022deepclassification">delgado2022deepclassification</a>,
  author = {Delgado, A and Demirel, E and Subramanian, V and Saitis, C and Sandler, M},
  month = {Apr},
  title = {Deep Embeddings for Robust User-Based Amateur Vocal Percussion  Classification},
  year = {2022},
  doi = {10.48550/arxiv.2204.04646},
  day = {10}
}
</pre>

<a name="delgado2022deepvocalisation"></a><pre>
@misc{<a href="pubs2022_raw.html#delgado2022deepvocalisation">delgado2022deepvocalisation</a>,
  author = {Delgado, A and Saitis, C and Benetos, E and Sandler, M},
  month = {Apr},
  title = {Deep Conditional Representation Learning for Drum Sample Retrieval by  Vocalisation},
  year = {2022},
  doi = {10.48550/arxiv.2204.04651},
  keyword = {Clinical Research},
  day = {10}
}
</pre>

<a name="hayes2022disembodiedsynthesis3"></a><pre>
@article{<a href="pubs2022_raw.html#hayes2022disembodiedsynthesis3">hayes2022disembodiedsynthesis3</a>,
  author = {Hayes, B and Saitis, C and Fazekas, G},
  journal = {Journal of the Audio Engineering Society},
  month = {May},
  number = {5},
  pages = {373--391},
  publisher = {Audio Engineering Society},
  title = {Disembodied Timbres: A Study on Semantically Prompted FM Synthesis},
  volume = {70},
  year = {2022},
  abstract = {Disembodied electronic sounds constitute a large part of the modern auditory lexicon, but research into timbre perception has focused mostly on the tones of conventional acoustic musical instruments. It is unclear whether insights from these studies generalize to electronic sounds, nor is it obvious how these relate to the creation of such sounds. This work presents an experiment on the semantic associations of sounds produced by FM synthesis with the aim of identifying whether existing models of timbre semantics are appropriate for such sounds. A novel experimental paradigm, in which experienced sound designers responded to semantic prompts by programming a synthesizer, was applied, and semantic ratings on the sounds they created were provided. Exploratory factor analysis revealed a five-dimensional semantic space. The first two factors mapped well to the concepts of luminance, texture, and mass. The remaining three factors did not have clear parallels, but correlation analysis with acoustic descriptors suggested an acoustical relationship to luminance and texture. The results suggest that further inquiry into the timbres of disembodied electronic sounds, their synthesis, and their semantic associations would be worthwhile and that this could benefit research into auditory perception and cognition and synthesis control and audio engineering.},
  doi = {10.17743/jaes.2022.0006},
  issn = {0004-7554},
  day = {11},
  publicationstatus = {published}
}
</pre>

<a name="turchet2022theontology"></a><pre>
@article{<a href="pubs2022_raw.html#turchet2022theontology">turchet2022theontology</a>,
  author = {Turchet, L and Bouquet, P and Molinari, A and Fazekas, G},
  journal = {Journal of Web Semantics},
  month = {Apr},
  title = {The Smart Musical Instruments Ontology},
  volume = {72},
  year = {2022},
  abstract = {The Smart Musical Instruments (SMIs) are an emerging category of musical instruments that belongs to the wider class of Musical Things within the Internet of Musical Things paradigm. SMIs encompass sensors, actuators, embedded intelligence, and wireless connectivity to local networks and to the Internet. Interoperability represents a key issue within this domain, where heterogeneous SMIs are envisioned to exchange information between each other and a plethora of Musical Things. This paper proposes an ontology for the representation of the knowledge related to SMIs, with the aim of facilitating interoperability between SMIs as well as with other Musical Things interacting with them. There was no previous comprehensive data model for the SMIs domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators, the Audio Effects Ontology dealing with the description of digital audio effects, and the IoMusT Ontology for the representation Musical Things and IoMusT ecosystems. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving SMIs stakeholders, such as performers and studio producers. The SMI Ontology can be accessed at: https://w3id.org/smi#.},
  doi = {10.1016/j.websem.2021.100687},
  issn = {1570-8268},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="thalmann2016themetadata"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#thalmann2016themetadata">thalmann2016themetadata</a>,
  author = {Thalmann, F and Perez Carillo, A and Fazekas, G and Sandler, M},
  conference = {Proc. Web Audio Conference WAC-2016, April 4–6, Atlanta, USA},
  note = {date-added: 2017-12-22 19:47:03 +0000
date-modified: 2017-12-22 19:52:36 +0000
keywords: ontolgies, mobile music player, mobile applications, mobile audio ontology
local-url: thalmann2016wac.pdf
publisher-url: http://hdl.handle.net/1853/54596
bdsk-url-1: https://smartech.gatech.edu/bitstream/handle/1853/54596/WAC2016-71.pdf},
  title = {The Semantic Music Player: A Smart Mobile Player Based on Ontological Structures and Analytical Feature Metadata},
  url = {https://smartech.gatech.edu/bitstream/handle/1853/54596/WAC2016-71.pdf},
  year = {2016},
  abstract = {The Semantic Music Player is a cross-platform web and mobile app built with Ionic and the Web Audio API that explores new ways of playing back music on mobile devices, particularly indeterministic, context-dependent, and interactive ways. It is based on Dynamic Music Objects, a format that represents musical content and structure in an abstract way and makes it modifiable within definable constraints. For each Dynamic Music Object, the Semantic Music Player generates a custom graphical interface and enables appropriate user interface controls and mobile sensors based on its requirements. When the object is played back, the player takes spontaneous decisions based on the given structural information and the analytical data and reacts to sensor and user interface inputs. In this paper, we introduce the player and its underlying concepts and give some examples of the potentially infinite amount of use cases and musical results.}
}
</pre>

<a name="thalmann2016thedevices"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#thalmann2016thedevices">thalmann2016thedevices</a>,
  author = {Thalmann, F and Carrillo, G and Wiggins, GA and Sandler, M},
  conference = {IEEE International Conference on Semantic Computing (ICSC), Feb. 4-6, Laguna Hills, CA, USA},
  note = {date-added: 2017-12-22 13:07:02 +0000
date-modified: 2017-12-22 19:55:24 +0000
keywords: music ontologies, artificial intelligence, user interfaces, dynamic music objects, mobile audio ontology, mobile sensor data, music consumption experiences, semantic audio framework, user interface controls, Data mining, Feature extraction
bdsk-url-1: https://dx.doi.org/10.1109/ICSC.2016.61},
  pages = {47--54},
  title = {The Mobile Audio Ontology: Experiencing Dynamic Music Objects on Mobile Devices},
  url = {<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=7439304">http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=7439304</a>},
  year = {2016},
  abstract = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few (<;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  doi = {10.1109/ICSC.2016.61}
}
</pre>

<a name="allik2016mymoodplayapp"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#allik2016mymoodplayapp">allik2016mymoodplayapp</a>,
  author = {Allik, A and Fazekas, G and Barthet, M and Sandler, M},
  conference = {Proc. of the 2nd Web Audio Conference (WAC), April 4–6, Atlanta, Georgia, USA.},
  note = {date-added: 2017-12-29 19:26:47 +0000
date-modified: 2017-12-29 19:38:36 +0000
keywords: Semantic Audio, mood-based interaction, Ontology-based systems
local-url: http://www.semanticaudio.net/files/papers/allik2016wac.pdf
bdsk-url-1: http://hdl.handle.net/1853/54589},
  title = {myMoodplay: An interactive mood-based music discovery app},
  url = {<a href="http://hdl.handle.net/1853/54589">http://hdl.handle.net/1853/54589</a>},
  year = {2016},
  abstract = {myMoodplay is a web app that allows users to interactively discover music by selecting desired emotions. The application uses the Web Audio API, JavaScript animation for visualisation, linked data formats and affective computing technologies. We explore how artificial intelligence, the Semantic Web and audio synthesis can be combined to provide new personalised online musical experiences. Users can choose degrees of energy and pleasantness to shape the desired musical mood trajectory. Semantic Web technologies have been embedded in the system to query mood coordinates from a triple store using a SPARQL endpoint and to connect to external linked data sources for metadata.}
}
</pre>

<a name="frachi2022designbiosignals"></a><pre>
@incollection{<a href="pubs2022_raw.html#frachi2022designbiosignals">frachi2022designbiosignals</a>,
  author = {Frachi, Y and Takahashi, T and Wang, F and Barthet, M},
  conference = {},
  month = {Jan},
  pages = {160--179},
  title = {Design of Emotion-Driven Game Interaction Using Biosignals},
  volume = {13334 LNCS},
  year = {2022},
  abstract = {Video games can evoke a wide range of emotions in players through multiple modalities. However, on a broader scale, human emotions are probably an important missing part of the current generation of Human Computer Interaction (HCI). The main goal of this project is to start investigating how to design video games where the game mechanics and interactions are based on the player’s emotions. We designed a two-dimensional (2D) storytelling game prototype with Unity. Game designers and creators manage the user’s experience and emotions along the play through visual effects, sound effects, controls and narration. In particular for this project, we have chosen to create emotionally-driven interactions for two specific aspects: sound (audio effects, music), and narration (storytelling). Our prototype makes use of the Ovomind smart band and biosignals analysis technology developed by the first author. By wearing the smart band, human body physiological information are extracted and classified using signal processing method into groups of emotions mapped to the arousal \& valence (AV) plane. The 2D AV emotion representation is directly used as an interactive input into the game interaction system. Regarding music, we propose a system that automatically arranges background music by inputting emotions analysed by the smart band into an AI model. We evaluated the results using video recordings of the experience and collected feedback from a total of 30 participants. The results show that participants are favorable to narrative and music game adaptations based on real-time player emotion analysis. Some issues were also highlighted e.g. around the coherence of game progression. Participants also felt that the background music arrangements matched the player’s emotions well. Further experiments are required and planned to assess whether the prospects expressed by participants match their personal experience when playing the emotion-driven game.},
  doi = {10.1007/978-3-031-05637-6_10},
  isbn = {9783031056369},
  issn = {0302-9743},
  eissn = {1611-3349},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="oloweresiduumgeneration"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#oloweresiduumgeneration">oloweresiduumgeneration</a>,
  author = {OLOWE, I and MORO, G and BARTHET, M},
  conference = {},
  organization = {Brisbane, Australia},
  title = {residUUm: user mapping and performance strategies for multilayered live audiovisual generation},
  year = {},
  abstract = {We propose residUUm, an audiovisual performance tool that uses sonification to orchestrate a particle system of shapes, as an attempt to build an audiovisual user interface in which all the actions of a performer on a laptop are in- tended to be explicitly interpreted by the audience. We pro- pose two approaches to performing with residUUm and dis- cuss the methods utilized to fulfill the promise of audience- visible interaction: mapping and performance strategies ap- plied to express audiovisual interactions with multilayered sound-image relationships. The system received positive feedback from 34 audience participants on aspects such as aesthetics and audiovisual integration, and we identified fur- ther design challenges around performance clarity and strat- egy. We discuss residUUm’s development objectives, modes of interaction and the impact of an audience-visible inter- face on the performer and observer.},
  keyword = {sonification},
  keyword = {audiovisuals},
  keyword = {gui},
  booktitle = {International Conference on New Interfaces for Musical Expression (NIME)},
  publicationstatus = {published}
}
</pre>

<a name="barthetimprovingshaping"></a><pre>
@incollection{<a href="pubs2022_raw.html#barthetimprovingshaping">barthetimprovingshaping</a>,
  author = {BARTHET, M and Kronland-Martinet, R and Ystad, S},
  conference = {Sense of Sounds},
  pages = {313--336},
  publisher = {Springer},
  series = {Lecture Notes In Computer Science},
  title = {Improving Musical Expressiveness by Time-Varying Brightness Shaping},
  volume = {4969},
  year = {},
  abstract = {While listeners' emotional response to music is the subject of numerous studies, less attention is paid to the dynamic emotion variations due to the interaction between artists and audiences in live improvised music performances. By opening a direct communication channel from audience members to performers, the Mood Conductor system provides an experimental framework to study this phenomenon. Mood Conductor facilitates interactive performances and thus also has an inherent entertainment value. The framework allows audience members to send emotional directions using their mobile devices in order to ``conduct'' improvised performances. Emotion coordinates indicted by the audience in the arousal-valence space are aggregated and clustered to create a video projection. This is used by the musicians as guidance, and provides visual feedback to the audience. Three different systems were developed and tested within our framework so far. These systems were trialled in several public performances with different ensembles. Qualitative and quantitative evaluations demonstrated that musicians and audiences were highly engaged with the system, and raised new insights enabling future improvements of the framework.},
  doi = {10.1007/978-3-540-85035-9\_22},
  isbn = {978-3-540-85035-9}
}
</pre>

<a name="olowefeaturuxvisualization"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#olowefeaturuxvisualization">olowefeaturuxvisualization</a>,
  author = {OLOWE, I and BARTHET, M and GRIERSON, M and BRYAN-KINNS, N},
  conference = {},
  organization = {Cambridge},
  title = {FEATUR.UX: An approach to leveraging multitrack information for artistic music visualization},
  year = {},
  abstract = {FEATUR.UX (Feature - ous) is an audio visualisation tool, currently in the process of development, which proposes to introduce a new approach to sound visualisation using pre-mixed, independent multitracks and audio feature ex- traction. Sound visualisation is usually performed using a mixed mono or stereo track of audio. Audio feature ex- traction is commonly used in the field of music information retrieval to create search and recommendation systems for large music databases rather than generating live visual- isations. Visualizing multitrack audio circumvents prob- lems related to the source separation of mixed audio sig- nals and presents an opportunity to examine interdepen- dent relationships within and between separate streams of music. This novel approach to sound visualisation aims to provide an enhanced listening experience in a use case that employs non-tonal, non-notated forms of electronic music. Findings from prior research studies focused on live per- formance and preliminary quantitative results from a user survey have provided the basis from which to develop a prototype for an iterative design study that examines the impact of using multitrack audio and audio feature extrac- tion within sound visualisation practice.},
  startyear = {2016},
  startmonth = {May},
  startday = {27},
  finishyear = {2016},
  finishmonth = {May},
  finishday = {29},
  keyword = {audio features},
  keyword = {sound visualization},
  keyword = {gui},
  keyword = {multitrack audio},
  booktitle = {International Conference on Technologies for Music Notation and Representation (TENOR)},
  publicationstatus = {published}
}
</pre>

<a name="bryankinns2021exploringmusic"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#bryankinns2021exploringmusic">bryankinns2021exploringmusic</a>,
  author = {Bryan-Kinns, N and Banar, B and Ford, C and Reed, C and Zhang, Y and Colton, S and Armitage, J},
  conference = {},
  month = {Dec},
  title = {Exploring XAI for the Arts: Explaining Latent Space in Generative Music},
  year = {2021},
  booktitle = {NeurIPS 2021 - eXplainable AI Approaches for Debugging and Diagnosis Workshop},
  day = {14},
  publicationstatus = {published}
}
</pre>

<a name="wu2022exploringinterfaces"></a><pre>
@article{<a href="pubs2022_raw.html#wu2022exploringinterfaces">wu2022exploringinterfaces</a>,
  author = {Wu, Y and Bryan-Kinns, N and Zhi, J},
  journal = {Journal on Multimodal User Interfaces},
  month = {Sep},
  number = {3},
  pages = {343--356},
  title = {Exploring visual stimuli as a support for novices’ creative engagement with digital musical interfaces},
  volume = {16},
  year = {2022},
  abstract = {Visual materials are a widely used tool for stimulating creativity. This paper explores the potential for visual stimuli to support novices’ creative engagement with multimodal digital musical interfaces. An empirical study of 24 participants was conducted to compare the effect of abstract and literal forms of graphical scores on novices’ creative engagement, and whether being informed or uninformed about meanings of symbols in the score had any impact on creative engagement. The results suggest that abstract visual stimuli can provide an effective scaffold for creative engagement when participants are not informed about their design. It was found that providing information about visual stimuli has both advantages and disadvantages, depending largely on the stimuli’s visual style. Being informed about the meaning of a literal visual stimuli helped participants in making interpretations and gaining inspiration, whereas having information about abstract stimuli led to frustration. Qualitative data indicates that both forms of visual stimuli support creative engagement but at different stages of a creative process, and a descriptive model is presented to explain this. The findings highlight the benefits of visual stimuli in supporting creative engagement in the process of music making – a multimodal interaction domain typically involving few or no visual activities.},
  doi = {10.1007/s12193-022-00393-3},
  issn = {1783-7677},
  eissn = {1783-8738},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="robson2022beingsensors"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#robson2022beingsensors">robson2022beingsensors</a>,
  author = {Robson, N and McPherson, A and Bryan-Kinns, N},
  conference = {},
  month = {Jun},
  title = {Being With The Waves: An Ultrasonic Art Installation Enabling Rich Interaction Without Sensors},
  year = {2022},
  doi = {10.21428/92fbeb44.376bc758},
  booktitle = {NIME 2022},
  day = {28}
}
</pre>

<a name="ford2022identifyinghome"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#ford2022identifyinghome">ford2022identifyinghome</a>,
  address = {New York, NY, USA},
  author = {Ford, C and Bryan-Kinns, N},
  conference = {Creativity and Cognition},
  month = {Jun},
  organization = {Venice, Italy},
  pages = {443--456},
  publisher = {ACM Digital Library},
  title = {Identifying Engagement in Children's Interaction whilst Composing Digital Music at Home},
  url = {https://dl.acm.org/doi/10.1145/3527927.3532794},
  year = {2022},
  abstract = {Identifying points of engagement from a person’s interaction with computers could be used to assess their experience and to adapt user interfaces in real-time. However, it is difficult to identify points of engagement unobtrusively; HCI studies typically use retrospective protocols or rely on cumbersome sensors for real-time analysis. We present a case study on how children compose digital music at home in which we remotely identify points of engagement from patterns of interaction with a musical interface. A mixed-methods approach is contributed in which video recordings of children’s interactions whilst composing are labelled for engagement and linked to i) interaction logs from the interface to identify indicators of engagement in interaction, and ii) interview data gathered using a remote video-cued recall technique to understand the experiential qualities of engaging interactions directly from users. We conclude by speculating on how the suggested indicators of engagement inform the design of adaptive music systems.},
  doi = {10.1145/3527927.3532794},
  startyear = {2022},
  startmonth = {Jun},
  startday = {20},
  finishyear = {2022},
  finishmonth = {Jun},
  finishday = {23},
  isbn = {9781450393270},
  keyword = {adaptive systems},
  keyword = {children},
  keyword = {composition},
  keyword = {creativity},
  keyword = {creativity support tools},
  keyword = {engagement},
  keyword = {flow},
  keyword = {music},
  keyword = {novice},
  keyword = {online},
  keyword = {remote},
  booktitle = {ACM Conference on Creativity \& Cognition},
  day = {20},
  publicationstatus = {online-published}
}
</pre>

<a name="zheng2022squeezeprototypes"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#zheng2022squeezeprototypes">zheng2022squeezeprototypes</a>,
  author = {Zheng, J and Bryan-Kinns, N},
  conference = {},
  month = {Jun},
  title = {Squeeze, Twist, Stretch: Exploring Deformable Digital Musical Interfaces Design Through Non-Functional Prototypes},
  year = {2022},
  doi = {10.21428/92fbeb44.41da9da5},
  booktitle = {NIME 2022},
  day = {16},
  publicationstatus = {published}
}
</pre>

<a name="zheng2022materialdesign"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#zheng2022materialdesign">zheng2022materialdesign</a>,
  author = {Zheng, J and Bryan-Kinns, N and McPherson, AP},
  conference = {},
  month = {Jun},
  pages = {976--986},
  title = {Material Matters: Exploring Materiality in Digital Musical Instruments Design},
  year = {2022},
  doi = {10.1145/3532106.3533523},
  booktitle = {Designing Interactive Systems Conference},
  day = {13},
  publicationstatus = {published}
}
</pre>

<a name="zhang2022integratingdisciplines"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#zhang2022integratingdisciplines">zhang2022integratingdisciplines</a>,
  author = {Zhang, M and Stewart, R and Bryan-Kinns, N},
  conference = {DIS 2022 - Proceedings of the 2022 ACM Designing Interactive Systems Conference: Digital Wellbeing},
  month = {Jun},
  pages = {1277--1287},
  title = {Integrating Interactive Technology Concepts With Material Expertise in Textile Design Disciplines},
  year = {2022},
  abstract = {Textile and fashion designers are increasingly interested in integrating interactive technologies into their practice. However, traditional design education typically lacks support for them to develop technical digital and electronics skills alongside their expertise in materials. Reflecting on outputs from an e-textile design workshop and 8-week design projects with four textile design students using an e-textile toolkit, and follow-up data collection with the students one year after the projects, we argue that starting technical explorations with raw materials results in a better understanding and more flexible use of technical knowledge. We also argue that this newly acquired knowledge is then more fully integrated with their pre-existing material knowledge as it is applied to physical interface design. The results contribute to the development of tools and approaches in supporting designers with material expertise to learn tangible interaction design skills.},
  doi = {10.1145/3532106.3533535},
  isbn = {9781450393584},
  day = {13},
  publicationstatus = {published}
}
</pre>

<a name="bryankinns2022qi2heepistemology"></a><pre>
@article{<a href="pubs2022_raw.html#bryankinns2022qi2heepistemology">bryankinns2022qi2heepistemology</a>,
  author = {Bryan-Kinns, N and Wang, W and Ji, T},
  journal = {International Journal of Human Computer Studies},
  month = {Apr},
  title = {Qi2He: A co-design framework inspired by eastern epistemology},
  volume = {160},
  year = {2022},
  abstract = {The rapid development of rural societies mixed with the infrastructural transformation of emerging economies bring both challenges and opportunities to Human-Computer Interaction (HCI) design as illustrated through the emergence of the field of HCI for Development (HCI4D). A key challenge for HCI4D is how local knowledge, expertise, and culture can be constructively combined with global trends in digital innovation and socioeconomic development. Co-design and participatory design practices in HCI offer opportunities to engage diverse communities in design activities which embrace both transition and tradition in constructive ways. We present our co-design framework, Qi2He, which supports designers and local communities engaging in co-design activities. Qi2He is inspired by traditional Chinese epistemology and contributes (i) methods to support cross-cultural co-design engagement, and (ii) post-hoc critique of co-design participation. We illustrate the use of Qi2He through three case studies of HCI design over four years in rural China where local culture and traditions are in a state of flux from waves of migration to cities whilst also being an integral part of the broader national and global transformation. The first case study examines how local rural knowledge can be shared and acquired to create a design system for ethnic brocade production. The second case study explores how the creation of an interactive drama can be used as a driver for rural community engagement. The third case study focusses on the iterative design of cross-cultural interactive product innovation. We conclude by reflecting on lessons we learnt when structuring and restructuring our co-design process and offer suggestions for how our Qi2He framework could be used by others and in different cultural settings.},
  doi = {10.1016/j.ijhcs.2022.102773},
  issn = {1071-5819},
  eissn = {1095-9300},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="fordspeculatingai"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#fordspeculatingai">fordspeculatingai</a>,
  author = {Ford, C and Bryan-Kinns, N},
  conference = {},
  title = {Speculating on Reflection and People’s Music Co-Creation with AI},
  year = {},
  startyear = {2022},
  startmonth = {May},
  startday = {10},
  finishyear = {2022},
  finishmonth = {May},
  finishday = {10},
  booktitle = {Generative AI and HCI Workshop at CHI 2022},
  publicationstatus = {accepted}
}
</pre>

<a name="robson2022onpractitioners"></a><pre>
@article{<a href="pubs2022_raw.html#robson2022onpractitioners">robson2022onpractitioners</a>,
  author = {Robson, N and Bryan-Kinns, N and Mcpherson, A},
  journal = {Organised Sound: an international journal of music and technology},
  month = {Feb},
  number = {1},
  publisher = {Cambridge University Press (CUP)},
  title = {On mediating space, sound and experience: interviews with situated sound art practitioners},
  volume = {28},
  year = {2022},
  abstract = {This article reports on an interview-based study with ten sound artists and composers, all engaged in situated sonic practices. We propose that these artists engage the ear and shape possible interactions with the artwork by altering the relationship between sound, the space in which it is heard and the people who hear it. Our interviews probe the creative process and explore how a sound artist’s methods and tools might influence the reception of their work. A thematic analysis of interview transcriptions leads us to characterise artist processes as mediatory, in the sense that they act in-between site and audience experience and are guided by the nonhuman agencies of settings and material things. We propose that artists transfer their own situated and embodied listening to that of the audience and develop sonic and staging devices to direct perceptual activity and listening attention. Our findings also highlight a number of engagement challenges, in particular the difficulty artists face in understanding their audience’s experience and the specificity of an artwork’s effect to not just its location, but to the disposition, abilities and prior experiences of listeners.},
  doi = {10.1017/S1355771822000103},
  issn = {1355-7718},
  day = {9},
  publicationstatus = {published}
}
</pre>

<a name="zhang2022qiaolevr"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#zhang2022qiaolevr">zhang2022qiaolevr</a>,
  author = {Zhang, J and Bryan-Kinns, N},
  conference = {Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022},
  month = {Jan},
  pages = {357--362},
  title = {QiaoLe: Accessing Traditional Chinese Musical Instruments in VR},
  year = {2022},
  abstract = {Virtual Reality (VR) offers the potential for more engaging access to Intangible Cultural Heritage. We present the design of a VR system (QiaoLe) in which people can access and learn about traditional Chinese musical instruments. We undertook a user study of QiaoLe (24 participants) comparing three interaction modes. Results suggest that embodied interaction and gamification improved users' expe-rience, presence, and enjoyment in QiaoLe, but gamification may distract from rote learning.},
  doi = {10.1109/VRW55335.2022.00080},
  isbn = {9781665484022},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="soave2022designingreality"></a><pre>
@incollection{<a href="pubs2022_raw.html#soave2022designingreality">soave2022designingreality</a>,
  author = {Soave, F and Bryan-Kinns, N and Farkhatdinov, I},
  conference = {},
  month = {Jan},
  pages = {92--101},
  title = {Designing Audio Feedback to Enhance Motion Perception in Virtual Reality},
  volume = {13417 LNCS},
  year = {2022},
  abstract = {We present our study on the design and evaluation of sound samples for motion perception in a Virtual Reality (VR) application. In previous study we found our sound samples to be incoherent with the VR visual channel. In current research we designed four new samples and tested them adapting standard subjective evaluation protocols to our needs. Twenty participants participated to the study and rated each animation in Realism, Matching and Plausibility. Significant differences were found among the sounds and discussion rose on the need for realism in VR applications as well as users’ expectation and how it could influence their experience.},
  doi = {10.1007/978-3-031-15019-7_9},
  isbn = {9783031150180},
  issn = {0302-9743},
  eissn = {1611-3349},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="lepri2022uselesspractice"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#lepri2022uselesspractice">lepri2022uselesspractice</a>,
  author = {Lepri, G and Mcpherson, A and Bowers, J},
  conference = {},
  month = {Jul},
  title = {Useless, not Worthless: Absurd Making as Critical Practice},
  year = {2022},
  abstract = {We report on the outcomes of a hackathon organised aroundthe themes of absurd musical interfaces, questionable sonicinteractions and unworkable music designs. At the core of theproject is the intention to explore absurd making as a way tosupport critical and disruptive design practices. We reflect onhow surreal, nonsensical and fragile artefacts can be helpfulto stretch and critique conventional ideas of what is useful andappropriate in technology research and development.  Afterintroducing both concepts and methods that shaped the eventwe present a selection of useless interfaces designed by thehackathon’s attendees. These musical artefacts, and the con-siderations around them, are then discussed as a viable meansfor communicating both design concerns and future visions.We also consider two features identified as playing a crucialrole within the event: the discovery of contradictions and theimportance of context-based ingredients.},
  doi = {10.1145/10.1145/3357236.3395547},
  booktitle = {ACM conference on Designing Interactive Systems},
  day = {1},
  publicationstatus = {accepted}
}
</pre>

<a name="mice2022theperformances"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#mice2022theperformances">mice2022theperformances</a>,
  address = {Auckland, New Zealand},
  author = {Mice, L and Mcpherson, A},
  conference = {},
  month = {Jun},
  organization = {Auckland, New Zealand},
  publisher = {International Conference on New Interfaces for Musical Expression},
  title = {The M in NIME: Motivic analysis and the case for a musicology of NIME performances},
  url = {https://www.nime.org/},
  year = {2022},
  abstract = {While the value of new digital musical instruments lies to a large extent in their music making capacity, analyses of new instruments in the research literature often focus on
analyses of gesture or performer experience rather than the content of the music made with the instrument. In this paper we present a motivic analysis of music made with
new instruments. In the context of music, a motive is a small, analysable musical fragment or phrase that is important in or characteristic of a composition. We outline
our method for identifying and analysing motives in music made with new instruments, and display its use in a case study in which 10 musicians created performances with a
new large-scale digital musical instrument that we designed. This research illustrates the value of a musicological approach to NIME research, suggesting the need for a broader conversation about a musicology of NIME performances, as distinct from its instruments.},
  startyear = {2022},
  startmonth = {Jun},
  startday = {28},
  finishyear = {2022},
  finishmonth = {Jul},
  finishday = {1},
  keyword = {Motivic analysis},
  keyword = {digital musical instrument design},
  keyword = {musical interaction},
  keyword = {large DMI},
  keyword = {musicology of NIME performance},
  keyword = {music theory},
  keyword = {DMI evaluation},
  booktitle = {International Conference on New Interfaces for Musical Expression},
  day = {28},
  publicationstatus = {accepted}
}
</pre>

<a name="lepri2022thespeculation"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#lepri2022thespeculation">lepri2022thespeculation</a>,
  author = {Lepri, G and Bowers, J and Topley, S and Stapleton, P and Bennett, P and Andersen, K and McPherson, A},
  conference = {},
  month = {Jun},
  title = {The 10,000 Instruments Workshop - (Im)practical Research for Critical Speculation},
  year = {2022},
  doi = {10.21428/92fbeb44.9e7c9ba3},
  booktitle = {NIME 2022},
  day = {28}
}
</pre>

<a name="guidi2022quantitativeinstruments"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#guidi2022quantitativeinstruments">guidi2022quantitativeinstruments</a>,
  author = {Guidi, A and McPherson, A},
  conference = {},
  month = {Jun},
  title = {Quantitative evaluation of aspects of embodiment in new digital musical instruments},
  year = {2022},
  doi = {10.21428/92fbeb44.79d0b38f},
  booktitle = {NIME 2022},
  day = {28}
}
</pre>

<a name="reed2022exploringmicrophenomenology"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#reed2022exploringmicrophenomenology">reed2022exploringmicrophenomenology</a>,
  author = {Reed, CN and Nordmoen, C and Martelloni, A and Lepri, G and Robson, N and Zayas-Garin, E and Cotton, K and Mice, L and McPherson, A},
  conference = {},
  month = {Jun},
  title = {Exploring Experiences with New Musical Instruments through Micro-phenomenology},
  year = {2022},
  doi = {10.21428/92fbeb44.b304e4b1},
  keyword = {Clinical Research},
  booktitle = {NIME 2022},
  day = {28}
}
</pre>

<a name="pelinski2022embeddedopportunities"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#pelinski2022embeddedopportunities">pelinski2022embeddedopportunities</a>,
  author = {Pelinski, T and Shepardson, V and Symons, S and Caspe, FS and Benito Temprano, AL and Armitage, J and Kiefer, C and Fiebrink, R and Magnusson, T and McPherson, A},
  conference = {},
  month = {Jun},
  title = {Embedded AI for NIME: Challenges and Opportunities},
  year = {2022},
  doi = {10.21428/92fbeb44.76beab02},
  booktitle = {NIME 2022},
  day = {28}
}
</pre>

<a name="zayasgarin2022dialogicexperience"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#zayasgarin2022dialogicexperience">zayasgarin2022dialogicexperience</a>,
  author = {Zayas-Garin, E and McPherson, A},
  conference = {},
  month = {Jun},
  title = {Dialogic Design of Accessible Digital Musical Instruments: Investigating Performer Experience},
  year = {2022},
  doi = {10.21428/92fbeb44.2b8ce9a4},
  booktitle = {NIME 2022},
  day = {28}
}
</pre>

<a name="nordmoen2022makingsystem"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#nordmoen2022makingsystem">nordmoen2022makingsystem</a>,
  author = {Nordmoen, C and McPherson, AP},
  conference = {DIS 2022 - Proceedings of the 2022 ACM Designing Interactive Systems Conference: Digital Wellbeing},
  month = {Jun},
  pages = {415--423},
  title = {Making space for material entanglements: A diffractive analysis of woodwork and the practice of making an interactive system},
  year = {2022},
  abstract = {A shift in perspective is underway in design research and human-computer interaction (HCI) from humans as the centre of attention to considering complex assemblages of human and non-human stakeholders. While this shift is often approached from a broad ecological level, there is opportunity for a more local shift in understanding our day to day meeting with the material world. Drawing on the posthuman theories of Karen Barad, we explore the creation of a digital interactive system as a material-discursive practice in which matter and culture are inseparably entangled. We seek a fresh look at the process rather than the outcome of interactive system design through a diffractive reading of four traditional woodworking practices and an auto-ethnographic account of the development of a digital sensor and actuator apparatus as a way to find alternative ways of attending to materials in HCI.},
  doi = {10.1145/3532106.3533572},
  isbn = {9781450393584},
  day = {13},
  publicationstatus = {published}
}
</pre>

<a name="mice2022superdesign"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#mice2022superdesign">mice2022superdesign</a>,
  author = {Mice, L and McPherson, AP},
  conference = {Conference on Human Factors in Computing Systems - Proceedings},
  month = {Apr},
  title = {Super Size Me: Interface Size, Identity and Embodiment in Digital Musical Instrument Design},
  year = {2022},
  abstract = {Digital interfaces are shrinking, driven by pressures of mass production and consumer culture, and often accompanied by a discourse of control, precision or convenience. Meanwhile, human bodies remain the same size, and the changing size of interfaces has implications for the formation of user identities. Drawing on embodied cognition, effort and entanglement theories of HCI, we explored the impact of interface size on the co-constitution of humans and technology. We designed an oversized digital musical instrument and invited musicians to use the instrument to create original performances. We found that both the performances and the musicians' self-perception were influenced by the large size of the instrument, shining new light on the ways in which designing technology is designing humans and in turn culture.},
  doi = {10.1145/3491102.3517626},
  isbn = {9781450391573},
  day = {29},
  publicationstatus = {published}
}
</pre>

<a name="reed2022singingperformances"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#reed2022singingperformances">reed2022singingperformances</a>,
  author = {Reed, CN and Skach, S and Strohmeier, P and McPherson, AP},
  conference = {ACM International Conference Proceeding Series},
  month = {Mar},
  pages = {170--183},
  title = {Singing Knit: Soft Knit Biosensing for Augmenting Vocal Performances},
  year = {2022},
  abstract = {This paper discusses the design of the Singing Knit, a wearable knit collar for measuring a singer's vocal interactions through surface electromyography. We improve the ease and comfort of multi-electrode bio-sensing systems by adapting knit e-textile methods. The goal of the design was to preserve the capabilities of rigid electrode sensing while addressing its shortcomings, focusing on comfort and reliability during extended wear, practicality and convenience for performance settings, and aesthetic value. We use conductive, silver-plated nylon jersey fabric electrodes in a full rib knit accessory for sensing laryngeal muscular activation. We discuss the iterative design and the material decision-making process as a method for building integrated soft-sensing wearable systems for similar settings. Additionally, we discuss how the design choices through the construction process reflect its use in a musical performance context.},
  doi = {10.1145/3519391.3519412},
  isbn = {9781450396325},
  day = {13},
  publicationstatus = {published}
}
</pre>

<a name="armitagethecontrol"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#armitagethecontrol">armitagethecontrol</a>,
  author = {ARMITAGE, JDK and MCPHERSON, A},
  conference = {},
  title = {The Stenophone: Live coding on a chorded keyboard with continuous control},
  year = {},
  booktitle = {International Conference on Live Coding},
  publicationstatus = {published}
}
</pre>

<a name="bin2016skipresponse"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#bin2016skipresponse">bin2016skipresponse</a>,
  author = {BIN, SMA and MCPHERSON, AP and BRYAN-KINNS, N},
  conference = {},
  month = {Jul},
  organization = {Brisbane, Australia},
  pages = {200--205},
  title = {Skip the Pre-Concert Demo: How Technical Familiarity and Musical Style Affect Audience Response},
  url = {<a href="http://nime.org/archives">http://nime.org/archives</a>},
  volume = {16},
  year = {2016},
  abstract = {This paper explores the roles of technical and musical familiarity in shaping audience response to digital musical instrument (DMI) performances. In an audience study conducted during an evening concert, we examined two primary questions: first, whether a deeper understanding of how a DMI
works increases an audience’s enjoyment and interest in the performance; and second, given the same DMI and same performer, whether playing in a conventional (vernacular) versus an experimental musical style affects an audience’s response. We held a concert in which two DMI creator-performers each played two pieces in differing styles. Before the concert, each half the 64-person audience was given a technical explanation of one of the instruments. Results showed that receiving an explanation increased the reported understanding of that instrument, but had no effect on either
the reported level of interest or enjoyment. On the other hand, performances in experimental versus conventional style on the same instrument received widely divergent audience responses. We discuss implications of these
findings for DMI design.},
  startyear = {2016},
  startmonth = {Jul},
  startday = {11},
  finishyear = {2016},
  finishmonth = {Jul},
  finishday = {15},
  keyword = {NIME},
  keyword = {Audience studies},
  keyword = {Sound and music computing},
  keyword = {Human computer interaction},
  booktitle = {New Interfaces for Musical Expression},
  day = {11},
  publicationstatus = {published}
}
</pre>

<a name="morrealenimeperspective"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#morrealenimeperspective">morrealenimeperspective</a>,
  author = {MORREALE, F and MCPHERSON, A and WANDERLEY, M},
  conference = {},
  title = {NIME Identity from the Performer’s Perspective},
  year = {},
  abstract = {The term ‘NIME’ - New Interfaces for Musical Expression - has come to signify both technical and cultural characteristics. Not all new musical instruments are NIMEs, and not all NIMEs are defined as such for the sole ephemeral condition of being new. So, what are the typical characteristics of NIMEs and what are their roles in performers’ practice? Is there a typical NIME repertoire? This paper aims to address these questions with a bottom up approach. We reflect on the answers of 78 NIME performers to an online questionnaire discussing their performance experience with NIMEs. The results of our investigation explore the role of NIMEs in the performers’ practice and identify the values that are common among performers. We find that most NIMEs are viewed as exploratory tools created by and for performers, and that they are constantly in development and almost in no occasions in a finite state. The findings of our survey also reflect upon virtuosity with NIMEs, whose peculiar performance practice results in learning trajectories that often do not lead to the development of virtuosity as it is commonly understood in traditional performance},
  keyword = {artistic practice},
  keyword = {virtuosity},
  keyword = {DMI performance},
  booktitle = {New Interfaces for Musical Expreesion},
  publicationstatus = {accepted}
}
</pre>

<a name="leprimirroringtechnology"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#leprimirroringtechnology">leprimirroringtechnology</a>,
  author = {LEPRI, G and MCPHERSON, A},
  conference = {},
  organization = {Blacksburg, VA},
  title = {Mirroring the past, from typewriting to interactive art: an approach to the re-design of a vintage technology},
  year = {},
  abstract = {Obsolete and old technologies are often used in interactive
art and music performance. DIY practices such as hardware
hacking and circuit bending provide e ective methods to
the integration of old machines into new artistic inventions.
This paper presents the Cembalo Scrivano .1, an interactive
audio-visual installation based on an augmented typewriter.
Borrowing concepts from media archaeology studies, tangi-
ble interaction design and digital lutherie, we discuss how
investigations into the historical and cultural evolution of
a technology can suggest directions for the regeneration of
obsolete objects. The design approach outlined focuses on
the remediation of an old device and aims to evoke cultural
and physical properties associated to the source object.},
  startyear = {2018},
  startmonth = {Jun},
  startday = {3},
  finishyear = {2018},
  finishmonth = {Jun},
  finishday = {6},
  booktitle = {New Interfaces for Musical Expression},
  publicationstatus = {accepted}
}
</pre>

<a name="morrealedesign201014"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#morrealedesign201014">morrealedesign201014</a>,
  author = {MORREALE, F and MCPHERSON},
  conference = {},
  title = {Design for Longevity: Ongoing Use of Instruments from NIME 2010-14},
  year = {},
  abstract = {Every new edition of NIME brings dozens of new DMIs and the feeling that only a few of them will eventually break through. Previous work tried to address this issue with a deductive approach by formulating design frameworks; we addressed this issue with a inductive approach by elaborat- ing on successes and failures of previous DMIs. We contacted 97 DMI makers that presented a new instrument at five successive editions of NIME (2010-2014); 70 answered. They were asked to indicate the original motivation for designing the DMI and to present information about its uptake. Results confirmed that most of the instruments have di culties establishing themselves. Also, they were asked to reflect on the specific factors that facilitated and those that hindered instrument longevity. By grounding these reflections on existing reserach on NIME and HCI, we propose a series of design considerations for future DMIs.},
  keyword = {design},
  keyword = {evaluation},
  keyword = {performance},
  keyword = {survey},
  keyword = {digital musical instrument},
  keyword = {user experience},
  booktitle = {NIME},
  publicationstatus = {published}
}
</pre>

<a name="donovanbuildingtechnologies"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#donovanbuildingtechnologies">donovanbuildingtechnologies</a>,
  author = {DONOVAN, L and BIN, SMA and ARMITAGE, JDK and MCPHERSON, A},
  conference = {},
  title = {Building an IDE for an embedded system using web technologies},
  year = {},
  booktitle = {Web Audio Conference},
  publicationstatus = {published}
}
</pre>

<a name="moroapproximatingfilters"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#moroapproximatingfilters">moroapproximatingfilters</a>,
  author = {moro, G and mcpherson, A},
  conference = {},
  title = {APPROXIMATING NON-LINEAR INDUCTORS USING TIME-VARIANT LINEAR FILTERS},
  year = {},
  booktitle = {18th International Conference on Digital Audio Effects},
  publicationstatus = {published}
}
</pre>

<a name="luo2022towardsaudio"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#luo2022towardsaudio">luo2022towardsaudio</a>,
  author = {Luo, Y-J and Ewert, S and Dixon, S},
  conference = {},
  month = {Jul},
  pages = {3299--3305},
  title = {Towards Robust Unsupervised Disentanglement of Sequential Data — A Case Study Using Music Audio},
  year = {2022},
  doi = {10.24963/ijcai.2022/458},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence},
  day = {1}
}
</pre>

<a name="proutskova2022thejazz"></a><pre>
@article{<a href="pubs2022_raw.html#proutskova2022thejazz">proutskova2022thejazz</a>,
  author = {Proutskova, P and Wolff, D and Fazekas, G and Frieler, K and Höger, F and Velichkina, O and Solis, G and Weyde, T and Pfleiderer, M and Crayencour, HC and Peeters, G and Dixon, S},
  journal = {Journal of Web Semantics},
  month = {Jul},
  pages = {100735--100735},
  title = {The Jazz Ontology: A semantic model and large-scale RDF repositories for jazz},
  year = {2022},
  doi = {10.1016/j.websem.2022.100735},
  issn = {1570-8268},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="stollerwaveunetseparation"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#stollerwaveunetseparation">stollerwaveunetseparation</a>,
  author = {STOLLER, D and EWERT, S and DIXON, S},
  conference = {},
  title = {Wave-U-Net: A Multi-Scale Neural Network for End-to-End Audio Source Separation},
  year = {},
  booktitle = {19th International Society for Music Information Retrieval Conference (ISMIR)},
  publicationstatus = {accepted}
}
</pre>

<a name="pantelionsimilarity"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#pantelionsimilarity">pantelionsimilarity</a>,
  author = {PANTELI, M and Dixon, S},
  conference = {},
  organization = {New York, USA},
  title = {On the evaluation of rhythmic and melodic descriptors for music similarity},
  year = {},
  startyear = {2016},
  startmonth = {Aug},
  startday = {7},
  finishyear = {2016},
  finishmonth = {Aug},
  finishday = {11},
  booktitle = {International Society for Music Information Retrieval Conference},
  publicationstatus = {accepted}
}
</pre>

<a name="stolleranalysissinging"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#stolleranalysissinging">stolleranalysissinging</a>,
  author = {STOLLER, D and Dixon, S},
  conference = {},
  organization = {New York City, USA},
  title = {Analysis and classification of phonation modes in singing},
  year = {},
  abstract = {Phonation mode is an expressive aspect of the singing voice and can be
described using the four categories neutral, breathy, pressed and flow.
Previous attempts at automatically classifying the phonation mode on a dataset containing vowels sung by a female professional have been lacking in accuracy or have not sufficiently investigated the characteristic features of the different phonation modes which enable successful classification. In this paper, we extract a large range of features from this dataset, including specialised descriptors of pressedness and breathiness, to analyse their explanatory power and robustness against changes of pitch and vowel. We train and optimise a feed-forward neural network (NN) with one hidden layer on all features using cross validation to achieve a mean F-measure above 0.85 and an improved performance compared to previous work. Applying feature selection based on mutual information and retaining the nine highest ranked features as input to a NN results in a mean F-measure of 0.78, demonstrating the suitability of these features to discriminate between phonation modes. Training and pruning a decision tree yields a simple rule set based only on cepstral peak prominence (CPP), temporal flatness and average energy that correctly categorises 78\% of the recordings.},
  startyear = {2016},
  startmonth = {Aug},
  startday = {11},
  finishyear = {2016},
  finishmonth = {Aug},
  finishday = {7},
  booktitle = {17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
  publicationstatus = {accepted}
}
</pre>

<a name="sarkar2022ensemblesetseparation"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#sarkar2022ensemblesetseparation">sarkar2022ensemblesetseparation</a>,
  author = {Sarkar, S and Benetos, E and Sandler, M},
  conference = {},
  month = {Dec},
  organization = {Bangalore},
  title = {EnsembleSet: A new high-quality synthesised dataset for chamber ensemble separation},
  year = {2022},
  abstract = {Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free  multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.},
  startyear = {2022},
  startmonth = {Dec},
  startday = {5},
  finishyear = {2022},
  finishmonth = {Dec},
  finishday = {8},
  booktitle = {International Society for Music Information Retrieval},
  day = {8},
  publicationstatus = {accepted}
}
</pre>

<a name="caspe2022ddx7sounds"></a><pre>
@misc{<a href="pubs2022_raw.html#caspe2022ddx7sounds">caspe2022ddx7sounds</a>,
  author = {Caspe, F and McPherson, A and Sandler, M},
  month = {Aug},
  title = {DDX7: Differentiable FM Synthesis of Musical Instrument Sounds},
  year = {2022},
  doi = {10.48550/arxiv.2208.06169},
  day = {12}
}
</pre>

<a name="subramanian2022anomalousmethods"></a><pre>
@misc{<a href="pubs2022_raw.html#subramanian2022anomalousmethods">subramanian2022anomalousmethods</a>,
  author = {Subramanian, V and Gururani, S and Benetos, E and Sandler, M},
  month = {Jul},
  title = {Anomalous behaviour in loss-gradient based interpretability methods},
  year = {2022},
  doi = {10.48550/arxiv.2207.07769},
  day = {15}
}
</pre>

<a name="caspeddx7sounds"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#caspeddx7sounds">caspeddx7sounds</a>,
  author = {Caspe, F and Mcpherson, A and Sandler, M},
  conference = {},
  organization = {Bengaluru, India},
  title = {DDX7: Differentiable FM Synthesis of Musical Instrument Sounds},
  year = {},
  abstract = {FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source.
On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.},
  booktitle = {23rd International Society for Music Information Retrieval Conference (ISMIR 2022)},
  publicationstatus = {accepted}
}
</pre>

<a name="zhao2022violinistdistributions"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#zhao2022violinistdistributions">zhao2022violinistdistributions</a>,
  author = {Zhao, Y and Fazekas, G and Sandler, M},
  conference = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  month = {Jan},
  pages = {601--605},
  title = {VIOLINIST IDENTIFICATION USING NOTE-LEVEL TIMBRE FEATURE DISTRIBUTIONS},
  volume = {2022-May},
  year = {2022},
  abstract = {Modelling musical performers' individual playing styles based on audio features is important for music education, music expression analysis and music generation. In violin performance, the perception of playing styles are mainly affected by the characteristic musical timbre, which is mostly determined by performers, instruments and recording conditions. To verify if timbre features can describe a performer's style adequately, we examine a violinist identification method based on note-level timbre feature distributions. We first apply it using solo datasets to recognise professional violinists, then use it to identify master players from commercial concerto recordings. The results show that the designed features and method work very well for both datasets. The identification accuracy with the solo dataset using MFCCs and spectral constrast features are 0.94 and 0.91 respectively. Significantly lower but promising results are reported with the concerto dataset. Results suggest that the selected timbre features can model performers' individual playing reasonably objectively, regardless of the instrument they play.},
  doi = {10.1109/ICASSP43922.2022.9747606},
  isbn = {9781665405409},
  issn = {1520-6149},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="sandler2005sigmamodulation"></a><pre>
@misc{<a href="pubs2022_raw.html#sandler2005sigmamodulation">sandler2005sigmamodulation</a>,
  author = {SANDLER, M and Reiss, J},
  month = {Jul},
  number = {11/995,804},
  organization = {USA/GB},
  title = {Sigma Delta Modulation},
  url = {<a href="http://www.patents.com/us-7777657.html">http://www.patents.com/us-7777657.html</a>},
  year = {2005},
  abstract = {A method is provided for detecting limit cycles in a sigma delta modulator having an output signal that varies over a series of time intervals. In this method a first value that is indicative of the level of the modulator output signal after a predetermined time interval is stored in a first memory, and a second value that is indicative of the level of the modulator output signal after a further time interval subsequent to the predetermined time interval is stored in a second memory. The first value stored in the first memory is compared with the second value stored in the second memory, and an output indicative of a tendency for limit cycles to be produced in the modulator output signal is provided in response to such comparison. Such a method is particularly advantageous for detecting limit cycles in a sigma delta modulator as it can be implemented in a straightforward manner and offers a very accurate limit cycle detection mechanism. As a result it only becomes necessary to activate a limit cycle removal mechanism when limit cycle behavior has been observed, and major changes to design are not normally required to implement the detection mechanism},
  confidential = {False},
  filedyear = {2006},
  filedmonth = {Jun},
  filedday = {29},
  day = {18},
  publicationstatus = {accepted}
}
</pre>

<a name="sandler2006drumloops"></a><pre>
@misc{<a href="pubs2022_raw.html#sandler2006drumloops">sandler2006drumloops</a>,
  author = {SANDLER, MB and Bello, JP and Ravelli, E},
  month = {May},
  number = {0609408.0},
  title = {Drum Loops},
  year = {2006},
  confidential = {False},
  day = {12}
}
</pre>

<a name="sandler2005sonicvisual"></a><pre>
@misc{<a href="pubs2022_raw.html#sandler2005sonicvisual">sandler2005sonicvisual</a>,
  author = {SANDLER, MB and Landone, C and Cannam, C},
  month = {Nov},
  number = {0524287.0},
  title = {Sonic Visual},
  year = {2005},
  confidential = {False},
  day = {29}
}
</pre>

<a name="sandler2005amodulators"></a><pre>
@misc{<a href="pubs2022_raw.html#sandler2005amodulators">sandler2005amodulators</a>,
  author = {Sandler, MB and REISS, J},
  month = {Jul},
  note = {Page bookmark 	WO 2007010298  (A1)  -  SIGMA DELTA MODULATORS 
found as an unexamined application},
  number = {WO2006GB50175 20060629},
  title = {A mechanism for the detection and removal of limit cycles in the operation of sigma delta modulators},
  year = {2005},
  confidential = {False},
  day = {18}
}
</pre>

<a name="sandler2003methodsignals"></a><pre>
@misc{<a href="pubs2022_raw.html#sandler2003methodsignals">sandler2003methodsignals</a>,
  author = {SANDLER, MB},
  month = {Mar},
  number = {0305543.1},
  title = {Method for Accurate Time Position of Transients in Signals},
  year = {2003},
  confidential = {False},
  day = {11}
}
</pre>

<a name="choi2016towardstransitions"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#choi2016towardstransitions">choi2016towardstransitions</a>,
  author = {Choi, K and Fazekas, G and Sandler, M},
  conference = {Proc. 24th ACM Conference on User Modeling, Adaptation and Personalisation (UMAP 2016), Workshop on Surprise, Opposition, and Obstruction in Adaptive and Personalized Systems (SOAP) June 13–17, Halifax, Canada},
  note = {date-added: 2017-12-22 14:17:02 +0000
date-modified: 2017-12-22 15:19:16 +0000
keywords: playlist generation, semantic audio, music transition modeling
local-url: https://arxiv.org/pdf/1606.02096.pdf
bdsk-url-1: http://ceur-ws.org/Vol-1618/SOAP_paper4.pdf
bdsk-url-2: https://dx.doi.org/10.1145/1235},
  title = {Towards Playlist Generation Algorithms Using RNNs Trained on Within-Track Transitions},
  url = {<a href="http://ceur-ws.org/Vol-1618/SOAP_paper4.pdf">http://ceur-ws.org/Vol-1618/SOAP_paper4.pdf</a>},
  year = {2016},
  doi = {10.1145/1235}
}
</pre>

<a name="choitowardsdescriptions"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#choitowardsdescriptions">choitowardsdescriptions</a>,
  author = {CHOI, K and fazekas, G and sandler, M and McFee, B and Cho, K},
  conference = {},
  organization = {New York},
  title = {Towards Music Captioning: Generating Music Playlist Descriptions},
  year = {},
  abstract = {Descriptions are often provided along with recommenda- tions to help users’ discovery. Recommending automati- cally generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural lan- guage processing are adopted to utilise the information of each track.},
  booktitle = {Late-Breaking/Demo session of 17th International Society of Music Information Retrieval (ISMIR) Conference},
  publicationstatus = {online-published}
}
</pre>

<a name="fanoyelaonstudy"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#fanoyelaonstudy">fanoyelaonstudy</a>,
  author = {FANO YELA, D and ewert, SE and sandler, MS and FitzGerald, DF},
  conference = {},
  organization = {Erlangen, Germany},
  title = {On the Importance of Temporal Context in Proximity Kernels: A Vocal Separation Case Study},
  year = {},
  abstract = {Musical source separation methods exploit source-specific spectral characteristics to facilitate the decomposition process. Kernel Additive Modelling (KAM) models a source applying robust statistics to time-frequency bins as specified by a source-specific kernel, a function defining similarity between bins. Kernels in existing approaches are typically defined using metrics between single time frames. In the presence of noise and other sound sources information from a single-frame, however, turns out to be unreliable and often incorrect frames are selected as similar. In this paper, we incorporate a temporal context into the kernel to provide additional information stabilizing the similarity search. Evaluated in the context of vocal separation, our simple extension led to a considerable improvement in separation quality compared to previous kernels.},
  startyear = {2017},
  startmonth = {Jun},
  startday = {22},
  finishyear = {2017},
  finishmonth = {Jun},
  finishday = {24},
  keyword = {Source Separation},
  keyword = {Kernel Additive Modelling},
  booktitle = {Audio Engineering Society Conference on Semantic Audio},
  publicationstatus = {accepted}
}
</pre>

<a name="thalmann2015navigatingplayer"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#thalmann2015navigatingplayer">thalmann2015navigatingplayer</a>,
  author = {Thalmann, F and Carrillo, A and Fazekas, G and Wiggins, GA and Sandler, M},
  conference = {Proc. of the 16th International Society for Music Information Retrieval (ISMIR-15) conference, Late-breaking session, Oct. 26-30, Malaga, Spain},
  note = {date-added: 2017-12-22 19:18:33 +0000
date-modified: 2017-12-22 19:59:04 +0000
keywords: ontology, mobile applications, mobile audio ontology, web application
local-url: http://ismir2015.uma.es/LBD/LBD26.pdf
publisher-url: http://ismir2015.uma.es/LBD/LBD19.pdf
bdsk-url-1: https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16154/Thalmann\%20Navigating\%20Ontological\%20Structures\%202015\%20Published.pdf},
  title = {Navigating Ontological Structures based on Feature Metadata Using the Semantic Music Player},
  url = {https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16154/Thalmann\%20Navigating\%20Ontological\%20Structures\%202015\%20Published.pdf},
  year = {2015}
}
</pre>

<a name="tianmusicfeatures"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#tianmusicfeatures">tianmusicfeatures</a>,
  author = {TIAN, M and SANDLER, MARKB},
  conference = {},
  title = {Music Structural Segmentation Across Genres with Gammatone Features},
  year = {},
  booktitle = {ISMIR},
  publicationstatus = {accepted}
}
</pre>

<a name="fanoyelainterferencefactorization"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#fanoyelainterferencefactorization">fanoyelainterferencefactorization</a>,
  author = {FANO YELA, D and Ewert, SE and Sandler, MS and FitzGerald, DF},
  conference = {},
  organization = {New Orleans},
  title = {INTERFERENCE REDUCTION IN MUSIC RECORDINGS COMBINING KERNEL ADDITIVE MODELLING AND NON-NEGATIVE MATRIX FACTORIZATION},
  year = {},
  abstract = {In live and studio recordings unexpected sound events often lead to interferences in the signal. For non-stationary interferences, sound source separation techniques can be used to reduce the interference level in the recording. In this context, we present a novel approach combining the strengths of two algorithmic families: NMF and KAM. The recent KAM approach applies robust statistics on frames selected by a source-specific kernel to perform source separation. Based on semi-supervised NMF, we extend this approach in two ways. First, we locate the interference in the recording based on detected NMF activity. Second, we improve the kernel-based frame selection by incorporating an NMF-based estimate of the clean music signal. Further, we introduce a temporal context in the kernel, taking some musical structure into account. Our experiments show improved separation quality for our proposed method over a state-of-the-art approach for interference reduction.},
  startyear = {2017},
  startmonth = {Mar},
  startday = {5},
  finishyear = {2017},
  finishmonth = {Mar},
  finishday = {9},
  keyword = {Source Separation},
  keyword = {Kernel Additive Modelling},
  keyword = {Non-Negative Matrix Factorisation},
  keyword = {Interference Reduction},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processings},
  publicationstatus = {published}
}
</pre>

<a name="carrillo2016geolocationplayer"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#carrillo2016geolocationplayer">carrillo2016geolocationplayer</a>,
  author = {Carrillo, A and Thalmann, F and Fazekas, G and Sandler, M},
  conference = {Proc. Web Audio Conference WAC-2016, April 4–6, Atlanta, USA},
  note = {date-added: 2017-12-22 20:02:39 +0000
date-modified: 2017-12-22 20:05:50 +0000
keywords: adaptive music, intelligent music player, semantic audio, feature extraction
bdsk-url-1: https://smartech.gatech.edu/bitstream/handle/1853/54586/WAC2016-47.pdf},
  title = {Geolocation Adaptive Music Player},
  url = {https://smartech.gatech.edu/bitstream/handle/1853/54586/WAC2016-47.pdf},
  year = {2016},
  abstract = {We present a web-based cross-platform adaptive music player that combines music information retrieval (MIR) and audio processing technologies with the interaction capabilities offered by GPS-equipped mobile devices. The application plays back a list of music tracks, which are linked to geographic paths in a map. The music player has two main enhanced features that adjust to the location of the user, namely, adaptable length of the songs and automatic transitions between tracks. Music tracks are represented as data packages containing audio and metadata (descriptive and behavioral) that builds on the concept of Digital Music Object (DMO). This representation, in line with nextgeneration web technologies, allows for  exible production and consumption of novel musical experiences. A content provider assembles a data pack with music, descriptive analysis and action parameters that users can experience and control within the restrictions and templates defined by the provider.}
}
</pre>

<a name="thalmann2016creatingdesigner"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#thalmann2016creatingdesigner">thalmann2016creatingdesigner</a>,
  author = {Thalmann, F and Fazekas, G and Wiggins, GA and Sandler, M},
  conference = {Proc. ACM Audio Mostly Conference, Oct. 4-6, Norrköping, Sweden},
  note = {date-added: 2017-12-21 19:31:03 +0000
date-modified: 2017-12-22 13:13:01 +0000
keywords: music ontology, dynamic music objects, semantic audio, intelligent music production, mobile applications
local-url: https://dl.acm.org/citation.cfm?id=2986445
bdsk-url-1: https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16155/Thalmann\%20Creating\%20Visualizing\%20and\%20Analyzing\%202016\%20Submitted.pdf
bdsk-url-2: https://dx.doi.org/10.1145/2986416.2986445},
  pages = {39--46},
  title = {Creating, Visualizing, and Analyzing Dynamic Music Objects in the Browser with the Dymo Designer},
  url = {https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16155/Thalmann\%20Creating\%20Visualizing\%20and\%20Analyzing\%202016\%20Submitted.pdf},
  year = {2016},
  abstract = {Dynamic music is gaining increasing popularity outside of its initial environment, the videogame industry, and is gradually becoming an autonomous medium. Responsible for this is doubtlessly the prevalence of integrated multisensory platforms such as smartphones as well as the omnipresence of the internet as a provider of content on demand. The music format Dynamic Music Objects builds on these assumptions and on recent advances in music information retrieval and semantic web technologies. It is capable of describing a multitude of adaptive, interactive, and immersive musical experiences. This paper introduces the Dymo Designer, a prototypical web app that allows people to create and analyze Dynamic Music Objects in a visual, interactive, and computer-assisted way.},
  doi = {10.1145/2986416.2986445},
  isbn = {978-1-4503-4822-5}
}
</pre>

<a name="quintonaudiostructure"></a><pre>
@misc{<a href="pubs2022_raw.html#quintonaudiostructure">quintonaudiostructure</a>,
  author = {QUINTON, E and Harte, C and Sandler, M},
  title = {Audio tempo estimation using fusion of time-frequency analyses and metrical structure},
  publicationstatus = {published}
}
</pre>

<a name="ozaki2022similaritiesrecordings"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#ozaki2022similaritiesrecordings">ozaki2022similaritiesrecordings</a>,
  author = {Ozaki, Y and Kuroyanagi, J and McBride, J and Proutskova, P and Tierney, A and Pfordresher, P and Benetos, E and Liu, F and Savage, PE},
  conference = {},
  month = {Sep},
  organization = {Kanazawa, Japan},
  title = {Similarities and differences in a cross-linguistic sample of song and speech recordings},
  url = {https://sites.google.com/view/joint-conf-language-evolution/home},
  year = {2022},
  startyear = {2022},
  startmonth = {Sep},
  startday = {5},
  finishyear = {2022},
  finishmonth = {Sep},
  finishday = {8},
  booktitle = {Joint Conference on Language Evolution},
  day = {5},
  publicationstatus = {accepted}
}
</pre>

<a name="wang2022jointcallrecognition"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#wang2022jointcallrecognition">wang2022jointcallrecognition</a>,
  author = {Wang, C and Benetos, E and Versace, E and Wang, S},
  conference = {},
  month = {Aug},
  organization = {Belgrade, Serbia},
  pages = {195--199},
  title = {Joint Scattering for Automatic Chick Call
Recognition},
  year = {2022},
  abstract = {Animal vocalisations contain important information about health, emotional state, and behaviour, thus can be potentially used for animal welfare monitoring and behavioural neuroscience studies. Motivated by the spectro-temporal patterns of chick calls in the time–frequency domain, in this paper we propose an automatic system for chick call recognition using the joint time–frequency scattering transform (JTFS). Taking full-length recordings as input, the system first extracts chick call candidates by an onset detector and silence removal. After computing their JTFS features, a support vector machine classifier groups each candidate into different chick call types. Evaluating on a dataset comprising 3013 chick calls collected in laboratory conditions, the proposed recognition system using the JTFS features improves the frame- and event-based macro F-measures by 9.5\% and 11.7\%, respectively, than that of a melfrequency cepstral coefficients baseline.
Index Terms—Audio signal processing, bioacoustics, scattering transform.},
  startyear = {2022},
  startmonth = {Aug},
  startday = {29},
  finishyear = {2022},
  finishmonth = {Sep},
  finishday = {2},
  keyword = {audio signal processing},
  keyword = {bioacoustics},
  keyword = {scattering transform},
  booktitle = {30th European Signal Processing Conference},
  day = {29},
  publicationstatus = {published}
}
</pre>

<a name="singh2022hypernetworksproofofconcept"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#singh2022hypernetworksproofofconcept">singh2022hypernetworksproofofconcept</a>,
  author = {Singh, S and Benetos, E and Phan, QH},
  conference = {},
  month = {Aug},
  organization = {Belgrade, Serbia},
  pages = {429--433},
  publisher = {EURASIP},
  title = {Hypernetworks for sound event detection: a proof-of-concept},
  year = {2022},
  abstract = {Polyphonic sound event detection (SED) involves the prediction of sound events present in an audio recording along with their onset and offset times. Recently, Deep Neural Networks, specifically convolutional recurrent neural networks (CRNN) have achieved impressive results for this task. The convolution part of the architecture is used to extract translational invariant features from the input and the recurrent part learns the underlying temporal relationship between audio frames. Recent studies showed that the weight sharing paradigm of recurrent networks might be a hindering factor in certain kinds of time series data, specifically where there is a temporal conditional shift, i.e. the conditional distribution of a label changes across the temporal scale. This warrants a relevant question - is there a similar phenomenon in polyphonic sound events due to dynamic polyphony level across the temporal axis? In this work, we explore this question and inquire if relaxed weight sharing improves performance of a CRNN for polyphonic SED. We propose to use hypernetworks to relax weight sharing in the recurrent part and show that the CRNN’s performance is improved by ~3\% across two datasets, thus paving the way for further exploration of the existence of temporal conditional shift for polyphonic SED.},
  startyear = {2022},
  startmonth = {Aug},
  startday = {29},
  finishyear = {2022},
  finishmonth = {Sep},
  finishday = {3},
  booktitle = {30th European Signal Processing Conference (EUSIPCO 2022)},
  day = {29},
  publicationstatus = {published}
}
</pre>

<a name="liuperformancetracking"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#liuperformancetracking">liuperformancetracking</a>,
  author = {Liu, L and KONG, Q and Morfi, G-V and Benetos, E},
  conference = {},
  organization = {Bengaluru, India},
  title = {Performance MIDI-to-score conversion by neural beat tracking},
  url = {https://cheriell.github.io/},
  year = {},
  abstract = {Rhythm quantisation is an essential part of converting performance MIDI recordings into musical scores. Previous works on rhythm quantisation are limited to the use of probabilistic or statistical methods. In this paper, we propose a MIDI-to-score quantisation method using a convolutional-recurrent neural network (CRNN) trained on MIDI note sequences to predict whether notes are on beats. Then, we expand the CRNN model to predict the quantised times for all beat and non-beat notes. Furthermore, we enable the model to predict the key signatures, time signatures, and hand parts of all notes. Our proposed performance MIDI-to-score system achieves significantly better performance compared to commercial software evaluated on the MV2H metric. We release the toolbox for converting performance MIDI into MIDI scores at: https://github.com/cheriell/PM2S},
  startyear = {2022},
  startmonth = {Dec},
  startday = {4},
  finishyear = {2022},
  finishmonth = {Dec},
  finishday = {8},
  booktitle = {Proceedings of the 23rd International Society for Music Information Retrieval Conference},
  publicationstatus = {accepted}
}
</pre>

<a name="daikoku2022agreementsample"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#daikoku2022agreementsample">daikoku2022agreementsample</a>,
  author = {Daikoku, H and Ding, S and Benetos, E and Wood, ALC and Shimizono, T and Sanne, US and Fujii, S and Savage, PE},
  conference = {},
  month = {Jun},
  organization = {Sheffield, UK},
  title = {Agreement among human and automated estimates of similarity in a global music sample},
  year = {2022},
  abstract = {While music information retrieval (MIR) has made substantial progress in automatic analysis of audio similarity for Western music, it remains unclear whether these algorithms can be meaningfully applied to cross-cultural analyses of more diverse musics. Here we collect perceptual ratings from 62 Japanese participants using a global sample of 30 traditional songs, and compare these ratings against both pre-existing expert annotations and audio similarity algorithms. We find that different methods of perceptual ratings all produced similar, moderate levels of inter-rater agreement comparable to previous studies, but that agreement between human and automated methods is always low regardless of the specific methods used to calculate musical similarity. Our findings suggest that the MIR methods tested are unable to measure cross-cultural music similarity in perceptually meaningful ways.},
  startyear = {2022},
  startmonth = {Jun},
  startday = {14},
  finishyear = {2022},
  finishmonth = {Jun},
  finishday = {17},
  booktitle = {10th International Workshop on Folk Music Analysis (FMA 2022)},
  day = {14},
  publicationstatus = {accepted}
}
</pre>

<a name="huang2022improvingdetection"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#huang2022improvingdetection">huang2022improvingdetection</a>,
  author = {Huang, J and Benetos, E and Ewert, S},
  conference = {},
  month = {May},
  organization = {Singapore},
  pages = {451--455},
  publisher = {IEEE},
  title = {Improving lyrics Alignment through Joint Pitch Detection},
  year = {2022},
  abstract = {In recent years, the accuracy of automatic lyrics alignment methods has increased considerably. Yet, many current approaches employ frameworks designed for automatic speech recognition (ASR) and do not exploit properties specific to music. Pitch is one important musical attribute of singing voice but it is often ignored by current systems as the lyrics content is considered independent of the pitch. In practice, however, there is a temporal correlation between the two as note starts often correlate with phoneme starts. At the same time the pitch is usually annotated with high temporal accuracy in ground truth data while the timing of lyrics is often only available at the line (or word) level. In this paper, we propose a multi-task learning approach for lyrics alignment that incorporates pitch and thus can make use of a new source of highly accurate temporal information. Our results show that the accuracy of the alignment result is indeed improved by our approach. As an additional contribution, we show that integrating boundary detection in the forced-alignment algorithm reduces cross-line errors, which improves the accuracy even further.},
  doi = {10.1109/ICASSP43922.2022.9746460},
  startyear = {2022},
  startmonth = {May},
  startday = {22},
  finishyear = {2022},
  finishmonth = {May},
  finishday = {27},
  booktitle = {2022 IEEE International Conference on Acoustics, Speech and Signal Processing},
  day = {22},
  publicationstatus = {published}
}
</pre>

<a name="manco2022learningsupervision"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#manco2022learningsupervision">manco2022learningsupervision</a>,
  author = {Manco, I and Benetos, E and Quinton, E and Fazekas, G},
  conference = {},
  month = {May},
  organization = {Singapore},
  pages = {456--460},
  publisher = {IEEE},
  title = {Learning music audio representations via weak language supervision},
  url = {https://ilariamanco.com/},
  year = {2022},
  abstract = {Audio representations for music information retrieval are typically learned via supervised learning in a task-specific fashion. Although effective at producing state-of-the-art results, this scheme lacks flexibility with respect to the range of applications a model can have and requires extensively annotated datasets. In this work, we pose the question of whether it may be possible to exploit weakly aligned text as the only supervisory signal to learn general-purpose music audio representations. To address this question, we design a multimodal architecture for music and language pre-training (MuLaP) optimised via a set of proxy tasks. Weak supervision is provided in the form of noisy natural language descriptions conveying the overall musical content of the track. After pre-training, we transfer the audio backbone of the model to a set of music audio classification and regression tasks. We demonstrate the usefulness of our approach by comparing the performance of audio representations produced by the same audio backbone with different training strategies and show that our pre-training method consistently achieves comparable or higher scores on all tasks and datasets considered. Our experiments also confirm that MuLaP effectively leverages audio-caption pairs to learn representations that are competitive with audio-only and cross-modal self-supervised methods in the literature.},
  doi = {10.1109/ICASSP43922.2022.9746996},
  startyear = {2022},
  startmonth = {May},
  startday = {22},
  finishyear = {2022},
  finishmonth = {May},
  finishday = {27},
  keyword = {audio and language},
  keyword = {audio representations},
  keyword = {multimodal learning},
  keyword = {music information retrieval},
  booktitle = {2022 IEEE International Conference on Acoustics, Speech and Signal Processing},
  day = {22},
  publicationstatus = {published}
}
</pre>

<a name="ou2022exploringtranscription"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#ou2022exploringtranscription">ou2022exploringtranscription</a>,
  author = {Ou, L and Guo, Z and Benetos, E and Han, J and Wang, Y},
  conference = {},
  month = {May},
  organization = {Singapore},
  pages = {776--780},
  publisher = {IEEE},
  title = {Exploring transformer's potential on automatic piano transcription},
  url = {https://2022.ieeeicassp.org/},
  year = {2022},
  abstract = {Most recent research about automatic music transcription (AMT) uses convolutional neural networks and recurrent neural networks to model the mapping from music signals to symbolic notation. Based on a high-resolution piano transcription system, we explore the possibility of incorporating another powerful sequence transformation tool—the Transformer—to deal with the AMT problem. We argue that the properties of the Transformer make it more suitable for certain AMT subtasks. We confirm the Transformer’s superiority on the velocity detection task by experiments on the MAESTRO dataset and a cross-dataset evaluation on the MAPS dataset. We observe a performance improvement on both frame-level and note-level metrics after introducing the Transformer network.},
  doi = {10.1109/ICASSP43922.2022.9746789},
  startyear = {2022},
  startmonth = {May},
  startday = {7},
  finishyear = {2022},
  finishmonth = {May},
  finishday = {13},
  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)},
  day = {7},
  publicationstatus = {published}
}
</pre>

<a name="ragano2022aquality"></a><pre>
@misc{<a href="pubs2022_raw.html#ragano2022aquality">ragano2022aquality</a>,
  author = {Ragano, A and Benetos, E and Chinen, M and Martinez, HB and Reddy, CKA and Skoglund, J and Hines, A},
  month = {Apr},
  title = {A Comparison of Deep Learning MOS Predictors for Speech Synthesis  Quality},
  year = {2022},
  doi = {10.48550/arxiv.2204.02249},
  keyword = {Clinical Research},
  day = {5}
}
</pre>

<a name="ragano2022automaticarchives"></a><pre>
@article{<a href="pubs2022_raw.html#ragano2022automaticarchives">ragano2022automaticarchives</a>,
  author = {Ragano, A and Benetos, E and Hines, A},
  journal = {Journal of the Audio Engineering Society},
  month = {Apr},
  number = {4},
  pages = {252--270},
  publisher = {Audio Engineering Society},
  title = {Automatic Quality Assessment of Digitized and Restored Sound Archives},
  volume = {70},
  year = {2022},
  abstract = {Archiving digital audio is conducted to preserve and make records accessible. However, techniques for assessing the quality of experience (QoE) of sound archives are usually neglected. In this paper, we present a framework to assess the QoE of sound archives in an automatic fashion. We describe the QoE influence factors, stakeholders, and audio archive degradations and explore the above concepts through a case study on the NASA Apollo audio archive. Each component of the framework is described in the audio archive lifecycle based on digitization, restoration, and consumption. We provide insights and real-world examples on why digitized and restored audio archives benefit from QoE assessment techniques similar to other multimedia applications such as video calling and streaming services. The reasons why stakeholders such as archivists, broadcasters, or public listeners would benefit from our proposed framework are also provided.},
  doi = {10.17743/jaes.2022.0002},
  issn = {0004-7554},
  day = {1},
  publicationstatus = {accepted}
}
</pre>

<a name="wang2022adaptiverecognition"></a><pre>
@article{<a href="pubs2022_raw.html#wang2022adaptiverecognition">wang2022adaptiverecognition</a>,
  author = {Wang, C and Benetos, E and Lostanlen, V and Chew, E},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
  month = {Mar},
  pages = {1407--1421},
  publisher = {Institute of Electrical and Electronics Engineers},
  title = {Adaptive Scattering Transforms for Playing Technique Recognition},
  url = {https://changhongw.github.io/},
  volume = {30},
  year = {2022},
  abstract = {Playing techniques contain distinctive information about musical expressivity and interpretation. Yet, current research in music signal analysis suffers from a scarcity of computational models for playing techniques, especially in the context of live performance. To address this problem, our paper develops a general framework for playing technique recognition. We propose the adaptive scattering transform, which refers to any scattering transform that includes a stage of data-driven dimensionality reduction over at least one of its wavelet variables, for representing playing techniques. Two adaptive scattering features are presented: frequency-adaptive scattering and direction-adaptive scattering. We analyse seven playing techniques: vibrato, tremolo, trill, flutter-tongue, acciaccatura, portamento, and glissando. To evaluate the proposed methodology, we create a new dataset containing full-length Chinese bamboo flute performances (CBFdataset) with expert playing technique annotations. Once trained on the proposed scattering representations, a support vector classifier achieves state-of-the-art results. We provide explanatory visualisations of scattering coefficients for each technique and verify the system over three additional datasets with various instrumental and vocal techniques: VPset, SOL, and VocalSet.},
  doi = {10.1109/TASLP.2022.3156785},
  issn = {2329-9304},
  keyword = {music performance analysis},
  keyword = {music signal analysis},
  keyword = {scattering transform},
  day = {7},
  publicationstatus = {published}
}
</pre>

<a name="benetos2022measuringdata"></a><pre>
@article{<a href="pubs2022_raw.html#benetos2022measuringdata">benetos2022measuringdata</a>,
  author = {Benetos, E and Ragano, A and Sgroi, D and Tuckwell, A},
  journal = {Behavior Research Methods},
  month = {Feb},
  publisher = {Springer (part of Springer Nature)},
  title = {Measuring national mood with music: using machine learning to construct a measure of national valence from audio data},
  year = {2022},
  abstract = {We propose a new measure of national valence based on the emotional content of a country’s most popular songs. We first trained a machine learning model using 191 different audio features embedded within music and use this model to construct a long-run valence index for the UK. This index correlates strongly and significantly with survey-based life satisfaction and outperforms an equivalent text-based measure. Our methods have the potential to be applied widely and to provide a solution to the severe lack of historical time-series data on psychological well-being.},
  doi = {10.3758/s13428-021-01747-7},
  issn = {1554-351X},
  day = {25},
  publicationstatus = {published}
}
</pre>

<a name="terenzi2022comparisonactivity"></a><pre>
@article{<a href="pubs2022_raw.html#terenzi2022comparisonactivity">terenzi2022comparisonactivity</a>,
  author = {Terenzi, A and Ortolani, N and De Almeida Nolasco, I and Benetos, E and Cecchi, S},
  journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing},
  month = {Jan},
  pages = {112--122},
  publisher = {Institute of Electrical and Electronics Engineers},
  title = {Comparison of feature extraction methods for sound-based classification of honey bee activity},
  volume = {30},
  year = {2022},
  abstract = {Honey bees are one of the most important insects on the planet since they play a key role in the pollination services of both cultivated and spontaneous flora. Recent years have seen an increase in bee mortality which points out the necessity of intensive beehive monitoring in order to better understand this phenomenon and try to help these important insects. In this scenario, this work presents an algorithm for sound-based classification of honey bee activity reporting a preliminary comparison between various extracted features used separately as input to a convolutional neural network classifier. In particular, the orphaned colony situation has been considered using a dataset acquired in a real situation. Different experiments with different setups have been carried out in order to test the performance of the proposed system, and the results have confirmed its potentiality.},
  doi = {10.1109/TASLP.2021.3133194},
  issn = {2329-9304},
  keyword = {Convolutional neural networks},
  keyword = {feature extraction},
  keyword = {continuous wavelet transform},
  keyword = {Hilbert-Huang transform},
  keyword = {mel frequency cepstrum coefficients},
  keyword = {honey bees},
  keyword = {bioacoustics},
  keyword = {computational bioacoustic scene analysis},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="stowell2022computationalroadmap"></a><pre>
@article{<a href="pubs2022_raw.html#stowell2022computationalroadmap">stowell2022computationalroadmap</a>,
  author = {Stowell, D},
  journal = {PeerJ},
  month = {Mar},
  title = {Computational bioacoustics with deep learning: a review and roadmap},
  year = {2022},
  abstract = {Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.},
  doi = {10.7717/peerj.13152},
  eissn = {2167-8359},
  day = {21},
  publicationstatus = {published}
}
</pre>

<a name="linhart2022themammals"></a><pre>
@article{<a href="pubs2022_raw.html#linhart2022themammals">linhart2022themammals</a>,
  author = {Linhart, P and Mahamoud-Issa, M and Stowell, D and Blumstein, DT},
  journal = {Mammalian Biology},
  month = {Jan},
  title = {The potential for acoustic individual identification in mammals},
  year = {2022},
  abstract = {Many studies have revealed that animal vocalizations, including those from mammals, are individually distinctive. Therefore, acoustic identification of individuals (AIID) has been repeatedly suggested as a non-invasive and labor efficient alternative to mark-recapture identification methods. We present a pipeline of steps for successful AIID in a given species. By conducting such work, we will also improve our understanding of identity signals in general. Strong and stable acoustic signatures are necessary for successful AIID. We reviewed studies of individual variation in mammalian vocalizations as well as pilot studies using acoustic identification to census mammals and birds. We found the greatest potential for AIID (characterized by strong and stable acoustic signatures) was in Cetacea and Primates (including humans). In species with weaker acoustic signatures, AIID could still be a valuable tool once its limitations are fully acknowledged. A major obstacle for widespread utilization of AIID is the absence of tools integrating all AIID subtasks within a single package. Automation of AIID could be achieved with the use of advanced machine learning techniques inspired by those used in human speaker recognition or tailored to specific challenges of animal AIID. Unfortunately, further progress in this area is currently hindered by the lack of appropriate publicly available datasets. However, we believe that after overcoming the issues outlined above, AIID can quickly become a widespread and valuable tool in field research and conservation of mammals and other animals.},
  doi = {10.1007/s42991-021-00222-2},
  issn = {1616-5047},
  eissn = {1618-1476},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="nolasco2022rankbasedrepresentations"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#nolasco2022rankbasedrepresentations">nolasco2022rankbasedrepresentations</a>,
  author = {Nolasco, I and Stowell, D},
  conference = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  month = {Jan},
  pages = {3623--3627},
  title = {RANK-BASED LOSS FOR LEARNING HIERARCHICAL REPRESENTATIONS},
  volume = {2022-May},
  year = {2022},
  abstract = {Hierarchical taxonomies are common in many contexts, and they are a very natural structure humans use to organise information. In machine learning, the family of methods that use this”extra” information is called hierarchical classification. However, applied to audio classification, this remains relatively unexplored. Here we focus on how to integrate the hierarchical information of a problem to learn embeddings representative of the hierarchical relationships. Previously, triplet loss has been proposed to address this problem, however it presents some issues like requiring the careful construction of the triplets, and being limited in the extent of hierarchical information it uses at each iteration. In this work we propose a rank based loss function that uses hierarchical information and translates this into a rank ordering of target distances between the examples. We show that rank based loss is suitable to learn hierarchical representations of the data. By testing on unseen fine level classes we show that this method is also capable of learning hierarchically correct representations of the new classes. Rank based loss has two promising aspects, it is generalisable to hierarchies with any number of levels, and is capable of dealing with data with incomplete hierarchical labels.},
  doi = {10.1109/ICASSP43922.2022.9746907},
  isbn = {9781665405409},
  issn = {1520-6149},
  day = {1},
  publicationstatus = {published}
}
</pre>

<a name="alvaradodurangaussiananalysis"></a><pre>
@inproceedings{<a href="pubs2022_raw.html#alvaradodurangaussiananalysis">alvaradodurangaussiananalysis</a>,
  author = {Alvarado Duran, PA and STOWELL, DF},
  conference = {},
  title = {Gaussian Processes for Music Audio Modelling and Content Analysis},
  year = {},
  abstract = {Real music signals are highly variable, yet they have strong statistical
structure. Prior information about the underlying physical mechanisms
by which sounds are generated and rules by which complex sound struc-
ture is constructed (notes, chords, a complete musical score), can be nat-
urally unified using Bayesian modelling techniques. Typically algorithms
for Automatic Music Transcription independently carry out individual
tasks such as multiple-F0 detection and beat tracking. The challenge
remains to perform joint estimation of all parameters. We present a Ba-
yesian approach for modelling music audio, and content analysis. The
proposed methodology based on Gaussian processes seeks joint estima-
tion of multiple music concepts by incorporating into the kernel prior
information about non-stationary behaviour, dynamics, and rich spectral
content present in the modelled music signal. We illustrate the benefits
of this approach via two tasks: pitch estimation, and inferring missing
segments in a polyphonic audio recording.},
  booktitle = {IEEE International Workshop on Machine Learning for Signal Processing (MLSP)},
  publicationstatus = {published}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.96.</em></p>
