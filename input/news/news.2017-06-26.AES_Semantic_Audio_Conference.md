Last week saw the <a href="http://www.aes.org/conferences/2017/semantic/">2017 International Conference on Semantic Audio</a> by the Audio Engineering Society. Held at <a href="https://www.iis.fraunhofer.de/">Fraunhofer Institute for Integrated Circuits</a> in Erlangen, Germany, delegates enjoyed a well-organised and high-quality programme, interleaved with social and networking events such as a jazz concert and a visit to Erlangen’s famous beer cellars. The conference was a combined effort of Fraunhofer IIS, Friedrich-Alexander Universität, and their joint venture Audio Labs.

With 5 papers and a late-breaking demo, the Centre for Digital Music was the most strongly represented institution, surpassing even the hosting organisations combined. Our group has a strong tradition in Semantic Audio: director Mark Sandler is Vice Chair on the [AES Technical Committee on Semantic Audio Analysis](http://www.aes.org/technical/saa/), and several more of us are members; and we hosted the [previous Semantic Audio conference](http://www.aes.org/conferences/53/). 

Most notably, Rodrigo Schramm and Emmanouil Benetos won the Best Paper award for their paper "<a href="http://www.aes.org/e-lib/browse.cfm?elib=18757">Automatic Transcription of a Cappella recordings from Multiple Singers</a>".

Emmanouil further presented another paper, "<a href="http://www.aes.org/e-lib/browse.cfm?elib=18760">Polyphonic Note and Instrument Tracking Using Linear Dynamical Systems</a>", and coauthored "<a href="http://www.aes.org/e-lib/browse.cfm?elib=18774">Assessing the Relevance of Onset Information for Note Tracking in Piano Music Transcription</a>".

Adán Benito presented "<a href="http://www.aes.org/e-lib/browse.cfm?elib=18766">Intelligent Multitrack Reverberation Based on Hinge-Loss Markov Random Fields</a>", a machine learning approach to automatic application of a reverb effect to musical audio.

Delia Fano Yela delivered a beautifully hand-drawn and compelling presentation about source separation in general and how temporal context can be employed to considerably improve vocal extraction (<a href="http://www.aes.org/e-lib/browse.cfm?elib=18752">"On the Importance of Temporal Context in Proximity Kernels: A Vocal Separation Case Study"</a>).

Brecht De Man demoed the "<a href="http://c4dm.eecs.qmul.ac.uk/multitrack/MixEvaluation/">Mix Evaluation Browser</a>", an online interface to access a dataset comprising several mixes of a number of songs, complete with corresponding DAW files, raw tracks, preference ratings, and annotated comments from subjective listening tests.

&nbsp;

Several other delegates were frequent collaborators or previously affiliated with Queen Mary. The opening keynote was delivered by Mark Plumbley, former director of the Centre for Digital Music, who gave an overview of the field of machine listening, specifically audio event detection and scene recognition. Nick Jillings, formerly research assistant and master project student at the Audio Engineering group, and currently a PhD student at Birmingham City University cosupervised by Josh Reiss, head of our Audio Engineering group, presented his paper "<a href="http://www.aes.org/e-lib/browse.cfm?elib=18770">Investigating Music Production Using a Semantically Powered Digital Audio Workstation in the Browser</a>" and demoed "Automatic channel routing using musical instrument linked data".

Other keynotes were delivered by Udo Zölzer, best known from editing the collection “DAFX: Digital Audio Effects”, and Masataka Goto, a household name in the MIR community who discussed his own web-based implementations of music discovery and visualisation.

Paper proceedings are already available in the <a href="http://www.aes.org/e-lib/online/search.cfm?type=paper&amp;title=&amp;conference=Conference%3A%202017%20AES%20International%20Conference%20on%20Semantic%20Audio">AES E-library</a>, free for AES members.
