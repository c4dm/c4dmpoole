
<!-- This document was automatically generated with bibtex2html 1.99
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html -nodoc -dl -a -noabstract -nokeywords -o pubs2019_raw publications_bibtex/pubs2019.bib  -->


<dl>

<dt>
[<a name="agrawal2019aalignment">1</a>]
</dt>
<dd>
R&nbsp;Agrawal and S&nbsp;Dixon.
 A hybrid approach to audio-to-score alignment.
 May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#agrawal2019aalignment">bib</a>&nbsp;]

</dd>


<dt>
[<a name="albert2019drawinganalysis">2</a>]
</dt>
<dd>
S&nbsp;Albert, C&nbsp;Heath, S&nbsp;Skach, MT&nbsp;Harris, M&nbsp;Miller, and PGT Healey.
 Drawing as transcription: how do graphical techniques inform
  interaction analysis?
 <em>Social Interaction Video-Based Studies of Human Sociality</em>,
  2(1), Mar 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#albert2019drawinganalysis">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.7146/si.v2i1.113145">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="allik2019joinstreaming">3</a>]
</dt>
<dd>
A&nbsp;Allik, F&nbsp;Thalmann, C&nbsp;Metzig, and M&nbsp;Sandler.
 Join my party! how can we enhance social interactions in music
  streaming?
 Trondheim, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#allik2019joinstreaming">bib</a>&nbsp;]

</dd>


<dt>
[<a name="alvarado2019sparsetimedomain">4</a>]
</dt>
<dd>
PA&nbsp;Alvarado, MA&nbsp;Alvarez, and D&nbsp;Stowell.
 Sparse gaussian process audio source separation using spectrum priors
  in the time-domain.
 In <em>ICASSP, IEEE International Conference on Acoustics, Speech
  and Signal Processing - Proceedings</em>, volume 2019-May, pages 995--999, Apr
  2019.
[&nbsp;<a href="pubs2019_raw_bib.html#alvarado2019sparsetimedomain">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8683287">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="armendariz2019cosimlexcontext">5</a>]
</dt>
<dd>
CS&nbsp;Armendariz, M&nbsp;Purver, M&nbsp;Ulčar, S&nbsp;Pollak, N&nbsp;Ljubešić, M&nbsp;Robnik-Šikonja,
  M&nbsp;Granroth-Wilding, and K&nbsp;Vaik.
 Cosimlex: A resource for evaluating graded word similarity in
  context, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#armendariz2019cosimlexcontext">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1912.05320v2">http</a>&nbsp;]

</dd>


<dt>
[<a name="armitage2019bricolagestudy">6</a>]
</dt>
<dd>
J&nbsp;Armitage and A&nbsp;Mcpherson.
 Bricolage in a hybrid digital lutherie context: a workshop study.
 University of Nottingham, ACM, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#armitage2019bricolagestudy">bib</a>&nbsp;| 
<a href="http://acm.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="bauer2019designingreality">7</a>]
</dt>
<dd>
V&nbsp;Bauer, A&nbsp;Nagele, C&nbsp;Baume, T&nbsp;Cowlishaw, H&nbsp;Cooke, C&nbsp;Pike, and PGT Healey.
 Designing an interactive and collaborative experience in audio
  augmented reality.
 In <em>Lecture Notes in Computer Science (including subseries
  Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)</em>, volume 11883 LNCS, pages 305--311, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#bauer2019designingreality">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-030-31908-3_20">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="bear2019cityscenes">8</a>]
</dt>
<dd>
H&nbsp;Bear, T&nbsp;Heittola, A&nbsp;Mesaros, E&nbsp;Benetos, and T&nbsp;Virtanen.
 City classification from multiple real-world sound scenes.
 In <em>http://www.waspaa.com/</em>, pages 11--15. New Paltz, NY, USA,
  IEEE, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#bear2019cityscenes">bib</a>&nbsp;]

</dd>


<dt>
[<a name="bear2019towardsrecognition">9</a>]
</dt>
<dd>
H&nbsp;Bear, I&nbsp;Nolasco, and E&nbsp;Benetos.
 Towards joint sound scene and polyphonic sound event recognition.
 pages 4594--4598. Graz, Austria, International Speech Communication
  Association (ISCA), Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#bear2019towardsrecognition">bib</a>&nbsp;| 
<a href="https://www.interspeech2019.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="benetos2019automaticoverview">10</a>]
</dt>
<dd>
E&nbsp;BENETOS, S&nbsp;DIXON, Z&nbsp;Duan, and S&nbsp;EWERT.
 Automatic music transcription: An overview.
 <em>IEEE Signal Processing Magazine</em>, 36(1):20--30, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#benetos2019automaticoverview">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/MSP.2018.2869928">DOI</a>&nbsp;| 
<a href="http://www.eecs.qmul.ac.uk/~emmanouilb/">http</a>&nbsp;]

</dd>


<dt>
[<a name="bromham2019thewarmth">11</a>]
</dt>
<dd>
G&nbsp;BROMHAM, D&nbsp;Moffat, M&nbsp;Barthet, A&nbsp;Daneilsen, and G&nbsp;Fazekas.
 The impact of audio effects processing on the perception of
  brightness and warmth.
 Nottingham, UK, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#bromham2019thewarmth">bib</a>&nbsp;]

</dd>


<dt>
[<a name="bruford2019modellingevaluation">12</a>]
</dt>
<dd>
F&nbsp;Bruford, M&nbsp;Barthet, S&nbsp;McDonald, and M&nbsp;Sandler.
 Modelling musical similarity for drum patterns: A perceptual
  evaluation.
 In <em>ACM International Conference Proceeding Series</em>, pages
  131--138, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#bruford2019modellingevaluation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3356590.3356611">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="bruford2019groovenavigation">13</a>]
</dt>
<dd>
F&nbsp;Bruford, ST&nbsp;McDonald, M&nbsp;Barthet, and M&nbsp;Sandler.
 Groove explorer: An intelligent visual interface for drum loop
  library navigation.
 In <em>CEUR Workshop Proceedings</em>, volume 2327, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#bruford2019groovenavigation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="caetano2019audiotimbre">14</a>]
</dt>
<dd>
M&nbsp;Caetano, C&nbsp;Saitis, and K&nbsp;Siedenburg.
 Audio content descriptors of timbre.
 In <em>Timbre: Acoustics, Perception, and Cognition</em>, volume&nbsp;69,
  pages 297--333. Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#caetano2019audiotimbre">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-030-14832-4_11">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="cameron2019neuralrhythms">15</a>]
</dt>
<dd>
DJ&nbsp;Cameron, I&nbsp;Zioga, JP&nbsp;Lindsen, MT&nbsp;Pearce, GA&nbsp;Wiggins, K&nbsp;Potter, and
  J&nbsp;Bhattacharya.
 Neural entrainment is associated with subjective groove and
  complexity for performed but not mechanical musical rhythms.
 <em>Experimental Brain Research</em>, 237(8):1981--1991, Aug 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#cameron2019neuralrhythms">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s00221-019-05557-4">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="chettri2019ensembleverification">16</a>]
</dt>
<dd>
B&nbsp;Chettri, D&nbsp;Stoller, V&nbsp;Morfi, M&nbsp;Martinez&nbsp;Ramirez, E&nbsp;Benetos, and B&nbsp;Sturm.
 Ensemble models for spoofing detection in automatic speaker
  verification.
 pages 1018--1022. Graz, Austria, International Speech Communication
  Association (ISCA), Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#chettri2019ensembleverification">bib</a>&nbsp;| 
<a href="https://www.interspeech2019.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="cheung2019uncertaintyactivity">17</a>]
</dt>
<dd>
V&nbsp;Cheung, PMC HARRISON, L&nbsp;Meyer, M&nbsp;Pearce, J-D Haynes, and S&nbsp;Koelsch.
 Uncertainty and surprise jointly predict musical pleasure and
  amygdala, hippocampus, and auditory cortex activity.
 <em>Current Biology</em>, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#cheung2019uncertaintyactivity">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.cub.2019.09.067">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="chourdakis2019modellingmix">18</a>]
</dt>
<dd>
E&nbsp;Chourdakis, L&nbsp;Ward, M&nbsp;Paradis, and JD&nbsp;Reiss.
 Modelling experts’ decisions on assigning narrative importances of
  objects in a radio drama mix.
 http://dafx2019.bcu.ac.uk/programme/tue/oral-session-3, Sep 2019.
  Birmigham, UK.
[&nbsp;<a href="pubs2019_raw_bib.html#chourdakis2019modellingmix">bib</a>&nbsp;]

</dd>


<dt>
[<a name="chourdakis2019taggingreverberation">19</a>]
</dt>
<dd>
ET&nbsp;Chourdakis and JD&nbsp;Reiss.
 Tagging and retrieval of room impulse responses using semantic word
  vectors and perceptual measures of reverberation.
 In <em>AES 146th International Convention</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#chourdakis2019taggingreverberation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="clark2019mappinginteractions">20</a>]
</dt>
<dd>
L&nbsp;Clark, BR&nbsp;Cowan, J&nbsp;Edwards, J&nbsp;Edlund, E&nbsp;Szekely, C&nbsp;Munteanu, C&nbsp;Murad,
  P&nbsp;Healey, M&nbsp;Aylett, N&nbsp;Harte, I&nbsp;Torre, RK&nbsp;Moore, and P&nbsp;Doyle.
 Mapping theoretical and methodological perspectives for understanding
  speech interface interactions.
 In <em>Conference on Human Factors in Computing Systems -
  Proceedings</em>, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#clark2019mappinginteractions">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3290607.3299009">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="colonel2019exploringfeatures">21</a>]
</dt>
<dd>
J&nbsp;Colonel and J&nbsp;Reiss.
 Exploring preference for multitrack mixes using statistical analysis
  of mir and textual features.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#colonel2019exploringfeatures">bib</a>&nbsp;]

</dd>


<dt>
[<a name="covas2019optimalforecasting">22</a>]
</dt>
<dd>
E&nbsp;Covas and E&nbsp;Benetos.
 Optimal neural network feature selection for spatial-temporal
  forecasting.
 <em>Chaos</em>, 29(6), Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#covas2019optimalforecasting">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1063/1.5095060">DOI</a>&nbsp;| 
<a href="https://aip.scitation.org/journal/cha">http</a>&nbsp;]

</dd>


<dt>
[<a name="dai2019intonationsinging">23</a>]
</dt>
<dd>
J&nbsp;Dai and S&nbsp;Dixon.
 Intonation trajectories within tones in unaccompanied soprano, alto,
  tenor, bass quartet singing.
 <em>J Acoust Soc Am</em>, 146(2):1005--1005, Aug 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#dai2019intonationsinging">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1121/1.5120483">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/31472532">http</a>&nbsp;]

</dd>


<dt>
[<a name="dai2019singingsinging">24</a>]
</dt>
<dd>
J&nbsp;Dai and S&nbsp;Dixon.
 Singing together: Pitch accuracy and interaction in unaccompanied
  unison and duet singing.
 <em>J Acoust Soc Am</em>, 145(2):663--663, Feb 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#dai2019singingsinging">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1121/1.5087817">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/30823797">http</a>&nbsp;]

</dd>


<dt>
[<a name="dai2019understandingnotes">25</a>]
</dt>
<dd>
J&nbsp;Dai and S&nbsp;Dixon.
 Understanding intonation trajectories and patterns of vocal notes.
 In <em>Lecture Notes in Computer Science (including subseries
  Lecture Notes in Artificial Intelligence and Lecture Notes in
  Bioinformatics)</em>, volume 11296 LNCS, pages 243--253, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#dai2019understandingnotes">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-030-05716-9_20">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="defleurian2019rewardpleasure">26</a>]
</dt>
<dd>
R&nbsp;de&nbsp;Fleurian, PMC Harrison, MT&nbsp;Pearce, and DR&nbsp;Quiroga-Martinez.
 Reward prediction tells us less than expected about musical pleasure.
 <em>Proc Natl Acad Sci U S A</em>, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#defleurian2019rewardpleasure">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1073/pnas.1913244116">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/31537748">http</a>&nbsp;]

</dd>


<dt>
[<a name="deacon2019shapingcomposition">27</a>]
</dt>
<dd>
T&nbsp;Deacon, N&nbsp;Bryan-Kinns, PGT Healey, and M&nbsp;Barthet.
 Shaping sounds: The role of gesture in collaborative spatial music
  composition.
 In <em>C and C 2019 - Proceedings of the 2019 Creativity and
  Cognition</em>, pages 121--132, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#deacon2019shapingcomposition">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3325480.3325493">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="delgado2019aanalysis">28</a>]
</dt>
<dd>
A&nbsp;Delgado, S&nbsp;McDonald, N&nbsp;Xu, and M&nbsp;Sandler.
 A new dataset for amateur vocal percussion analysis.
 In <em>ACM International Conference Proceeding Series</em>, pages
  17--23, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#delgado2019aanalysis">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3356590.3356844">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="demirel2019chordscale">29</a>]
</dt>
<dd>
Emir Demirel, Baris Bozkurt, and Xavier Serra.
 Automatic chord-scale recognition using harmonic pitch class
  profiles.
 In <em>Barbancho I, Tard&oacute;n LJ, Peinado A, Barbancho AM, editors.
  Proceedings of the 16th Sound &amp; Music Computing Conference; 2019 May 28-31;
  M&aacute;laga, Spain.[M&aacute;laga]: SMC; 2019.</em> Sound &amp; Music Computing
  Conference, 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#demirel2019chordscale">bib</a>&nbsp;| 
<a href="https://zenodo.org/record/3249258">http</a>&nbsp;]

</dd>


<dt>
[<a name="drooghayes2019detectingfeatures">30</a>]
</dt>
<dd>
M&nbsp;DROOG-HAYES, GA&nbsp;WIGGINS, and MRJ PURVER.
 Detecting summary-worthy sentences: the effect of discourse features.
 Newport Beach, CA, Feb 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#drooghayes2019detectingfeatures">bib</a>&nbsp;]

</dd>


<dt>
[<a name="feng2019augmentedrehabilitation">31</a>]
</dt>
<dd>
F&nbsp;Feng and T&nbsp;Stockman.
 Augmented visuotactile feedback support sensorimotor synchronization
  skill for rehabilitation.
 In <em>Conference on Human Factors in Computing Systems -
  Proceedings</em>, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#feng2019augmentedrehabilitation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3290607.3312812">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="foster2019reproducingsynthesis">32</a>]
</dt>
<dd>
D&nbsp;Foster and JD&nbsp;Reiss.
 Reproducing bass guitar performances using descriptor driven
  synthesis.
 In <em>AES 146th International Convention</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#foster2019reproducingsynthesis">bib</a>&nbsp;]

</dd>


<dt>
[<a name="frieler2019dontalgorithms">33</a>]
</dt>
<dd>
K&nbsp;Frieler, D&nbsp;Basaran, F&nbsp;Höger, HC&nbsp;Crayencour, G&nbsp;Peeters, and S&nbsp;Dixon.
 Don't hide in the frames: Note-and pattern-based evaluation of
  automated melody extraction algorithms.
 In <em>ACM International Conference Proceeding Series</em>, pages
  25--32, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#frieler2019dontalgorithms">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3358664.3358672">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="galindoesparza2019embodiedtechnology">34</a>]
</dt>
<dd>
RP&nbsp;Galindo&nbsp;Esparza, PGT Healey, L&nbsp;Weaver, and M&nbsp;Delbridge.
 Embodied imagination: An approach to stroke recovery combining
  participatory performance and interactive technology.
 In <em>Conference on Human Factors in Computing Systems -
  Proceedings</em>, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#galindoesparza2019embodiedtechnology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3290605.3300735">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="gold2019predictabilitylearning">35</a>]
</dt>
<dd>
B&nbsp;Gold, M&nbsp;Pearce, E&nbsp;Mas-Herrero, A&nbsp;Dagher, and RJ&nbsp;Zatorre.
 Predictability and uncertainty in the pleasure of music: a reward for
  learning?
 <em>The Journal of Neuroscience</em>, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#gold2019predictabilitylearning">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1523/JNEUROSCI.0428-19.2019">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="harrison2019accessiblewild">36</a>]
</dt>
<dd>
J&nbsp;Harrison, A&nbsp;Chamberlain, and AP&nbsp;McPherson.
 Accessible instruments in the wild.
 pages 1--6, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#harrison2019accessiblewild">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3290607.3313037">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hazzard2019failingperformance">37</a>]
</dt>
<dd>
A&nbsp;Hazzard, C&nbsp;Greenhalgh, M&nbsp;Kallionpaa, S&nbsp;Benford, A&nbsp;Veinberg, Z&nbsp;Kanga, and
  A&nbsp;McPherson.
 Failing with style: Designing for aesthetic failure in interactive
  performance.
 page&nbsp;30, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#hazzard2019failingperformance">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3290605.3300260">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="holland2019understandingmatters">38</a>]
</dt>
<dd>
S&nbsp;Holland, T&nbsp;Mudd, K&nbsp;Wilkie-McKenna, A&nbsp;McPherson, and MM&nbsp;Wanderley.
 Understanding music interaction, and why it matters.
 In <em>New Directions in Music and Human-Computer Interaction</em>,
  pages 1--20. Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#holland2019understandingmatters">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-92069-6_1">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="holzapfel2019automaticstudy">39</a>]
</dt>
<dd>
A&nbsp;Holzapfel and E&nbsp;Benetos.
 Automatic music transcription and ethnomusicology: a user study.
 pages 678--684. Delft, The Netherlands, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#holzapfel2019automaticstudy">bib</a>&nbsp;| 
<a href="https://ismir2019.ewi.tudelft.nl/">http</a>&nbsp;]

</dd>


<dt>
[<a name="kudumakis2019mpegpersonalization">40</a>]
</dt>
<dd>
P&nbsp;Kudumakis, T&nbsp;Wilmering, M&nbsp;Sandler, and J&nbsp;Foss.
 Mpeg ipr ontologies for media trading and personalization.
 volume 2423. Manchester, UK, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#kudumakis2019mpegpersonalization">bib</a>&nbsp;| 
<a href="http://ceur-ws.org/Vol-2423/DataTV2019_paper_2.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="lepri2019fictionalprototypes">41</a>]
</dt>
<dd>
G&nbsp;LEPRI and A&nbsp;MCPHERSON.
 Fictional instruments, real values: Discovering musical backgrounds
  with non-functional prototypes.
 Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#lepri2019fictionalprototypes">bib</a>&nbsp;]

</dd>


<dt>
[<a name="lepri2019makingpractice">42</a>]
</dt>
<dd>
G&nbsp;LEPRI and A&nbsp;MCPHERSON.
 Making up instruments: Design fiction for value discovery in
  communities of musical practice.
 San Diego California USA, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#lepri2019makingpractice">bib</a>&nbsp;| 
<a href="http://www.giacomolepri.com/">http</a>&nbsp;]

</dd>


<dt>
[<a name="li2019aphrase">43</a>]
</dt>
<dd>
S&nbsp;Li, S&nbsp;Dixon, DAA Black, and MD&nbsp;Plumbley.
 A model selection test for factors affecting the choice of expressive
  timing clusters for a phrase.
 In <em>SMC 2016 - 13th Sound and Music Computing Conference,
  Proceedings</em>, pages 247--252, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#li2019aphrase">bib</a>&nbsp;]

</dd>


<dt>
[<a name="liang2019analysissensors">44</a>]
</dt>
<dd>
A&nbsp;Liang, R&nbsp;Stewart, and N&nbsp;Bryan-Kinns.
 Analysis of sensitivity, linearity, hysteresis, responsiveness, and
  fatigue of textile knit stretch sensors.
 <em>Sensors (Switzerland)</em>, 19(16), Aug 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#liang2019analysissensors">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3390/s19163618">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="liang2019effectproperties">45</a>]
</dt>
<dd>
A&nbsp;Liang, R&nbsp;Stewart, R&nbsp;Freire, and N&nbsp;Bryan-Kinns.
 Effect of bonding and washing on electronic textile stretch sensor
  properties.
 In <em>UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM
  International Joint Conference on Pervasive and Ubiquitous Computing and
  Proceedings of the 2019 ACM International Symposium on Wearable Computers</em>,
  pages 121--124, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#liang2019effectproperties">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3341162.3343817">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="liang2019pianonetworks">46</a>]
</dt>
<dd>
B&nbsp;Liang, G&nbsp;Fazekas, and M&nbsp;Sandler.
 Piano sustain-pedal detection using convolutional neural networks.
 In <em>ICASSP, IEEE International Conference on Acoustics, Speech
  and Signal Processing - Proceedings</em>, volume 2019-May, pages 241--245, May
  2019.
[&nbsp;<a href="pubs2019_raw_bib.html#liang2019pianonetworks">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8683505">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="liang2019transferdetection">47</a>]
</dt>
<dd>
B&nbsp;Liang, G&nbsp;Fazekas, and M&nbsp;Sandler.
 Transfer learning for piano sustain-pedal detection.
 In <em>Proceedings of the International Joint Conference on Neural
  Networks</em>, volume 2019-July, Jul 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#liang2019transferdetection">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/IJCNN.2019.8851724">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="lins2019automaticrecordings">48</a>]
</dt>
<dd>
F&nbsp;Lins, M&nbsp;Johann, E&nbsp;BENETOS, and R&nbsp;Schramm.
 Automatic transcription of diatonic harmonica recordings.
 Brighton, UK, IEEE, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#lins2019automaticrecordings">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8682334">DOI</a>&nbsp;| 
<a href="https://2019.ieeeicassp.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="liu2019automaticrepresentation">49</a>]
</dt>
<dd>
L&nbsp;Liu and E&nbsp;Benetos.
 Automatic music accompaniment with a chroma-based music data
  representation, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#liu2019automaticrepresentation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="martinezramirez2019modelingnetworks">50</a>]
</dt>
<dd>
M&nbsp;Martinez&nbsp;Ramirez and J&nbsp;Reiss.
 Modeling nonlinear audio effects with end-to-end deep neural
  networks.
 May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#martinezramirez2019modelingnetworks">bib</a>&nbsp;| 
<a href="http://www.m-marco.com/">http</a>&nbsp;]

</dd>


<dt>
[<a name="martnvide2019prefacepreface">51</a>]
</dt>
<dd>
C&nbsp;Martín-Vide, S&nbsp;Pollak, and M&nbsp;Purver.
 <em>Preface</em>, volume 11816 LNAI.
 Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#martnvide2019prefacepreface">bib</a>&nbsp;]

</dd>


<dt>
[<a name="matt2019estimatingsignalling">52</a>]
</dt>
<dd>
A&nbsp;Matt and D&nbsp;Stowell.
 Estimating &amp;amp; mitigating the impact of acoustic environments on
  machine-to-machine signalling.
 In <em>European Signal Processing Conference</em>, volume
  2019-September, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#matt2019estimatingsignalling">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.23919/EUSIPCO.2019.8902634">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="mcginity2019theagents">53</a>]
</dt>
<dd>
MM&nbsp;McGinity, M&nbsp;Purver, and G&nbsp;Wiggins.
 The influence of cost on the emergence of a common language among
  cooperating agents.
 In <em>2019 AISB Convention</em>, pages 21--27, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mcginity2019theagents">bib</a>&nbsp;]

</dd>


<dt>
[<a name="mcgregor2019rerepresentingsemantics">54</a>]
</dt>
<dd>
SE&nbsp;MCGREGOR, K&nbsp;AGRES, K&nbsp;Rataj, MRJ PURVER, and GA&nbsp;WIGGINS.
 Re-representing metaphor: Modelling metaphor perception using
  dynamically contextual distributional semantics.
 <em>Frontiers in Psychology</em>, Apr 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mcgregor2019rerepresentingsemantics">bib</a>&nbsp;]

</dd>


<dt>
[<a name="mcpherson2019musicbenford">55</a>]
</dt>
<dd>
A&nbsp;McPherson and S&nbsp;Benford.
 Music, design and ethnography: An interview with steve benford.
 In <em>New Directions in Music and Human-Computer Interaction</em>,
  pages 213--220. Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mcpherson2019musicbenford">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-92069-6_13">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="mcpherson2019musicalapproaches">56</a>]
</dt>
<dd>
A&nbsp;MCPHERSON, F&nbsp;MORREALE, and J&nbsp;HARRISON.
 Musical instruments for novices: Comparing nime, hci and crowdfunding
  approaches.
 In S&nbsp;Holland, K&nbsp;Wilkie-McKenna, T&nbsp;Mudd, A&nbsp;MCPHERSON, and M&nbsp;Wanderley,
  editors, <em>New Directions in Music and Human-Computer Interaction</em>.
  Springer, 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mcpherson2019musicalapproaches">bib</a>&nbsp;]

</dd>


<dt>
[<a name="mcpherson2019theverplank">57</a>]
</dt>
<dd>
A&nbsp;McPherson and B&nbsp;Verplank.
 The poetry of strange connections: An interview with bill verplank.
 In <em>New Directions in Music and Human-Computer Interaction</em>,
  pages 61--70. Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mcpherson2019theverplank">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-319-92069-6_4">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="mehrabi2019erratumdoi101371journalpone0219955">58</a>]
</dt>
<dd>
A&nbsp;Mehrabi, S&nbsp;Dixon, and M&nbsp;Sandler.
 Erratum: Vocal imitation of percussion sounds: On the perceptual
  similarity between imitations and imitated sounds (plos one (2019)14:8
  (e0221722) doi:10.1371/journal.pone.0219955).
 <em>PLoS One</em>, 14(8), Aug 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mehrabi2019erratumdoi101371journalpone0219955">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1371/journal.pone.0221722">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="mehrabi2019vocalsounds">59</a>]
</dt>
<dd>
A&nbsp;Mehrabi, S&nbsp;Dixon, and M&nbsp;Sandler.
 Vocal imitation of percussion sounds: On the perceptual similarity
  between imitations and imitated sounds.
 <em>PLoS One</em>, 14(7):e0219955--e0219955, Jul 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mehrabi2019vocalsounds">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1371/journal.pone.0219955">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/31344080">http</a>&nbsp;]

</dd>


<dt>
[<a name="men2019lemocreativity">60</a>]
</dt>
<dd>
L&nbsp;Men and N&nbsp;Bryan-Kinns.
 Lemo: Exploring virtual space for collaborative creativity.
 In <em>C and C 2019 - Proceedings of the 2019 Creativity and
  Cognition</em>, pages 71--82, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#men2019lemocreativity">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3325480.3325495">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="men2019designingenvironments">61</a>]
</dt>
<dd>
L&nbsp;Men, N&nbsp;Bryan-Kinns, and L&nbsp;Bryce.
 Designing spaces to support collaborative creativity in shared
  virtual environments.
 <em>PeerJ Computer Science</em>, 5:1--39, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#men2019designingenvironments">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.7717/peerj-cs.229">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="mice2019embodiedinstruments">62</a>]
</dt>
<dd>
L&nbsp;Mice and AP&nbsp;Mcpherson.
 Embodied cognition in performers of large acoustic instruments as a
  method of designing new large digital musical instruments.
 In L&nbsp;Mice and AP&nbsp;Mcpherson, editors, <em>14th International
  Symposium on Computer Music Multidisciplinary Research</em>, Marseille, France,
  Oct 2019. Marseille, France.
[&nbsp;<a href="pubs2019_raw_bib.html#mice2019embodiedinstruments">bib</a>&nbsp;| 
<a href="http://www.mat.qmul.ac.uk/students/lia-mice/">http</a>&nbsp;]

</dd>


<dt>
[<a name="mishra2019ganbasednetworks">63</a>]
</dt>
<dd>
S&nbsp;MISHRA, D&nbsp;STOLLER, E&nbsp;BENETOS, B&nbsp;STURM, and S&nbsp;DIXON.
 Gan-based generation and automatic selection of explanations for
  neural networks.
 In <em>https://sites.google.com/view/safeml-iclr2019</em>. New Orleans,
  USA, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mishra2019ganbasednetworks">bib</a>&nbsp;| 
<a href="https://sites.google.com/site/saumitramishrac4dm/">http</a>&nbsp;]

</dd>


<dt>
[<a name="moffat2019approachesproduction">64</a>]
</dt>
<dd>
D&nbsp;Moffat and M&nbsp;Sandler.
 Approaches in intelligent music production.
 <em>Arts</em>, 8(4), Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#moffat2019approachesproduction">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3390/arts8040125">DOI</a>&nbsp;| 
<a href="https://www.mdpi.com/2076-0752/8/4/125">http</a>&nbsp;]

</dd>


<dt>
[<a name="moffat2019anreverberation">65</a>]
</dt>
<dd>
D&nbsp;Moffat and MB&nbsp;Sandler.
 An automated approach to the application of reverberation.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#moffat2019anreverberation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="moffat2019automaticidentification">66</a>]
</dt>
<dd>
D&nbsp;Moffat and MB&nbsp;Sandler.
 Automatic mixing level balancing enhanced through source interference
  identification.
 In <em>AES 146th International Convention</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#moffat2019automaticidentification">bib</a>&nbsp;]

</dd>


<dt>
[<a name="moffat2019machinedrums">67</a>]
</dt>
<dd>
D&nbsp;Moffat and MB&nbsp;Sandler.
 Machine learning multitrack gain mixing of drums.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#moffat2019machinedrums">bib</a>&nbsp;]

</dd>


<dt>
[<a name="moffat2019soundsynthesis">68</a>]
</dt>
<dd>
D&nbsp;Moffat, R&nbsp;SELFRIDGE, and J&nbsp;Reiss.
 Sound effect synthesis.
 In M&nbsp;Filimowicz, editor, <em>Foundations in Sound Design for
  Interactive Media: A Multidisciplinary Approach</em>, number&nbsp;13 in Foundations in
  Sound Design. Routledge, 1 edition, Jul 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#moffat2019soundsynthesis">bib</a>&nbsp;]

</dd>


<dt>
[<a name="mourgela2019perceptuallyreference">69</a>]
</dt>
<dd>
A&nbsp;Mourgela, T&nbsp;Agus, and JD&nbsp;Reiss.
 Perceptually motivated hearing loss simulation for audio mixing
  reference.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mourgela2019perceptuallyreference">bib</a>&nbsp;]

</dd>


<dt>
[<a name="mycroft2019visuallymixing">70</a>]
</dt>
<dd>
J&nbsp;Mycroft, JD&nbsp;Reiss, and T&nbsp;Stockman.
 Visually representing and interpreting multivariate data for audio
  mixing.
 In <em>SMC 2016 - 13th Sound and Music Computing Conference,
  Proceedings</em>, pages 332--337, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#mycroft2019visuallymixing">bib</a>&nbsp;]

</dd>


<dt>
[<a name="nakamura2019probabilistictranscription">71</a>]
</dt>
<dd>
E&nbsp;Nakamura, R&nbsp;Nishikimi, S&nbsp;Dixon, and K&nbsp;Yoshii.
 Probabilistic sequential patterns for singing transcription.
 In <em>2018 Asia-Pacific Signal and Information Processing
  Association Annual Summit and Conference, APSIPA ASC 2018 - Proceedings</em>,
  pages 1905--1912, Mar 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#nakamura2019probabilistictranscription">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.23919/APSIPA.2018.8659637">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="nasreen2019apatients">72</a>]
</dt>
<dd>
S&nbsp;Nasreen, M&nbsp;Purver, and J&nbsp;Hough.
 A corpus study on questions, responses and misunderstanding signals
  in conversations with alzheimer’s patients.
 In <em>Proceedings of the 23rd Workshop on the Semantics and
  Pragmatics of Dialogue - Full Papers</em>. SEMDIAL, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#nasreen2019apatients">bib</a>&nbsp;| 
<a href="http://semdial.org/anthology/Z19-Nasreen_semdial_0013.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="nolasco2019audiobasedstates">73</a>]
</dt>
<dd>
I&nbsp;Nolasco, A&nbsp;Terenzi, S&nbsp;Cecchi, S&nbsp;Orcioni, H&nbsp;BEAR, and E&nbsp;BENETOS.
 Audio-based identification of beehive states.
 Brighton, UK, IEEE, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#nolasco2019audiobasedstates">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8682981">DOI</a>&nbsp;| 
<a href="https://2019.ieeeicassp.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="nonnis2019maziplay">74</a>]
</dt>
<dd>
A&nbsp;Nonnis and N&nbsp;Bryan-Kinns.
 Mazi: Tangible technologies as a channel for collaborative play.
 In <em>Conference on Human Factors in Computing Systems -
  Proceedings</em>, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#nonnis2019maziplay">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3290605.3300670">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="nonnis2019maziautism">75</a>]
</dt>
<dd>
A&nbsp;Nonnis and NN&nbsp;Bryan-Kinns.
 Mazi: a tangible toy for collaborative play between children with
  autism.
 In <em>Proceedings of the 18th ACM International Conference on
  Interaction Design and Children</em>, https://dl.acm.org/citation.cfm?id=3325340,
  Jun 2019. ACM Press.
 Best Demo Award.
[&nbsp;<a href="pubs2019_raw_bib.html#nonnis2019maziautism">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3311927.3325340">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ohanlon2019comparingrecognition">76</a>]
</dt>
<dd>
K&nbsp;O'Hanlon and MB&nbsp;Sandler.
 Comparing cqt and reassignment based chroma features for
  template-based automatic chord recognition.
 In <em>ICASSP, IEEE International Conference on Acoustics, Speech
  and Signal Processing - Proceedings</em>, volume 2019-May, pages 860--864, May
  2019.
[&nbsp;<a href="pubs2019_raw_bib.html#ohanlon2019comparingrecognition">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8682774">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="omigie2019intracranialcortices">77</a>]
</dt>
<dd>
D&nbsp;Omigie, M&nbsp;Pearce, K&nbsp;Lehongre, D&nbsp;Hasboun, V&nbsp;Navarro, C&nbsp;Adam, and S&nbsp;Samson.
 Intracranial recordings and computational modeling of music reveal
  the time course of prediction error signaling in frontal and temporal
  cortices.
 <em>J Cogn Neurosci</em>, 31(6):855--873, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#omigie2019intracranialcortices">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1162/jocn_a_01388">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/30883293">http</a>&nbsp;]

</dd>


<dt>
[<a name="pankajakshan2019onsetsmodelling">78</a>]
</dt>
<dd>
A&nbsp;Pankajakshan, H&nbsp;Bear, and E&nbsp;Benetos.
 Onsets, activity, and events: a multi-task approach for polyphonic
  sound event modelling.
 In <em>http://dcase.community/workshop2019/</em>, pages 174--178. New
  York, USA, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pankajakshan2019onsetsmodelling">bib</a>&nbsp;| 
<a href="http://dcase.community/workshop2019/">http</a>&nbsp;]

</dd>


<dt>
[<a name="pankajakshan2019polyphonicapproach">79</a>]
</dt>
<dd>
A&nbsp;Pankajakshan, H&nbsp;Bear, and E&nbsp;Benetos.
 Polyphonic sound event and sound activity detection: a multi-task
  approach.
 In <em>http://www.waspaa.com/</em>, pages 318--322. New Paltz, NY, USA,
  IEEE, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pankajakshan2019polyphonicapproach">bib</a>&nbsp;| 
<a href="http://www.eecs.qmul.ac.uk/profiles/pankajakshanarjun.html">.html</a>&nbsp;]

</dd>


<dt>
[<a name="pardue2019realtimeintonation">80</a>]
</dt>
<dd>
LS&nbsp;Pardue and A&nbsp;McPherson.
 Real-time aural and visual feedback for improving violin intonation.
 <em>Front Psychol</em>, 10:627--627, Apr 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pardue2019realtimeintonation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3389/fpsyg.2019.00627">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/31001159">http</a>&nbsp;]

</dd>


<dt>
[<a name="pauwels201920audio">81</a>]
</dt>
<dd>
J&nbsp;Pauwels, K&nbsp;O'Hanlon, E&nbsp;Gómez, and M&nbsp;Sandler.
 20 years of automatic chord recognition from audio.
 Delft, Netherlands, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pauwels201920audio">bib</a>&nbsp;]

</dd>


<dt>
[<a name="pauwels2019findingcatalogue">82</a>]
</dt>
<dd>
J&nbsp;Pauwels and M&nbsp;Sandler.
 Finding new practice material through chord-based exploration of a
  large music catalogue.
 Málaga, Spain, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pauwels2019findingcatalogue">bib</a>&nbsp;]

</dd>


<dt>
[<a name="pauwels2019acontent">83</a>]
</dt>
<dd>
J&nbsp;Pauwels and MB&nbsp;Sandler.
 A web-based system for suggesting new practice material to music
  learners based on chord content.
 In <em>Joint Proceedings of the ACM IUI 2019 Workshops</em>, volume
  2327, Mar 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pauwels2019acontent">bib</a>&nbsp;]

</dd>


<dt>
[<a name="pearce2019informationtheoreticcomplexity">84</a>]
</dt>
<dd>
M&nbsp;Pearce and S&nbsp;Sauvé.
 Information-theoretic modeling of perceived musical complexity.
 <em>Music Perception</em>, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#pearce2019informationtheoreticcomplexity">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1525/mp.2019.37.2.165">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="phan2019seqsleepnetstaging">85</a>]
</dt>
<dd>
H&nbsp;Phan, F&nbsp;Andreotti, N&nbsp;Cooray, OY&nbsp;Chen, and M&nbsp;De&nbsp;Vos.
 Seqsleepnet: End-to-end hierarchical recurrent neural network for
  sequence-to-sequence automatic sleep staging.
 <em>IEEE Transactions on Neural Systems and Rehabilitation
  Engineering</em>, 27(3):400--410, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#phan2019seqsleepnetstaging">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/tnsre.2019.2896659">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="phaye2019subspectralnetclassification">86</a>]
</dt>
<dd>
SSR Phaye, E&nbsp;BENETOS, and Y&nbsp;Wang.
 Subspectralnet - using sub-spectrogram based convolutional neural
  networks for acoustic scene classification.
 Brighton, UK, IEEE, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#phaye2019subspectralnetclassification">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8683288">DOI</a>&nbsp;| 
<a href="https://ssrp.github.io/">http</a>&nbsp;]

</dd>


<dt>
[<a name="quirogamartinez2019musicalnonmusicians">87</a>]
</dt>
<dd>
DR&nbsp;Quiroga-Martinez, N&nbsp;C&nbsp;Hansen, A&nbsp;Højlund, M&nbsp;Pearce, E&nbsp;Brattico, and P&nbsp;Vuust.
 Musical prediction error responses similarly reduced by predictive
  uncertainty in musicians and non-musicians.
 <em>European Journal of Neuroscience</em>, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#quirogamartinez2019musicalnonmusicians">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1111/ejn.14667">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/31891423">http</a>&nbsp;]

</dd>


<dt>
[<a name="quirogamartinez2019reducedcontexts">88</a>]
</dt>
<dd>
DR&nbsp;Quiroga-Martinez, NC&nbsp;Hansen, A&nbsp;Højlund, MT&nbsp;Pearce, E&nbsp;Brattico, and P&nbsp;Vuust.
 Reduced prediction error responses in high-as compared to
  low-uncertainty musical contexts.
 <em>Cortex</em>, 120:181--200, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#quirogamartinez2019reducedcontexts">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.cortex.2019.06.010">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ragano2019adaptingevaluation">89</a>]
</dt>
<dd>
A&nbsp;Ragano, E&nbsp;BENETOS, and A&nbsp;Hines.
 Adapting the quality of experience framework for audio archive
  evaluation.
 In <em>https://www.qomex2019.de/</em>. Berlin, Germany, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#ragano2019adaptingevaluation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="rodrguezalgarra2019characterisinginterventions">90</a>]
</dt>
<dd>
F&nbsp;Rodríguez-Algarra, BL&nbsp;Sturm, and S&nbsp;Dixon.
 Characterising confounding effects in music classification
  experiments through interventions.
 <em>Transactions of the International Society for Music Information
  Retrieval</em>, 2(1):52--66, Aug 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#rodrguezalgarra2019characterisinginterventions">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.5334/tismir.24">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="rohanian2019detectingfusion">91</a>]
</dt>
<dd>
M&nbsp;Rohanian, J&nbsp;Hough, and M&nbsp;Purver.
 Detecting depression with word-level multimodal fusion.
 In <em>Proceedings of the Annual Conference of the International
  Speech Communication Association, INTERSPEECH</em>, volume 2019-September, pages
  1443--1447. Graz, Austria, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#rohanian2019detectingfusion">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21437/Interspeech.2019-2283">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ronan2019intelligentaudio">92</a>]
</dt>
<dd>
DM&nbsp;RONAN.
 <em>Intelligent Subgrouping of Multitrack Audio</em>.
 PhD thesis, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#ronan2019intelligentaudio">bib</a>&nbsp;]

</dd>


<dt>
[<a name="saitis2019thetimbre">93</a>]
</dt>
<dd>
C&nbsp;Saitis and S&nbsp;Weinzierl.
 The semantics of timbre.
 In <em>Timbre: Acoustics, Perception, and Cognition</em>, volume&nbsp;69,
  pages 119--149. Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#saitis2019thetimbre">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-030-14832-4_5">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="sandler2019semanticchain">94</a>]
</dt>
<dd>
M&nbsp;Sandler, D&nbsp;De&nbsp;Roure, S&nbsp;Benford, and K&nbsp;Page.
 Semantic web technology for new experiences throughout the music
  production-consumption chain.
 In <em>Proceedings - 2019 International Workshop on Multilayer Music
  Representation and Processing, MMRP 2019</em>, pages 49--55, Mar 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#sandler2019semanticchain">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/MMRP.2019.8665378">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="senvaityte2019guitardeconvolution">95</a>]
</dt>
<dd>
D&nbsp;Senvaityte, J&nbsp;Pauwels, and M&nbsp;Sandler.
 Guitar string separation using non-negative matrix factorization and
  factor deconvolution.
 Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#senvaityte2019guitardeconvolution">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3356590.3356628">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="sheng2019acompressor">96</a>]
</dt>
<dd>
D&nbsp;Sheng and G&nbsp;Fazekas.
 A feature learning siamese model for intelligent control of the
  dynamic range compressor.
 In <em>Proceedings of the International Joint Conference on Neural
  Networks</em>, volume 2019-July, Jul 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#sheng2019acompressor">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/IJCNN.2019.8851950">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="shukla2019realtimepanning">97</a>]
</dt>
<dd>
R&nbsp;SHUKLA, IT&nbsp;Radu, M&nbsp;SANDLER, and R&nbsp;STEWART.
 Real-time binaural rendering with virtual vector base amplitude
  panning.
 New York, Mar 2019. York, United Kingdom, Audio Engineering Society.
[&nbsp;<a href="pubs2019_raw_bib.html#shukla2019realtimepanning">bib</a>&nbsp;| 
<a href="http://www.aes.org/e-lib/">http</a>&nbsp;]

</dd>


<dt>
[<a name="siedenburg2019theresearch">98</a>]
</dt>
<dd>
K&nbsp;Siedenburg, C&nbsp;Saitis, and S&nbsp;McAdams.
 The present, past, and future of timbre research.
 In <em>Timbre: Acoustics, Perception, and Cognition</em>, volume&nbsp;69,
  pages 1--19. Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#siedenburg2019theresearch">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-030-14832-4_1">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="singh2019audiolayer">99</a>]
</dt>
<dd>
S&nbsp;Singh, A&nbsp;Pankajakshan, and E&nbsp;Benetos.
 Audio tagging using a linear noise modelling layer.
 In <em>http://dcase.community/workshop2019/</em>, pages 234--238. New
  York, USA, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#singh2019audiolayer">bib</a>&nbsp;| 
<a href="http://dcase.community/workshop2019/">http</a>&nbsp;]

</dd>


<dt>
[<a name="stolfi2019playsoundspaceobjects">100</a>]
</dt>
<dd>
AS&nbsp;Stolfi, A&nbsp;Milo, and M&nbsp;Barthet.
 Playsound.space: Improvising in the browser with semantic sound
  objects.
 <em>Journal of New Music Research</em>, 48(4):366--384, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#stolfi2019playsoundspaceobjects">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1080/09298215.2019.1649433">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="stowell2019statecome">101</a>]
</dt>
<dd>
D&nbsp;Stowell.
 State of the art in computational bioacoustics and machine learning:
  How far have we come?
 In <em>Biodiversity Information Science and Standards</em>, volume&nbsp;3,
  page e37227, Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#stowell2019statecome">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3897/biss.3.37227">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="stowell2019automaticconditions">102</a>]
</dt>
<dd>
D&nbsp;Stowell, T&nbsp;Petrusková, M&nbsp;Šálek, and P&nbsp;Linhart.
 Automatic acoustic identification of individual animals: Improving
  generalisation across species and recording conditions.
 <em>Journal of the Royal Society Interface</em>, 16(153), Apr 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#stowell2019automaticconditions">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1098/rsif.2018.0940">DOI</a>&nbsp;| 
<a href="http://arxiv.org/abs/1810.09273v1">http</a>&nbsp;]

</dd>


<dt>
[<a name="subramanian2019robustnessclassification">103</a>]
</dt>
<dd>
V&nbsp;SUBRAMANIAN, E&nbsp;Benetos, and M&nbsp;Sandler.
 Robustness of adversarial attacks in sound event classification.
 In <em>http://dcase.community/workshop2019/</em>, pages 239--243. New
  York, USA, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#subramanian2019robustnessclassification">bib</a>&nbsp;| 
<a href="http://dcase.community/workshop2019/">http</a>&nbsp;]

</dd>


<dt>
[<a name="supej2019gendercorpora">104</a>]
</dt>
<dd>
A&nbsp;Supej, M&nbsp;Plahuta, M&nbsp;Purver, M&nbsp;Mathioudakis, and S&nbsp;Pollak.
 Gender, language and society - word embeddings as a reflection of
  social inequalities in linguistic corpora.
 pages 75--83, Ljubljana, Slovenia, Dec 2019. Bled, Slovenia,
  Slovensko sociološko društvo.
[&nbsp;<a href="pubs2019_raw_bib.html#supej2019gendercorpora">bib</a>&nbsp;| 
<a href="http://www.eecs.qmul.ac.uk/~mpurver/papers/supej-et-al19sss.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="snchez2019realtimesolids">105</a>]
</dt>
<dd>
P&nbsp;Sánchez and JD&nbsp;Reiss.
 Real-time synthesis of sound effects caused by the interaction
  between two solids.
 In <em>AES 146th International Convention</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#snchez2019realtimesolids">bib</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2019moodplaygithubioplayer">106</a>]
</dt>
<dd>
F&nbsp;Thalmann, A&nbsp;Allik, C&nbsp;Metzig, and M&nbsp;Sandler.
 moodplay.github.io: an online collaborative music player.
 Trondheim, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#thalmann2019moodplaygithubioplayer">bib</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2019representingstructures">107</a>]
</dt>
<dd>
F&nbsp;Thalmann, G&nbsp;Wiggins, and M&nbsp;Sandler.
 Representing modifiable and reusable musical content on the web with
  constrained multi-hierarchical structures.
 <em>IEEE Transactions on Multimedia</em>, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#thalmann2019representingstructures">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TMM.2019.2961207">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2019querybasedrecordings">108</a>]
</dt>
<dd>
F&nbsp;Thalmann, T&nbsp;WILMERING, and M&nbsp;Sandler.
 Query-based mashups of historical live music recordings.
 Aug 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#thalmann2019querybasedrecordings">bib</a>&nbsp;]

</dd>


<dt>
[<a name="theodorou2019engagingresponses">109</a>]
</dt>
<dd>
L&nbsp;Theodorou, PGT Healey, and F&nbsp;Smeraldi.
 Engaging with contemporary dance: What can body movements tell us
  about audience responses?
 <em>Front Psychol</em>, 10:71--71, Feb 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#theodorou2019engagingresponses">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3389/fpsyg.2019.00071">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/30774609">http</a>&nbsp;]

</dd>


<dt>
[<a name="thompson2019aapplications">110</a>]
</dt>
<dd>
A&nbsp;Thompson and G&nbsp;Fazekas.
 A model-view-update framework for interactive web audio applications.
 In <em>ACM International Conference Proceeding Series</em>, pages
  219--222, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#thompson2019aapplications">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3356590.3356623">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="tom2019anpractices">111</a>]
</dt>
<dd>
A&nbsp;Tom, J&nbsp;Reiss, and P&nbsp;Depalle.
 An automatic mixing system for multitrack spatialization for stereo
  based on unmasking and best panning practices.
 In <em>AES 146th International Convention</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#tom2019anpractices">bib</a>&nbsp;]

</dd>


<dt>
[<a name="turchet2019haptificationperformance">112</a>]
</dt>
<dd>
L&nbsp;Turchet and M&nbsp;Barthet.
 Haptification of performer's control gestures in live electronic
  music performance.
 In <em>ACM International Conference Proceeding Series</em>, pages
  244--247, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#turchet2019haptificationperformance">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3356590.3356629">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="turchet2019anpractice">113</a>]
</dt>
<dd>
L&nbsp;Turchet and M&nbsp;Barthet.
 An ubiquitous smart guitar system for collaborative musical practice.
 <em>Journal of New Music Research</em>, 48(4):352--365, Jul 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#turchet2019anpractice">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1080/09298215.2019.1637439">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="viannalordelo2019investigatingseparation">114</a>]
</dt>
<dd>
C&nbsp;Vianna&nbsp;Lordelo, E&nbsp;Benetos, S&nbsp;Dixon, and S&nbsp;Ahlbäck.
 Investigating kernel shapes and skip connections for deep
  learning-based harmonic-percussive separation.
 In <em>http://www.waspaa.com/</em>, pages 40--44. New Paltz, NY, USA,
  IEEE, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#viannalordelo2019investigatingseparation">bib</a>&nbsp;| 
<a href="http://www.eecs.qmul.ac.uk/profiles/viannalordelocarlospedro.html">.html</a>&nbsp;]

</dd>


<dt>
[<a name="wang2019cbfperidbanalysis">115</a>]
</dt>
<dd>
C&nbsp;Wang, E&nbsp;Benetos, and E&nbsp;Chew.
 Cbf-peridb: A chinese bamboo flute dataset for periodic modulation
  analysis.
 Delft, The Netherlands, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wang2019cbfperidbanalysis">bib</a>&nbsp;]

</dd>


<dt>
[<a name="wang2019adaptivesignals">116</a>]
</dt>
<dd>
C&nbsp;Wang, E&nbsp;Benetos, V&nbsp;Lostanlen, and E&nbsp;Chew.
 Adaptive time–frequency scattering for periodic modulation
  recognition in music signals.
 pages 809--815. Delft, The Netherlands, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wang2019adaptivesignals">bib</a>&nbsp;]

</dd>


<dt>
[<a name="wang2019hmmbasedflute">117</a>]
</dt>
<dd>
C&nbsp;WANG, E&nbsp;BENETOS, X&nbsp;MENG, and E&nbsp;CHEW.
 Hmm-based glissando detection for recordings of chinese bamboo flute.
 In <em>Proceedings of Sound and Music Computing Conference</em>, pages
  545--550. Malaga, Spain, May 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wang2019hmmbasedflute">bib</a>&nbsp;]

</dd>


<dt>
[<a name="weaver2019fillingperformance">118</a>]
</dt>
<dd>
J&nbsp;Weaver, M&nbsp;Barthet, and E&nbsp;Chew.
 Filling the space: The impact of convolution reverberation time on
  note duration and velocity in duet performance.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#weaver2019fillingperformance">bib</a>&nbsp;]

</dd>


<dt>
[<a name="wei2019investigatingapproach">119</a>]
</dt>
<dd>
C&nbsp;Weiß, M&nbsp;Mauch, S&nbsp;Dixon, and M&nbsp;Müller.
 Investigating style evolution of western classical music: A
  computational approach.
 <em>Musicae Scientiae</em>, 23(4):486--507, Dec 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wei2019investigatingapproach">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1177/1029864918757595">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="wilkinson2019endtoendanalysis">120</a>]
</dt>
<dd>
WJ&nbsp;Wilkinson, MR&nbsp;Andersen, JD&nbsp;Reiss, D&nbsp;Stowell, and A&nbsp;Solin.
 End-to-end probabilistic inference for nonstationary audio analysis.
 In <em>36th International Conference on Machine Learning, ICML
  2019</em>, volume 2019-June, pages 11751--11760, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wilkinson2019endtoendanalysis">bib</a>&nbsp;]

</dd>


<dt>
[<a name="wilkinson2019unifyinganalysis">121</a>]
</dt>
<dd>
WJ&nbsp;Wilkinson, M&nbsp;Riis&nbsp;Andersen, JD&nbsp;Reiss, D&nbsp;Stowell, and A&nbsp;Solin.
 Unifying probabilistic models for time-frequency analysis.
 In <em>ICASSP, IEEE International Conference on Acoustics, Speech
  and Signal Processing - Proceedings</em>, volume 2019-May, pages 3352--3356, Apr
  2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wilkinson2019unifyinganalysis">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP.2019.8682306">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="wilmering2019alignmentconcerts">122</a>]
</dt>
<dd>
T&nbsp;Wilmering, F&nbsp;Thalmann, and MB&nbsp;Sandler.
 Alignment and timeline construction for incomplete analogue audience
  recordings of historical live music concerts.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#wilmering2019alignmentconcerts">bib</a>&nbsp;]

</dd>


<dt>
[<a name="xamb2019leveragingproduction">123</a>]
</dt>
<dd>
A&nbsp;Xambó, F&nbsp;Font, G&nbsp;Fazekas, and M&nbsp;Barthet.
 Leveraging online audio commons content for media production.
 In <em>Foundations in Sound Design for Linear Media</em>, pages
  248--282. Jun 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#xamb2019leveragingproduction">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.4324/9781315106335-10">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="xiao2019conceptualcreation">124</a>]
</dt>
<dd>
P&nbsp;Xiao, H&nbsp;Toivonen, O&nbsp;Gross, A&nbsp;Cardoso, J&nbsp;Correia, P&nbsp;Machado, P&nbsp;Martins,
  HG&nbsp;Oliveira, R&nbsp;Sharma, AM&nbsp;Pinto, A&nbsp;Díaz, V&nbsp;Francisco, P&nbsp;Gervás, R&nbsp;Hervás,
  C&nbsp;León, J&nbsp;Forth, M&nbsp;Purver, GA&nbsp;Wiggins, D&nbsp;Miljkovic, V&nbsp;Podpecan, S&nbsp;Pollak,
  J&nbsp;Kralj, M&nbsp;Znidarsic, M&nbsp;Bohanec, N&nbsp;Lavrac, T&nbsp;Urbancic, FVD Velde, and
  SA&nbsp;Battersby.
 Conceptual representations for computational concept creation.
 <em>ACM Computing Surveys</em>, 52(1):9:1--9:33, Feb 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#xiao2019conceptualcreation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3186729">DOI</a>&nbsp;| 
<a href="http://www.eecs.qmul.ac.uk/~mpurver/papers/xiao-et-al19acm.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="ycart2019blendingtranscription">125</a>]
</dt>
<dd>
A&nbsp;Ycart, A&nbsp;McLeod, E&nbsp;Benetos, and K&nbsp;Yoshii.
 Blending acoustic and language model predictions for automatic music
  transcription.
 pages 454--461. Delft, The Netherlands, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#ycart2019blendingtranscription">bib</a>&nbsp;| 
<a href="https://ismir2019.ewi.tudelft.nl/">http</a>&nbsp;]

</dd>


<dt>
[<a name="ycart2019atransduction">126</a>]
</dt>
<dd>
A&nbsp;Ycart, D&nbsp;Stoller, and E&nbsp;Benetos.
 A comparative study of neural models for polyphonic music sequence
  transduction.
 pages 470--477. Delft, The Netherlands, Nov 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#ycart2019atransduction">bib</a>&nbsp;| 
<a href="https://ismir2019.ewi.tudelft.nl/">http</a>&nbsp;]

</dd>


<dt>
[<a name="yela2019spectralsignals">127</a>]
</dt>
<dd>
DF&nbsp;Yela, D&nbsp;Stowell, and M&nbsp;Sandler.
 Spectral visibility graphs: Application to similarity of harmonic
  signals.
 In <em>European Signal Processing Conference</em>, volume
  2019-September, Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#yela2019spectralsignals">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.23919/EUSIPCO.2019.8903056">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhang2019coordinationlisteners">128</a>]
</dt>
<dd>
L&nbsp;Zhang and PGT Healey.
 Co-ordination of head nods: Asymmetries between speakers and
  listeners.
 Sep 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#zhang2019coordinationlisteners">bib</a>&nbsp;]

</dd>


<dt>
[<a name="zhou2019adaptivenmf">129</a>]
</dt>
<dd>
Q&nbsp;Zhou, Z&nbsp;Feng, and E&nbsp;Benetos.
 Adaptive noise reduction for sound event detection using
  subband-weighted nmf.
 <em>Sensors</em>, 19(14), Jul 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#zhou2019adaptivenmf">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3390/s19143206">DOI</a>&nbsp;| 
<a href="https://www.mdpi.com/">http</a>&nbsp;]

</dd>


<dt>
[<a name="zioga2019fromcreativity">130</a>]
</dt>
<dd>
I&nbsp;Zioga, P&nbsp;Harrison, M&nbsp;Pearce, J&nbsp;Bhattacharya, and C&nbsp;Di&nbsp;Bernardi&nbsp;Luft.
 From learning to creativity: Identifying the behavioural and neural
  correlates of learning to predict human judgements of musical creativity.
 <em>NeuroImage</em>, Oct 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#zioga2019fromcreativity">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.neuroimage.2019.116311">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ziga2019realisticoptimisation">131</a>]
</dt>
<dd>
J&nbsp;Zúñiga and JD&nbsp;Reiss.
 Realistic procedural sound synthesis of bird song using particle
  swarm optimisation.
 In <em>147th Audio Engineering Society International Convention
  2019</em>, Jan 2019.
[&nbsp;<a href="pubs2019_raw_bib.html#ziga2019realisticoptimisation">bib</a>&nbsp;]

</dd>
</dl><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.99.</em></p>
