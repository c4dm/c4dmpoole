
<!-- This document was automatically generated with bibtex2html 1.96
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     ./bibtex2html -nodoc -dl -a -noabstract -nokeywords -o pubs2022_raw pubs2022.bib  -->


<dl>

<dt>
[<a name="allik2016mymoodplayapp">1</a>]
</dt>
<dd>
A&nbsp;Allik, G&nbsp;Fazekas, M&nbsp;Barthet, and M&nbsp;Sandler.
 mymoodplay: An interactive mood-based music discovery app.
 2016.
 date-added: 2017-12-29 19:26:47 +0000 date-modified: 2017-12-29
  19:38:36 +0000 keywords: Semantic Audio, mood-based interaction,
  Ontology-based systems local-url:
  http://www.semanticaudio.net/files/papers/allik2016wac.pdf bdsk-url-1:
  http://hdl.handle.net/1853/54589.
[&nbsp;<a href="pubs2022_raw_bib.html#allik2016mymoodplayapp">bib</a>&nbsp;| 
<a href="http://hdl.handle.net/1853/54589">http</a>&nbsp;]

</dd>


<dt>
[<a name="alvaradodurangaussiananalysis">2</a>]
</dt>
<dd>
PA&nbsp;Alvarado&nbsp;Duran and DF&nbsp;STOWELL.
 Gaussian processes for music audio modelling and content analysis.
 In <em>IEEE International Workshop on Machine Learning for Signal
  Processing (MLSP)</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#alvaradodurangaussiananalysis">bib</a>&nbsp;]

</dd>


<dt>
[<a name="armitagethecontrol">3</a>]
</dt>
<dd>
JDK ARMITAGE and A&nbsp;MCPHERSON.
 The stenophone: Live coding on a chorded keyboard with continuous
  control.
 In <em>International Conference on Live Coding</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#armitagethecontrol">bib</a>&nbsp;]

</dd>


<dt>
[<a name="aziz2022planningoverviews">4</a>]
</dt>
<dd>
N&nbsp;Aziz, T&nbsp;Stockman, and R&nbsp;Stewart.
 Planning your journey in audio: Design and evaluation of auditory
  route overviews.
 <em>ACM Transactions on Accessible Computing</em>, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#aziz2022planningoverviews">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3531529">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="barthetimprovingshaping">5</a>]
</dt>
<dd>
M&nbsp;BARTHET, R&nbsp;Kronland-Martinet, and S&nbsp;Ystad.
 Improving musical expressiveness by time-varying brightness shaping.
 volume 4969 of <em>Lecture Notes In Computer Science</em>, pages
  313-336. Springer.
[&nbsp;<a href="pubs2022_raw_bib.html#barthetimprovingshaping">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-540-85035-9\_22">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="benetos2022measuringdata">6</a>]
</dt>
<dd>
E&nbsp;Benetos, A&nbsp;Ragano, D&nbsp;Sgroi, and A&nbsp;Tuckwell.
 Measuring national mood with music: using machine learning to
  construct a measure of national valence from audio data.
 <em>Behavior Research Methods</em>, Feb 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#benetos2022measuringdata">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3758/s13428-021-01747-7">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="bin2016skipresponse">7</a>]
</dt>
<dd>
SMA BIN, AP&nbsp;MCPHERSON, and N&nbsp;BRYAN-KINNS.
 Skip the pre-concert demo: How technical familiarity and musical
  style affect audience response.
 In <em>New Interfaces for Musical Expression</em>, volume&nbsp;16, pages
  200-205. Brisbane, Australia, Jul 2016.
[&nbsp;<a href="pubs2022_raw_bib.html#bin2016skipresponse">bib</a>&nbsp;| 
<a href="http://nime.org/archives">http</a>&nbsp;]

</dd>


<dt>
[<a name="bryankinns2021exploringmusic">8</a>]
</dt>
<dd>
N&nbsp;Bryan-Kinns, B&nbsp;Banar, C&nbsp;Ford, C&nbsp;Reed, Y&nbsp;Zhang, S&nbsp;Colton, and J&nbsp;Armitage.
 Exploring xai for the arts: Explaining latent space in generative
  music.
 In <em>NeurIPS 2021 - eXplainable AI Approaches for Debugging and
  Diagnosis Workshop</em>, Dec 2021.
[&nbsp;<a href="pubs2022_raw_bib.html#bryankinns2021exploringmusic">bib</a>&nbsp;]

</dd>


<dt>
[<a name="bryankinns2022qi2heepistemology">9</a>]
</dt>
<dd>
N&nbsp;Bryan-Kinns, W&nbsp;Wang, and T&nbsp;Ji.
 Qi2he: A co-design framework inspired by eastern epistemology.
 <em>International Journal of Human Computer Studies</em>, 160, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#bryankinns2022qi2heepistemology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.ijhcs.2022.102773">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="carrillo2016geolocationplayer">10</a>]
</dt>
<dd>
A&nbsp;Carrillo, F&nbsp;Thalmann, G&nbsp;Fazekas, and M&nbsp;Sandler.
 Geolocation adaptive music player.
 2016.
 date-added: 2017-12-22 20:02:39 +0000 date-modified: 2017-12-22
  20:05:50 +0000 keywords: adaptive music, intelligent music player, semantic
  audio, feature extraction bdsk-url-1:
  https://smartech.gatech.edu/bitstream/handle/1853/54586/WAC2016-47.pdf.
[&nbsp;<a href="pubs2022_raw_bib.html#carrillo2016geolocationplayer">bib</a>&nbsp;| 
<a href="https://smartech.gatech.edu/bitstream/handle/1853/54586/WAC2016-47.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="caspeddx7sounds">11</a>]
</dt>
<dd>
F&nbsp;Caspe, A&nbsp;Mcpherson, and M&nbsp;Sandler.
 Ddx7: Differentiable fm synthesis of musical instrument sounds.
 In <em>23rd International Society for Music Information Retrieval
  Conference (ISMIR 2022)</em>. Bengaluru, India.
[&nbsp;<a href="pubs2022_raw_bib.html#caspeddx7sounds">bib</a>&nbsp;]

</dd>


<dt>
[<a name="caspe2022ddx7sounds">12</a>]
</dt>
<dd>
F&nbsp;Caspe, A&nbsp;McPherson, and M&nbsp;Sandler.
 Ddx7: Differentiable fm synthesis of musical instrument sounds, Aug
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#caspe2022ddx7sounds">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2208.06169">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="choi2016towardstransitions">13</a>]
</dt>
<dd>
K&nbsp;Choi, G&nbsp;Fazekas, and M&nbsp;Sandler.
 Towards playlist generation algorithms using rnns trained on
  within-track transitions.
 2016.
 date-added: 2017-12-22 14:17:02 +0000 date-modified: 2017-12-22
  15:19:16 +0000 keywords: playlist generation, semantic audio, music
  transition modeling local-url: https://arxiv.org/pdf/1606.02096.pdf
  bdsk-url-1: http://ceur-ws.org/Vol-1618/SOAP_paper4.pdf bdsk-url-2:
  https://dx.doi.org/10.1145/1235.
[&nbsp;<a href="pubs2022_raw_bib.html#choi2016towardstransitions">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/1235">DOI</a>&nbsp;| 
<a href="http://ceur-ws.org/Vol-1618/SOAP_paper4.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="choitowardsdescriptions">14</a>]
</dt>
<dd>
K&nbsp;CHOI, G&nbsp;fazekas, M&nbsp;sandler, B&nbsp;McFee, and K&nbsp;Cho.
 Towards music captioning: Generating music playlist descriptions.
 In <em>Late-Breaking/Demo session of 17th International Society of
  Music Information Retrieval (ISMIR) Conference</em>. New York.
[&nbsp;<a href="pubs2022_raw_bib.html#choitowardsdescriptions">bib</a>&nbsp;]

</dd>


<dt>
[<a name="daikoku2022agreementsample">15</a>]
</dt>
<dd>
H&nbsp;Daikoku, S&nbsp;Ding, E&nbsp;Benetos, ALC Wood, T&nbsp;Shimizono, US&nbsp;Sanne, S&nbsp;Fujii, and
  PE&nbsp;Savage.
 Agreement among human and automated estimates of similarity in a
  global music sample.
 In <em>10th International Workshop on Folk Music Analysis (FMA
  2022)</em>. Sheffield, UK, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#daikoku2022agreementsample">bib</a>&nbsp;]

</dd>


<dt>
[<a name="delgado2022deepclassification">16</a>]
</dt>
<dd>
A&nbsp;Delgado, E&nbsp;Demirel, V&nbsp;Subramanian, C&nbsp;Saitis, and M&nbsp;Sandler.
 Deep embeddings for robust user-based amateur vocal percussion
  classification, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#delgado2022deepclassification">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2204.04646">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="delgado2022deepvocalisation">17</a>]
</dt>
<dd>
A&nbsp;Delgado, C&nbsp;Saitis, E&nbsp;Benetos, and M&nbsp;Sandler.
 Deep conditional representation learning for drum sample retrieval by
  vocalisation, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#delgado2022deepvocalisation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2204.04651">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="donovanbuildingtechnologies">18</a>]
</dt>
<dd>
L&nbsp;DONOVAN, SMA BIN, JDK ARMITAGE, and A&nbsp;MCPHERSON.
 Building an ide for an embedded system using web technologies.
 In <em>Web Audio Conference</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#donovanbuildingtechnologies">bib</a>&nbsp;]

</dd>


<dt>
[<a name="fanoyelainterferencefactorization">19</a>]
</dt>
<dd>
D&nbsp;FANO&nbsp;YELA, SE&nbsp;Ewert, MS&nbsp;Sandler, and DF&nbsp;FitzGerald.
 Interference reduction in music recordings combining kernel additive
  modelling and non-negative matrix factorization.
 In <em>IEEE International Conference on Acoustics, Speech and Signal
  Processings</em>. New Orleans.
[&nbsp;<a href="pubs2022_raw_bib.html#fanoyelainterferencefactorization">bib</a>&nbsp;]

</dd>


<dt>
[<a name="fanoyelaonstudy">20</a>]
</dt>
<dd>
D&nbsp;FANO&nbsp;YELA, SE&nbsp;ewert, MS&nbsp;sandler, and DF&nbsp;FitzGerald.
 On the importance of temporal context in proximity kernels: A vocal
  separation case study.
 In <em>Audio Engineering Society Conference on Semantic Audio</em>.
  Erlangen, Germany.
[&nbsp;<a href="pubs2022_raw_bib.html#fanoyelaonstudy">bib</a>&nbsp;]

</dd>


<dt>
[<a name="fordspeculatingai">21</a>]
</dt>
<dd>
C&nbsp;Ford and N&nbsp;Bryan-Kinns.
 Speculating on reflection and people’s music co-creation with ai.
 In <em>Generative AI and HCI Workshop at CHI 2022</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#fordspeculatingai">bib</a>&nbsp;]

</dd>


<dt>
[<a name="ford2022identifyinghome">22</a>]
</dt>
<dd>
C&nbsp;Ford and N&nbsp;Bryan-Kinns.
 Identifying engagement in children's interaction whilst composing
  digital music at home.
 In <em>ACM Conference on Creativity &amp; Cognition</em>, pages 443-456,
  New York, NY, USA, Jun 2022. Venice, Italy, ACM Digital Library.
[&nbsp;<a href="pubs2022_raw_bib.html#ford2022identifyinghome">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3527927.3532794">DOI</a>&nbsp;| 
<a href="https://dl.acm.org/doi/10.1145/3527927.3532794">http</a>&nbsp;]

</dd>


<dt>
[<a name="frachi2022designbiosignals">23</a>]
</dt>
<dd>
Y&nbsp;Frachi, T&nbsp;Takahashi, F&nbsp;Wang, and M&nbsp;Barthet.
 Design of emotion-driven game interaction using biosignals.
 volume 13334 LNCS, pages 160-179. Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#frachi2022designbiosignals">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-031-05637-6_10">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="guidi2022quantitativeinstruments">24</a>]
</dt>
<dd>
A&nbsp;Guidi and A&nbsp;McPherson.
 Quantitative evaluation of aspects of embodiment in new digital
  musical instruments.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#guidi2022quantitativeinstruments">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.79d0b38f">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hayes2022disembodiedsynthesis1">25</a>]
</dt>
<dd>
B&nbsp;Hayes, C&nbsp;Saitis, and G&nbsp;Fazekas.
 Disembodied timbres: a study on semantically prompted fm synthesis,
  Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#hayes2022disembodiedsynthesis1">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.31234/osf.io/ksw5j">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hayes2022disembodiedsynthesis2">26</a>]
</dt>
<dd>
B&nbsp;Hayes, C&nbsp;Saitis, and G&nbsp;Fazekas.
 Disembodied timbres: A study on semantically prompted fm synthesis.
 <em>Journal of the Audio Engineering Society</em>, 70(5):373-391, May
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#hayes2022disembodiedsynthesis2">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17743/jaes.2022.0006">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hayes2022disembodiedsynthesis3">27</a>]
</dt>
<dd>
B&nbsp;Hayes, C&nbsp;Saitis, and G&nbsp;Fazekas.
 Disembodied timbres: A study on semantically prompted fm synthesis.
 <em>Journal of the Audio Engineering Society</em>, 70(5):373-391, May
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#hayes2022disembodiedsynthesis3">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17743/jaes.2022.0006">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="huang2022improvingdetection">28</a>]
</dt>
<dd>
J&nbsp;Huang, E&nbsp;Benetos, and S&nbsp;Ewert.
 Improving lyrics alignment through joint pitch detection.
 In <em>2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing</em>, pages 451-455. Singapore, IEEE, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#huang2022improvingdetection">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746460">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="lepri2022thespeculation">29</a>]
</dt>
<dd>
G&nbsp;Lepri, J&nbsp;Bowers, S&nbsp;Topley, P&nbsp;Stapleton, P&nbsp;Bennett, K&nbsp;Andersen, and
  A&nbsp;McPherson.
 The 10,000 instruments workshop - (im)practical research for critical
  speculation.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#lepri2022thespeculation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.9e7c9ba3">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="leprimirroringtechnology">30</a>]
</dt>
<dd>
G&nbsp;LEPRI and A&nbsp;MCPHERSON.
 Mirroring the past, from typewriting to interactive art: an approach
  to the re-design of a vintage technology.
 In <em>New Interfaces for Musical Expression</em>. Blacksburg, VA.
[&nbsp;<a href="pubs2022_raw_bib.html#leprimirroringtechnology">bib</a>&nbsp;]

</dd>


<dt>
[<a name="lepri2022uselesspractice">31</a>]
</dt>
<dd>
G&nbsp;Lepri, A&nbsp;Mcpherson, and J&nbsp;Bowers.
 Useless, not worthless: Absurd making as critical practice.
 In <em>ACM conference on Designing Interactive Systems</em>, Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#lepri2022uselesspractice">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/10.1145/3357236.3395547">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="linhart2022themammals">32</a>]
</dt>
<dd>
P&nbsp;Linhart, M&nbsp;Mahamoud-Issa, D&nbsp;Stowell, and DT&nbsp;Blumstein.
 The potential for acoustic individual identification in mammals.
 <em>Mammalian Biology</em>, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#linhart2022themammals">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s42991-021-00222-2">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="liuperformancetracking">33</a>]
</dt>
<dd>
L&nbsp;Liu, Q&nbsp;KONG, G-V Morfi, and E&nbsp;Benetos.
 Performance midi-to-score conversion by neural beat tracking.
 In <em>Proceedings of the 23rd International Society for Music
  Information Retrieval Conference</em>. Bengaluru, India.
[&nbsp;<a href="pubs2022_raw_bib.html#liuperformancetracking">bib</a>&nbsp;| 
<a href="https://cheriell.github.io/">http</a>&nbsp;]

</dd>


<dt>
[<a name="luo2022towardsaudio">34</a>]
</dt>
<dd>
Y-J Luo, S&nbsp;Ewert, and S&nbsp;Dixon.
 Towards robust unsupervised disentanglement of sequential data — a
  case study using music audio.
 In <em>Proceedings of the Thirty-First International Joint
  Conference on Artificial Intelligence</em>, pages 3299-3305, Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#luo2022towardsaudio">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.24963/ijcai.2022/458">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="manco2022learningsupervision">35</a>]
</dt>
<dd>
I&nbsp;Manco, E&nbsp;Benetos, E&nbsp;Quinton, and G&nbsp;Fazekas.
 Learning music audio representations via weak language supervision.
 In <em>2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing</em>, pages 456-460. Singapore, IEEE, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#manco2022learningsupervision">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746996">DOI</a>&nbsp;| 
<a href="https://ilariamanco.com/">http</a>&nbsp;]

</dd>


<dt>
[<a name="metatlaaudiohapticworkstations">36</a>]
</dt>
<dd>
O&nbsp;Metatla, F&nbsp;Martin, A&nbsp;Parkinson, N&nbsp;Bryan-Kinns, T&nbsp;Stockman, and A&nbsp;Tanaka.
 Audio-haptic interfaces for digital audio workstations.
 <em>Journal on Multimodal User Interfaces</em>, pages 1-12.
[&nbsp;<a href="pubs2022_raw_bib.html#metatlaaudiohapticworkstations">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s12193-016-0217-8">DOI</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s12193-016-0217-8">http</a>&nbsp;]

</dd>


<dt>
[<a name="mice2022theperformances">37</a>]
</dt>
<dd>
L&nbsp;Mice and A&nbsp;Mcpherson.
 The m in nime: Motivic analysis and the case for a musicology of nime
  performances.
 In <em>International Conference on New Interfaces for Musical
  Expression</em>, Auckland, New Zealand, Jun 2022. Auckland, New Zealand,
  International Conference on New Interfaces for Musical Expression.
[&nbsp;<a href="pubs2022_raw_bib.html#mice2022theperformances">bib</a>&nbsp;| 
<a href="https://www.nime.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="mice2022superdesign">38</a>]
</dt>
<dd>
L&nbsp;Mice and AP&nbsp;McPherson.
 Super size me: Interface size, identity and embodiment in digital
  musical instrument design.
 Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#mice2022superdesign">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3491102.3517626">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="moroapproximatingfilters">39</a>]
</dt>
<dd>
G&nbsp;moro and A&nbsp;mcpherson.
 Approximating non-linear inductors using time-variant linear filters.
 In <em>18th International Conference on Digital Audio Effects</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#moroapproximatingfilters">bib</a>&nbsp;]

</dd>


<dt>
[<a name="morrealedesign201014">40</a>]
</dt>
<dd>
F&nbsp;MORREALE and MCPHERSON.
 Design for longevity: Ongoing use of instruments from nime 2010-14.
 In <em>NIME</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#morrealedesign201014">bib</a>&nbsp;]

</dd>


<dt>
[<a name="morrealenimeperspective">41</a>]
</dt>
<dd>
F&nbsp;MORREALE, A&nbsp;MCPHERSON, and M&nbsp;WANDERLEY.
 Nime identity from the performer’s perspective.
 In <em>New Interfaces for Musical Expreesion</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#morrealenimeperspective">bib</a>&nbsp;]

</dd>


<dt>
[<a name="nolasco2022rankbasedrepresentations">42</a>]
</dt>
<dd>
I&nbsp;Nolasco and D&nbsp;Stowell.
 Rank-based loss for learning hierarchical representations.
 volume 2022-May, pages 3623-3627, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#nolasco2022rankbasedrepresentations">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746907">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="nordmoen2022makingsystem">43</a>]
</dt>
<dd>
C&nbsp;Nordmoen and AP&nbsp;McPherson.
 Making space for material entanglements: A diffractive analysis of
  woodwork and the practice of making an interactive system.
 pages 415-423, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#nordmoen2022makingsystem">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3532106.3533572">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="olowefeaturuxvisualization">44</a>]
</dt>
<dd>
I&nbsp;OLOWE, M&nbsp;BARTHET, M&nbsp;GRIERSON, and N&nbsp;BRYAN-KINNS.
 Featur.ux: An approach to leveraging multitrack information for
  artistic music visualization.
 In <em>International Conference on Technologies for Music Notation
  and Representation (TENOR)</em>. Cambridge.
[&nbsp;<a href="pubs2022_raw_bib.html#olowefeaturuxvisualization">bib</a>&nbsp;]

</dd>


<dt>
[<a name="oloweresiduumgeneration">45</a>]
</dt>
<dd>
I&nbsp;OLOWE, G&nbsp;MORO, and M&nbsp;BARTHET.
 residuum: user mapping and performance strategies for multilayered
  live audiovisual generation.
 In <em>International Conference on New Interfaces for Musical
  Expression (NIME)</em>. Brisbane, Australia.
[&nbsp;<a href="pubs2022_raw_bib.html#oloweresiduumgeneration">bib</a>&nbsp;]

</dd>


<dt>
[<a name="ou2022exploringtranscription">46</a>]
</dt>
<dd>
L&nbsp;Ou, Z&nbsp;Guo, E&nbsp;Benetos, J&nbsp;Han, and Y&nbsp;Wang.
 Exploring transformer's potential on automatic piano transcription.
 In <em>IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP)</em>, pages 776-780. Singapore, IEEE, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ou2022exploringtranscription">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746789">DOI</a>&nbsp;| 
<a href="https://2022.ieeeicassp.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="ozaki2022similaritiesrecordings">47</a>]
</dt>
<dd>
Y&nbsp;Ozaki, J&nbsp;Kuroyanagi, J&nbsp;McBride, P&nbsp;Proutskova, A&nbsp;Tierney, P&nbsp;Pfordresher,
  E&nbsp;Benetos, F&nbsp;Liu, and PE&nbsp;Savage.
 Similarities and differences in a cross-linguistic sample of song and
  speech recordings.
 In <em>Joint Conference on Language Evolution</em>. Kanazawa, Japan, Sep
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ozaki2022similaritiesrecordings">bib</a>&nbsp;| 
<a href="https://sites.google.com/view/joint-conf-language-evolution/home">http</a>&nbsp;]

</dd>


<dt>
[<a name="pantelionsimilarity">48</a>]
</dt>
<dd>
M&nbsp;PANTELI and S&nbsp;Dixon.
 On the evaluation of rhythmic and melodic descriptors for music
  similarity.
 In <em>International Society for Music Information Retrieval
  Conference</em>. New York, USA.
[&nbsp;<a href="pubs2022_raw_bib.html#pantelionsimilarity">bib</a>&nbsp;]

</dd>


<dt>
[<a name="pelinski2022embeddedopportunities">49</a>]
</dt>
<dd>
T&nbsp;Pelinski, V&nbsp;Shepardson, S&nbsp;Symons, FS&nbsp;Caspe, AL&nbsp;Benito&nbsp;Temprano, J&nbsp;Armitage,
  C&nbsp;Kiefer, R&nbsp;Fiebrink, T&nbsp;Magnusson, and A&nbsp;McPherson.
 Embedded ai for nime: Challenges and opportunities.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#pelinski2022embeddedopportunities">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.76beab02">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="preniqi2022morelyrics">50</a>]
</dt>
<dd>
V&nbsp;Preniqi, K&nbsp;Kalimeri, and C&nbsp;Saitis.
 "more than words": Linking music preferences and moral values through
  lyrics, Sep 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#preniqi2022morelyrics">bib</a>&nbsp;]

</dd>


<dt>
[<a name="proutskova2022thejazz">51</a>]
</dt>
<dd>
P&nbsp;Proutskova, D&nbsp;Wolff, G&nbsp;Fazekas, K&nbsp;Frieler, F&nbsp;Höger, O&nbsp;Velichkina, G&nbsp;Solis,
  T&nbsp;Weyde, M&nbsp;Pfleiderer, HC&nbsp;Crayencour, G&nbsp;Peeters, and S&nbsp;Dixon.
 The jazz ontology: A semantic model and large-scale rdf repositories
  for jazz.
 <em>Journal of Web Semantics</em>, pages 100735-100735, Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#proutskova2022thejazz">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.websem.2022.100735">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="quintonaudiostructure">52</a>]
</dt>
<dd>
E&nbsp;QUINTON, C&nbsp;Harte, and M&nbsp;Sandler.
 Audio tempo estimation using fusion of time-frequency analyses and
  metrical structure.
[&nbsp;<a href="pubs2022_raw_bib.html#quintonaudiostructure">bib</a>&nbsp;]

</dd>


<dt>
[<a name="ragano2022aquality">53</a>]
</dt>
<dd>
A&nbsp;Ragano, E&nbsp;Benetos, M&nbsp;Chinen, HB&nbsp;Martinez, CKA Reddy, J&nbsp;Skoglund, and A&nbsp;Hines.
 A comparison of deep learning mos predictors for speech synthesis
  quality, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ragano2022aquality">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2204.02249">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ragano2022automaticarchives">54</a>]
</dt>
<dd>
A&nbsp;Ragano, E&nbsp;Benetos, and A&nbsp;Hines.
 Automatic quality assessment of digitized and restored sound
  archives.
 <em>Journal of the Audio Engineering Society</em>, 70(4):252-270, Apr
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ragano2022automaticarchives">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17743/jaes.2022.0002">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="reed2022exploringmicrophenomenology">55</a>]
</dt>
<dd>
CN&nbsp;Reed, C&nbsp;Nordmoen, A&nbsp;Martelloni, G&nbsp;Lepri, N&nbsp;Robson, E&nbsp;Zayas-Garin, K&nbsp;Cotton,
  L&nbsp;Mice, and A&nbsp;McPherson.
 Exploring experiences with new musical instruments through
  micro-phenomenology.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#reed2022exploringmicrophenomenology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.b304e4b1">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="reed2022singingperformances">56</a>]
</dt>
<dd>
CN&nbsp;Reed, S&nbsp;Skach, P&nbsp;Strohmeier, and AP&nbsp;McPherson.
 Singing knit: Soft knit biosensing for augmenting vocal performances.
 pages 170-183, Mar 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#reed2022singingperformances">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3519391.3519412">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="robson2022onpractitioners">57</a>]
</dt>
<dd>
N&nbsp;Robson, N&nbsp;Bryan-Kinns, and A&nbsp;Mcpherson.
 On mediating space, sound and experience: interviews with situated
  sound art practitioners.
 <em>Organised Sound: an international journal of music and
  technology</em>, 28(1), Feb 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#robson2022onpractitioners">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1017/S1355771822000103">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="robson2022beingsensors">58</a>]
</dt>
<dd>
N&nbsp;Robson, A&nbsp;McPherson, and N&nbsp;Bryan-Kinns.
 Being with the waves: An ultrasonic art installation enabling rich
  interaction without sensors.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#robson2022beingsensors">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.376bc758">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="sandler2005sigmamodulation">59</a>]
</dt>
<dd>
M&nbsp;SANDLER and J&nbsp;Reiss.
 Sigma delta modulation, Jul 2005.
[&nbsp;<a href="pubs2022_raw_bib.html#sandler2005sigmamodulation">bib</a>&nbsp;| 
<a href="http://www.patents.com/us-7777657.html">.html</a>&nbsp;]

</dd>


<dt>
[<a name="sandler2003methodsignals">60</a>]
</dt>
<dd>
MB&nbsp;SANDLER.
 Method for accurate time position of transients in signals, Mar 2003.
[&nbsp;<a href="pubs2022_raw_bib.html#sandler2003methodsignals">bib</a>&nbsp;]

</dd>


<dt>
[<a name="sandler2006drumloops">61</a>]
</dt>
<dd>
MB&nbsp;SANDLER, JP&nbsp;Bello, and E&nbsp;Ravelli.
 Drum loops, May 2006.
[&nbsp;<a href="pubs2022_raw_bib.html#sandler2006drumloops">bib</a>&nbsp;]

</dd>


<dt>
[<a name="sandler2005sonicvisual">62</a>]
</dt>
<dd>
MB&nbsp;SANDLER, C&nbsp;Landone, and C&nbsp;Cannam.
 Sonic visual, Nov 2005.
[&nbsp;<a href="pubs2022_raw_bib.html#sandler2005sonicvisual">bib</a>&nbsp;]

</dd>


<dt>
[<a name="sandler2005amodulators">63</a>]
</dt>
<dd>
MB&nbsp;Sandler and J&nbsp;REISS.
 A mechanism for the detection and removal of limit cycles in the
  operation of sigma delta modulators, Jul 2005.
 Page bookmark WO 2007010298 (A1) - SIGMA DELTA MODULATORS found as an
  unexamined application.
[&nbsp;<a href="pubs2022_raw_bib.html#sandler2005amodulators">bib</a>&nbsp;]

</dd>


<dt>
[<a name="sarkar2022ensemblesetseparation">64</a>]
</dt>
<dd>
S&nbsp;Sarkar, E&nbsp;Benetos, and M&nbsp;Sandler.
 Ensembleset: A new high-quality synthesised dataset for chamber
  ensemble separation.
 In <em>International Society for Music Information Retrieval</em>.
  Bangalore, Dec 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#sarkar2022ensemblesetseparation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="singh2022hypernetworksproofofconcept">65</a>]
</dt>
<dd>
S&nbsp;Singh, E&nbsp;Benetos, and QH&nbsp;Phan.
 Hypernetworks for sound event detection: a proof-of-concept.
 In <em>30th European Signal Processing Conference (EUSIPCO 2022)</em>,
  pages 429-433. Belgrade, Serbia, EURASIP, Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#singh2022hypernetworksproofofconcept">bib</a>&nbsp;]

</dd>


<dt>
[<a name="soave2022designingreality">66</a>]
</dt>
<dd>
F&nbsp;Soave, N&nbsp;Bryan-Kinns, and I&nbsp;Farkhatdinov.
 Designing audio feedback to enhance motion perception in virtual
  reality.
 volume 13417 LNCS, pages 92-101. Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#soave2022designingreality">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-031-15019-7_9">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="stolleranalysissinging">67</a>]
</dt>
<dd>
D&nbsp;STOLLER and S&nbsp;Dixon.
 Analysis and classification of phonation modes in singing.
 In <em>17th International Society for Music Information Retrieval
  Conference (ISMIR 2016)</em>. New York City, USA.
[&nbsp;<a href="pubs2022_raw_bib.html#stolleranalysissinging">bib</a>&nbsp;]

</dd>


<dt>
[<a name="stollerwaveunetseparation">68</a>]
</dt>
<dd>
D&nbsp;STOLLER, S&nbsp;EWERT, and S&nbsp;DIXON.
 Wave-u-net: A multi-scale neural network for end-to-end audio source
  separation.
 In <em>19th International Society for Music Information Retrieval
  Conference (ISMIR)</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#stollerwaveunetseparation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="stowell2022computationalroadmap">69</a>]
</dt>
<dd>
D&nbsp;Stowell.
 Computational bioacoustics with deep learning: a review and roadmap.
 <em>PeerJ</em>, Mar 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#stowell2022computationalroadmap">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.7717/peerj.13152">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="subramanian2022anomalousmethods">70</a>]
</dt>
<dd>
V&nbsp;Subramanian, S&nbsp;Gururani, E&nbsp;Benetos, and M&nbsp;Sandler.
 Anomalous behaviour in loss-gradient based interpretability methods,
  Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#subramanian2022anomalousmethods">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2207.07769">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="terenzi2022comparisonactivity">71</a>]
</dt>
<dd>
A&nbsp;Terenzi, N&nbsp;Ortolani, I&nbsp;De&nbsp;Almeida&nbsp;Nolasco, E&nbsp;Benetos, and S&nbsp;Cecchi.
 Comparison of feature extraction methods for sound-based
  classification of honey bee activity.
 <em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>,
  30:112-122, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#terenzi2022comparisonactivity">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TASLP.2021.3133194">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2015navigatingplayer">72</a>]
</dt>
<dd>
F&nbsp;Thalmann, A&nbsp;Carrillo, G&nbsp;Fazekas, GA&nbsp;Wiggins, and M&nbsp;Sandler.
 Navigating ontological structures based on feature metadata using the
  semantic music player.
 2015.
 date-added: 2017-12-22 19:18:33 +0000 date-modified: 2017-12-22
  19:59:04 +0000 keywords: ontology, mobile applications, mobile audio
  ontology, web application local-url: http://ismir2015.uma.es/LBD/LBD26.pdf
  publisher-url: http://ismir2015.uma.es/LBD/LBD19.pdf bdsk-url-1:
  https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16154/Thalmann%20Navigating%20Ontological%20Structures%202015%20Published.pdf.
[&nbsp;<a href="pubs2022_raw_bib.html#thalmann2015navigatingplayer">bib</a>&nbsp;| 
<a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16154/Thalmann\%20Navigating\%20Ontological\%20Structures\%202015\%20Published.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2016thedevices">73</a>]
</dt>
<dd>
F&nbsp;Thalmann, G&nbsp;Carrillo, GA&nbsp;Wiggins, and M&nbsp;Sandler.
 The mobile audio ontology: Experiencing dynamic music objects on
  mobile devices.
 pages 47-54, 2016.
 date-added: 2017-12-22 13:07:02 +0000 date-modified: 2017-12-22
  19:55:24 +0000 keywords: music ontologies, artificial intelligence, user
  interfaces, dynamic music objects, mobile audio ontology, mobile sensor data,
  music consumption experiences, semantic audio framework, user interface
  controls, Data mining, Feature extraction bdsk-url-1:
  https://dx.doi.org/10.1109/ICSC.2016.61.
[&nbsp;<a href="pubs2022_raw_bib.html#thalmann2016thedevices">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICSC.2016.61">DOI</a>&nbsp;| 
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=7439304">http</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2016creatingdesigner">74</a>]
</dt>
<dd>
F&nbsp;Thalmann, G&nbsp;Fazekas, GA&nbsp;Wiggins, and M&nbsp;Sandler.
 Creating, visualizing, and analyzing dynamic music objects in the
  browser with the dymo designer.
 pages 39-46, 2016.
 date-added: 2017-12-21 19:31:03 +0000 date-modified: 2017-12-22
  13:13:01 +0000 keywords: music ontology, dynamic music objects, semantic
  audio, intelligent music production, mobile applications local-url:
  https://dl.acm.org/citation.cfm?id=2986445 bdsk-url-1:
  https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16155/Thalmann%20Creating%20Visualizing%20and%20Analyzing%202016%20Submitted.pdf
  bdsk-url-2: https://dx.doi.org/10.1145/2986416.2986445.
[&nbsp;<a href="pubs2022_raw_bib.html#thalmann2016creatingdesigner">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/2986416.2986445">DOI</a>&nbsp;| 
<a href="https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/16155/Thalmann\%20Creating\%20Visualizing\%20and\%20Analyzing\%202016\%20Submitted.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="thalmann2016themetadata">75</a>]
</dt>
<dd>
F&nbsp;Thalmann, A&nbsp;Perez&nbsp;Carillo, G&nbsp;Fazekas, and M&nbsp;Sandler.
 The semantic music player: A smart mobile player based on ontological
  structures and analytical feature metadata.
 2016.
 date-added: 2017-12-22 19:47:03 +0000 date-modified: 2017-12-22
  19:52:36 +0000 keywords: ontolgies, mobile music player, mobile applications,
  mobile audio ontology local-url: thalmann2016wac.pdf publisher-url:
  http://hdl.handle.net/1853/54596 bdsk-url-1:
  https://smartech.gatech.edu/bitstream/handle/1853/54596/WAC2016-71.pdf.
[&nbsp;<a href="pubs2022_raw_bib.html#thalmann2016themetadata">bib</a>&nbsp;| 
<a href="https://smartech.gatech.edu/bitstream/handle/1853/54596/WAC2016-71.pdf">.pdf</a>&nbsp;]

</dd>


<dt>
[<a name="tianmusicfeatures">76</a>]
</dt>
<dd>
M&nbsp;TIAN and MARKB SANDLER.
 Music structural segmentation across genres with gammatone features.
 In <em>ISMIR</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#tianmusicfeatures">bib</a>&nbsp;]

</dd>


<dt>
[<a name="turchet2022theontology">77</a>]
</dt>
<dd>
L&nbsp;Turchet, P&nbsp;Bouquet, A&nbsp;Molinari, and G&nbsp;Fazekas.
 The smart musical instruments ontology.
 <em>Journal of Web Semantics</em>, 72, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#turchet2022theontology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.websem.2021.100687">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="wang2022adaptiverecognition">78</a>]
</dt>
<dd>
C&nbsp;Wang, E&nbsp;Benetos, V&nbsp;Lostanlen, and E&nbsp;Chew.
 Adaptive scattering transforms for playing technique recognition.
 <em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>,
  30:1407-1421, Mar 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#wang2022adaptiverecognition">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TASLP.2022.3156785">DOI</a>&nbsp;| 
<a href="https://changhongw.github.io/">http</a>&nbsp;]

</dd>


<dt>
[<a name="wang2022jointcallrecognition">79</a>]
</dt>
<dd>
C&nbsp;Wang, E&nbsp;Benetos, E&nbsp;Versace, and S&nbsp;Wang.
 Joint scattering for automatic chick call recognition.
 In <em>30th European Signal Processing Conference</em>, pages 195-199.
  Belgrade, Serbia, Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#wang2022jointcallrecognition">bib</a>&nbsp;]

</dd>


<dt>
[<a name="wu2022exploringinterfaces">80</a>]
</dt>
<dd>
Y&nbsp;Wu, N&nbsp;Bryan-Kinns, and J&nbsp;Zhi.
 Exploring visual stimuli as a support for novices’ creative
  engagement with digital musical interfaces.
 <em>Journal on Multimodal User Interfaces</em>, 16(3):343-356, Sep
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#wu2022exploringinterfaces">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s12193-022-00393-3">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zayasgarin2022dialogicexperience">81</a>]
</dt>
<dd>
E&nbsp;Zayas-Garin and A&nbsp;McPherson.
 Dialogic design of accessible digital musical instruments:
  Investigating performer experience.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zayasgarin2022dialogicexperience">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.2b8ce9a4">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhang2022qiaolevr">82</a>]
</dt>
<dd>
J&nbsp;Zhang and N&nbsp;Bryan-Kinns.
 Qiaole: Accessing traditional chinese musical instruments in vr.
 pages 357-362, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zhang2022qiaolevr">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/VRW55335.2022.00080">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhang2022integratingdisciplines">83</a>]
</dt>
<dd>
M&nbsp;Zhang, R&nbsp;Stewart, and N&nbsp;Bryan-Kinns.
 Integrating interactive technology concepts with material expertise
  in textile design disciplines.
 pages 1277-1287, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zhang2022integratingdisciplines">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3532106.3533535">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhao2022violinistdistributions">84</a>]
</dt>
<dd>
Y&nbsp;Zhao, G&nbsp;Fazekas, and M&nbsp;Sandler.
 Violinist identification using note-level timbre feature
  distributions.
 volume 2022-May, pages 601-605, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zhao2022violinistdistributions">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9747606">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zheng2022squeezeprototypes">85</a>]
</dt>
<dd>
J&nbsp;Zheng and N&nbsp;Bryan-Kinns.
 Squeeze, twist, stretch: Exploring deformable digital musical
  interfaces design through non-functional prototypes.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zheng2022squeezeprototypes">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.41da9da5">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zheng2022materialdesign">86</a>]
</dt>
<dd>
J&nbsp;Zheng, N&nbsp;Bryan-Kinns, and AP&nbsp;McPherson.
 Material matters: Exploring materiality in digital musical
  instruments design.
 In <em>Designing Interactive Systems Conference</em>, pages 976-986,
  Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zheng2022materialdesign">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3532106.3533523">DOI</a>&nbsp;]

</dd>
</dl><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.96.</em></p>
