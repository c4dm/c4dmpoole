
<!-- This document was automatically generated with bibtex2html 1.96
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     ./bibtex2html -nodoc -dl -a -noabstract -nokeywords -o pubs2022_raw pubs2022.bib  -->


<dl>
  Cardinale, S and Colton, S

<dt>
  [<a name="cardinale2022neoriemannian">47</a>]
  </dt>
  <dd>
  S&nbsp;Cardinale and S&nbsp;Colton.
    Neo-Riemannian Theory for Generative Film and Videogame Music
    In <em>Proceedings of the International Conference on Computational Creativity</em>, Jun 2022.
  [&nbsp;<a href="pubs2022_raw_bib.html#cardinale2022neoriemannian">bib</a>&nbsp;| 
  
  </dd>


<dt>
[<a name="aziz2022planningoverviews">1</a>]
</dt>
<dd>
N&nbsp;Aziz, T&nbsp;Stockman, and R&nbsp;Stewart.
 Planning your journey in audio: Design and evaluation of auditory
  route overviews.
 <em>ACM Transactions on Accessible Computing</em>, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#aziz2022planningoverviews">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3531529">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="benetos2022measuringdata">2</a>]
</dt>
<dd>
E&nbsp;Benetos, A&nbsp;Ragano, D&nbsp;Sgroi, and A&nbsp;Tuckwell.
 Measuring national mood with music: using machine learning to
  construct a measure of national valence from audio data.
 <em>Behavior Research Methods</em>, Feb 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#benetos2022measuringdata">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.3758/s13428-021-01747-7">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="bryankinns2022qi2heepistemology">3</a>]
</dt>
<dd>
N&nbsp;Bryan-Kinns, W&nbsp;Wang, and T&nbsp;Ji.
 Qi2he: A co-design framework inspired by eastern epistemology.
 <em>International Journal of Human Computer Studies</em>, 160, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#bryankinns2022qi2heepistemology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.ijhcs.2022.102773">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="caspe2022ddx7sounds">4</a>]
</dt>
<dd>
F&nbsp;Caspe, A&nbsp;McPherson, and M&nbsp;Sandler.
 Ddx7: Differentiable fm synthesis of musical instrument sounds, Aug
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#caspe2022ddx7sounds">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2208.06169">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="daikoku2022agreementsample">5</a>]
</dt>
<dd>
H&nbsp;Daikoku, S&nbsp;Ding, E&nbsp;Benetos, ALC Wood, T&nbsp;Shimizono, US&nbsp;Sanne, S&nbsp;Fujii, and
  PE&nbsp;Savage.
 Agreement among human and automated estimates of similarity in a
  global music sample.
 In <em>10th International Workshop on Folk Music Analysis (FMA
  2022)</em>. Sheffield, UK, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#daikoku2022agreementsample">bib</a>&nbsp;]

</dd>


<dt>
[<a name="delgado2022deepclassification">6</a>]
</dt>
<dd>
A&nbsp;Delgado, E&nbsp;Demirel, V&nbsp;Subramanian, C&nbsp;Saitis, and M&nbsp;Sandler.
 Deep embeddings for robust user-based amateur vocal percussion
  classification, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#delgado2022deepclassification">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2204.04646">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="delgado2022deepvocalisation">7</a>]
</dt>
<dd>
A&nbsp;Delgado, C&nbsp;Saitis, E&nbsp;Benetos, and M&nbsp;Sandler.
 Deep conditional representation learning for drum sample retrieval by
  vocalisation, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#delgado2022deepvocalisation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2204.04651">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="fordspeculatingai">8</a>]
</dt>
<dd>
C&nbsp;Ford and N&nbsp;Bryan-Kinns.
 Speculating on reflection and people’s music co-creation with ai.
 In <em>Generative AI and HCI Workshop at CHI 2022</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#fordspeculatingai">bib</a>&nbsp;]

</dd>


<dt>
[<a name="ford2022identifyinghome">9</a>]
</dt>
<dd>
C&nbsp;Ford and N&nbsp;Bryan-Kinns.
 Identifying engagement in children's interaction whilst composing
  digital music at home.
 In <em>ACM Conference on Creativity &amp; Cognition</em>, pages 443-456,
  New York, NY, USA, Jun 2022. Venice, Italy, ACM Digital Library.
[&nbsp;<a href="pubs2022_raw_bib.html#ford2022identifyinghome">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3527927.3532794">DOI</a>&nbsp;| 
<a href="https://dl.acm.org/doi/10.1145/3527927.3532794">http</a>&nbsp;]

</dd>


<dt>
[<a name="frachi2022designbiosignals">10</a>]
</dt>
<dd>
Y&nbsp;Frachi, T&nbsp;Takahashi, F&nbsp;Wang, and M&nbsp;Barthet.
 Design of emotion-driven game interaction using biosignals.
 volume 13334 LNCS, pages 160-179. Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#frachi2022designbiosignals">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-031-05637-6_10">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="guidi2022quantitativeinstruments">11</a>]
</dt>
<dd>
A&nbsp;Guidi and A&nbsp;McPherson.
 Quantitative evaluation of aspects of embodiment in new digital
  musical instruments.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#guidi2022quantitativeinstruments">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.79d0b38f">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hayes2022disembodiedsynthesis1">12</a>]
</dt>
<dd>
B&nbsp;Hayes, C&nbsp;Saitis, and G&nbsp;Fazekas.
 Disembodied timbres: a study on semantically prompted fm synthesis,
  Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#hayes2022disembodiedsynthesis1">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.31234/osf.io/ksw5j">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hayes2022disembodiedsynthesis2">13</a>]
</dt>
<dd>
B&nbsp;Hayes, C&nbsp;Saitis, and G&nbsp;Fazekas.
 Disembodied timbres: A study on semantically prompted fm synthesis.
 <em>Journal of the Audio Engineering Society</em>, 70(5):373-391, May
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#hayes2022disembodiedsynthesis2">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17743/jaes.2022.0006">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="hayes2022disembodiedsynthesis3">14</a>]
</dt>
<dd>
B&nbsp;Hayes, C&nbsp;Saitis, and G&nbsp;Fazekas.
 Disembodied timbres: A study on semantically prompted fm synthesis.
 <em>Journal of the Audio Engineering Society</em>, 70(5):373-391, May
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#hayes2022disembodiedsynthesis3">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17743/jaes.2022.0006">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="heremans2022featureeeg">15</a>]
</dt>
<dd>
ERM Heremans, H&nbsp;Phan, AH&nbsp;Ansari, P&nbsp;Borzée, B&nbsp;Buyse, D&nbsp;Testelmans, and
  M&nbsp;De&nbsp;Vos.
 Feature matching as improved transfer learning technique for wearable
  eeg.
 <em>Biomedical Signal Processing and Control</em>, 78:104009-104009,
  Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#heremans2022featureeeg">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.bspc.2022.104009">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="heremansfromstaging">16</a>]
</dt>
<dd>
ERM Heremans, H&nbsp;Phan, P&nbsp;Borzée, B&nbsp;Buyse, D&nbsp;Testelmans, and M&nbsp;De&nbsp;Vos.
 From unsupervised to semi-supervised adversarial domain adaptation in
  eeg-based sleep staging.
 <em>Journal of Neural Engineering</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#heremansfromstaging">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1088/1741-2552/ac6ca8">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/35508121">http</a>&nbsp;]

</dd>


<dt>
[<a name="huang2022improvingdetection">17</a>]
</dt>
<dd>
J&nbsp;Huang, E&nbsp;Benetos, and S&nbsp;Ewert.
 Improving lyrics alignment through joint pitch detection.
 In <em>2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing</em>, pages 451-455. Singapore, IEEE, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#huang2022improvingdetection">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746460">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="lepri2022thespeculation">18</a>]
</dt>
<dd>
G&nbsp;Lepri, J&nbsp;Bowers, S&nbsp;Topley, P&nbsp;Stapleton, P&nbsp;Bennett, K&nbsp;Andersen, and
  A&nbsp;McPherson.
 The 10,000 instruments workshop - (im)practical research for critical
  speculation.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#lepri2022thespeculation">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.9e7c9ba3">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="lepri2022uselesspractice">19</a>]
</dt>
<dd>
G&nbsp;Lepri, A&nbsp;Mcpherson, and J&nbsp;Bowers.
 Useless, not worthless: Absurd making as critical practice.
 In <em>ACM conference on Designing Interactive Systems</em>, Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#lepri2022uselesspractice">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/10.1145/3357236.3395547">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="lifewshotnetworks">20</a>]
</dt>
<dd>
R&nbsp;Li, J&nbsp;Liang, and QH&nbsp;Phan.
 Few-shot bioacoustic event detection: Enhanced classifiers for
  prototypical networks.
 In <em>7th Workshop on Detection and Classification of Acoustic
  Scenes and Events (DCASE)</em>. Nancy, France.
[&nbsp;<a href="pubs2022_raw_bib.html#lifewshotnetworks">bib</a>&nbsp;| 
<a href="https://dcase.community/workshop2022/">http</a>&nbsp;]

</dd>


<dt>
[<a name="liang2022leveragingrecognition">21</a>]
</dt>
<dd>
J&nbsp;Liang, QH&nbsp;Phan, and E&nbsp;Benetos.
 Leveraging label hierarchies for few-shot everyday sound recognition.
 In <em>7th Workshop on Detection and Classification of Acoustic
  Scenes and Events (DCASE)</em>. Nancy, France, Nov 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#liang2022leveragingrecognition">bib</a>&nbsp;| 
<a href="https://jinhualiang.github.io/">http</a>&nbsp;]

</dd>


<dt>
[<a name="linhart2022themammals">22</a>]
</dt>
<dd>
P&nbsp;Linhart, M&nbsp;Mahamoud-Issa, D&nbsp;Stowell, and DT&nbsp;Blumstein.
 The potential for acoustic individual identification in mammals.
 <em>Mammalian Biology</em>, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#linhart2022themammals">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s42991-021-00222-2">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="liuperformancetracking">23</a>]
</dt>
<dd>
L&nbsp;Liu, Q&nbsp;KONG, G-V Morfi, and E&nbsp;Benetos.
 Performance midi-to-score conversion by neural beat tracking.
 In <em>Proceedings of the 23rd International Society for Music
  Information Retrieval Conference</em>. Bengaluru, India.
[&nbsp;<a href="pubs2022_raw_bib.html#liuperformancetracking">bib</a>&nbsp;| 
<a href="https://cheriell.github.io/">http</a>&nbsp;]

</dd>


<dt>
[<a name="luo2022towardsaudio">24</a>]
</dt>
<dd>
Y-J Luo, S&nbsp;Ewert, and S&nbsp;Dixon.
 Towards robust unsupervised disentanglement of sequential data — a
  case study using music audio.
 In <em>Proceedings of the Thirty-First International Joint
  Conference on Artificial Intelligence</em>, pages 3299-3305, Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#luo2022towardsaudio">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.24963/ijcai.2022/458">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="manco2022learningsupervision">25</a>]
</dt>
<dd>
I&nbsp;Manco, E&nbsp;Benetos, E&nbsp;Quinton, and G&nbsp;Fazekas.
 Learning music audio representations via weak language supervision.
 In <em>2022 IEEE International Conference on Acoustics, Speech and
  Signal Processing</em>, pages 456-460. Singapore, IEEE, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#manco2022learningsupervision">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746996">DOI</a>&nbsp;| 
<a href="https://ilariamanco.com/">http</a>&nbsp;]

</dd>


<dt>
[<a name="mice2022theperformances">26</a>]
</dt>
<dd>
L&nbsp;Mice and A&nbsp;Mcpherson.
 The m in nime: Motivic analysis and the case for a musicology of nime
  performances.
 In <em>International Conference on New Interfaces for Musical
  Expression</em>, Auckland, New Zealand, Jun 2022. Auckland, New Zealand,
  International Conference on New Interfaces for Musical Expression.
[&nbsp;<a href="pubs2022_raw_bib.html#mice2022theperformances">bib</a>&nbsp;| 
<a href="https://www.nime.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="mice2022superdesign">27</a>]
</dt>
<dd>
L&nbsp;Mice and AP&nbsp;McPherson.
 Super size me: Interface size, identity and embodiment in digital
  musical instrument design.
 Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#mice2022superdesign">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3491102.3517626">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="mikkelsen2022sleepconfigurations">28</a>]
</dt>
<dd>
KB&nbsp;Mikkelsen, H&nbsp;Phan, ML&nbsp;Rank, MC&nbsp;Hemmsen, M&nbsp;de&nbsp;Vos, and P&nbsp;Kidmose.
 Sleep monitoring using ear-centered setups: Investigating the
  influence from electrode configurations.
 <em>IEEE Trans Biomed Eng</em>, 69(5):1564-1572, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#mikkelsen2022sleepconfigurations">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TBME.2021.3116274">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/34587000">http</a>&nbsp;]

</dd>


<dt>
[<a name="nguyen2022salsalitearrays">29</a>]
</dt>
<dd>
TNT Nguyen, DL&nbsp;Jones, KN&nbsp;Watcharasupat, H&nbsp;Phan, and WS&nbsp;Gan.
 Salsa-lite: A fast and effective feature for polyphonic sound event
  localization and detection with microphone arrays.
 volume 2022-May, pages 716-720, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#nguyen2022salsalitearrays">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746132">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="nolasco2022rankbasedrepresentations">30</a>]
</dt>
<dd>
I&nbsp;Nolasco and D&nbsp;Stowell.
 Rank-based loss for learning hierarchical representations.
 volume 2022-May, pages 3623-3627, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#nolasco2022rankbasedrepresentations">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746907">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="nordmoen2022makingsystem">31</a>]
</dt>
<dd>
C&nbsp;Nordmoen and AP&nbsp;McPherson.
 Making space for material entanglements: A diffractive analysis of
  woodwork and the practice of making an interactive system.
 pages 415-423, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#nordmoen2022makingsystem">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3532106.3533572">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ou2022exploringtranscription">32</a>]
</dt>
<dd>
L&nbsp;Ou, Z&nbsp;Guo, E&nbsp;Benetos, J&nbsp;Han, and Y&nbsp;Wang.
 Exploring transformer's potential on automatic piano transcription.
 In <em>IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP)</em>, pages 776-780. Singapore, IEEE, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ou2022exploringtranscription">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746789">DOI</a>&nbsp;| 
<a href="https://2022.ieeeicassp.org/">http</a>&nbsp;]

</dd>


<dt>
[<a name="ozaki2022similaritiesrecordings">33</a>]
</dt>
<dd>
Y&nbsp;Ozaki, J&nbsp;Kuroyanagi, J&nbsp;McBride, P&nbsp;Proutskova, A&nbsp;Tierney, P&nbsp;Pfordresher,
  E&nbsp;Benetos, F&nbsp;Liu, and PE&nbsp;Savage.
 Similarities and differences in a cross-linguistic sample of song and
  speech recordings.
 In <em>Joint Conference on Language Evolution</em>. Kanazawa, Japan, Sep
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ozaki2022similaritiesrecordings">bib</a>&nbsp;| 
<a href="https://sites.google.com/view/joint-conf-language-evolution/home">http</a>&nbsp;]

</dd>


<dt>
[<a name="pelinski2022embeddedopportunities">34</a>]
</dt>
<dd>
T&nbsp;Pelinski, V&nbsp;Shepardson, S&nbsp;Symons, FS&nbsp;Caspe, AL&nbsp;Benito&nbsp;Temprano, J&nbsp;Armitage,
  C&nbsp;Kiefer, R&nbsp;Fiebrink, T&nbsp;Magnusson, and A&nbsp;McPherson.
 Embedded ai for nime: Challenges and opportunities.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#pelinski2022embeddedopportunities">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.76beab02">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="phan2022xsleepnetstaging">35</a>]
</dt>
<dd>
H&nbsp;Phan, OY&nbsp;Chen, MC&nbsp;Tran, P&nbsp;Koch, A&nbsp;Mertins, and M&nbsp;De&nbsp;Vos.
 Xsleepnet: Multi-view sequential model for automatic sleep staging.
 <em>IEEE Trans Pattern Anal Mach Intell</em>, 44(9):5903-5915, Sep
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#phan2022xsleepnetstaging">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TPAMI.2021.3070057">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/33788679">http</a>&nbsp;]

</dd>


<dt>
[<a name="phan2022pediatricmethods">36</a>]
</dt>
<dd>
H&nbsp;Phan, A&nbsp;Mertins, and M&nbsp;Baumert.
 Pediatric automatic sleep staging: A comparative study of
  state-of-the-art deep learning methods.
 <em>IEEE Trans Biomed Eng</em>, PP, May 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#phan2022pediatricmethods">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TBME.2022.3174680">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/35552153">http</a>&nbsp;]

</dd>


<dt>
[<a name="phan2022automaticdirections">37</a>]
</dt>
<dd>
H&nbsp;Phan and K&nbsp;Mikkelsen.
 Automatic sleep staging of eeg signals: recent development,
  challenges, and future directions.
 <em>Physiol Meas</em>, 43(4), Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#phan2022automaticdirections">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1088/1361-6579/ac6049">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/35320788">http</a>&nbsp;]

</dd>


<dt>
[<a name="phan2022sleeptransformerquantification">38</a>]
</dt>
<dd>
H&nbsp;Phan, K&nbsp;Mikkelsen, OY&nbsp;Chen, P&nbsp;Koch, A&nbsp;Mertins, and M&nbsp;De&nbsp;Vos.
 Sleeptransformer: Automatic sleep staging with interpretability and
  uncertainty quantification.
 <em>IEEE Trans Biomed Eng</em>, 69(8):2456-2467, Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#phan2022sleeptransformerquantification">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TBME.2022.3147187">DOI</a>&nbsp;| 
<a href="https://www.ncbi.nlm.nih.gov/pubmed/35100107">http</a>&nbsp;]

</dd>


<dt>
[<a name="phan2022polyphonicproblem">39</a>]
</dt>
<dd>
H&nbsp;Phan, TNT Nguyen, P&nbsp;Koch, and A&nbsp;Mertins.
 Polyphonic audio event detection: Multi-label or multi-class
  multi-task classification problem?
 volume 2022-May, pages 8877-8881, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#phan2022polyphonicproblem">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9746402">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="preniqi2022morelyrics">40</a>]
</dt>
<dd>
V&nbsp;Preniqi, K&nbsp;Kalimeri, and C&nbsp;Saitis.
 "more than words": Linking music preferences and moral values through
  lyrics, Sep 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#preniqi2022morelyrics">bib</a>&nbsp;]

</dd>


<dt>
[<a name="proutskova2022thejazz">41</a>]
</dt>
<dd>
P&nbsp;Proutskova, D&nbsp;Wolff, G&nbsp;Fazekas, K&nbsp;Frieler, F&nbsp;Höger, O&nbsp;Velichkina, G&nbsp;Solis,
  T&nbsp;Weyde, M&nbsp;Pfleiderer, HC&nbsp;Crayencour, G&nbsp;Peeters, and S&nbsp;Dixon.
 The jazz ontology: A semantic model and large-scale rdf repositories
  for jazz.
 <em>Journal of Web Semantics</em>, pages 100735-100735, Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#proutskova2022thejazz">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.websem.2022.100735">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ragano2022aquality">42</a>]
</dt>
<dd>
A&nbsp;Ragano, E&nbsp;Benetos, M&nbsp;Chinen, HB&nbsp;Martinez, CKA Reddy, J&nbsp;Skoglund, and A&nbsp;Hines.
 A comparison of deep learning mos predictors for speech synthesis
  quality, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ragano2022aquality">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2204.02249">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="ragano2022automaticarchives">43</a>]
</dt>
<dd>
A&nbsp;Ragano, E&nbsp;Benetos, and A&nbsp;Hines.
 Automatic quality assessment of digitized and restored sound
  archives.
 <em>Journal of the Audio Engineering Society</em>, 70(4):252-270, Apr
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#ragano2022automaticarchives">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.17743/jaes.2022.0002">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="reed2022exploringmicrophenomenology">44</a>]
</dt>
<dd>
CN&nbsp;Reed, C&nbsp;Nordmoen, A&nbsp;Martelloni, G&nbsp;Lepri, N&nbsp;Robson, E&nbsp;Zayas-Garin, K&nbsp;Cotton,
  L&nbsp;Mice, and A&nbsp;McPherson.
 Exploring experiences with new musical instruments through
  micro-phenomenology.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#reed2022exploringmicrophenomenology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.b304e4b1">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="reed2022singingperformances">45</a>]
</dt>
<dd>
CN&nbsp;Reed, S&nbsp;Skach, P&nbsp;Strohmeier, and AP&nbsp;McPherson.
 Singing knit: Soft knit biosensing for augmenting vocal performances.
 pages 170-183, Mar 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#reed2022singingperformances">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3519391.3519412">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="robson2022onpractitioners">46</a>]
</dt>
<dd>
N&nbsp;Robson, N&nbsp;Bryan-Kinns, and A&nbsp;Mcpherson.
 On mediating space, sound and experience: interviews with situated
  sound art practitioners.
 <em>Organised Sound: an international journal of music and
  technology</em>, 28(1), Feb 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#robson2022onpractitioners">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1017/S1355771822000103">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="robson2022beingsensors">47</a>]
</dt>
<dd>
N&nbsp;Robson, A&nbsp;McPherson, and N&nbsp;Bryan-Kinns.
 Being with the waves: An ultrasonic art installation enabling rich
  interaction without sensors.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#robson2022beingsensors">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.376bc758">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="sarkar2022ensemblesetseparation">48</a>]
</dt>
<dd>
S&nbsp;Sarkar, E&nbsp;Benetos, and M&nbsp;Sandler.
 Ensembleset: A new high-quality synthesised dataset for chamber
  ensemble separation.
 In <em>International Society for Music Information Retrieval</em>.
  Bangalore, Dec 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#sarkar2022ensemblesetseparation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="singh2022hypernetworksproofofconcept">49</a>]
</dt>
<dd>
S&nbsp;Singh, E&nbsp;Benetos, and QH&nbsp;Phan.
 Hypernetworks for sound event detection: a proof-of-concept.
 In <em>30th European Signal Processing Conference (EUSIPCO 2022)</em>,
  pages 429-433. Belgrade, Serbia, EURASIP, Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#singh2022hypernetworksproofofconcept">bib</a>&nbsp;]

</dd>


<dt>
[<a name="soave2022designingreality">50</a>]
</dt>
<dd>
F&nbsp;Soave, N&nbsp;Bryan-Kinns, and I&nbsp;Farkhatdinov.
 Designing audio feedback to enhance motion perception in virtual
  reality.
 volume 13417 LNCS, pages 92-101. Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#soave2022designingreality">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/978-3-031-15019-7_9">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="stollerwaveunetseparation">51</a>]
</dt>
<dd>
D&nbsp;STOLLER, S&nbsp;EWERT, and S&nbsp;DIXON.
 Wave-u-net: A multi-scale neural network for end-to-end audio source
  separation.
 In <em>19th International Society for Music Information Retrieval
  Conference (ISMIR)</em>.
[&nbsp;<a href="pubs2022_raw_bib.html#stollerwaveunetseparation">bib</a>&nbsp;]

</dd>


<dt>
[<a name="stowell2022computationalroadmap">52</a>]
</dt>
<dd>
D&nbsp;Stowell.
 Computational bioacoustics with deep learning: a review and roadmap.
 <em>PeerJ</em>, Mar 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#stowell2022computationalroadmap">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.7717/peerj.13152">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="subramanian2022anomalousmethods">53</a>]
</dt>
<dd>
V&nbsp;Subramanian, S&nbsp;Gururani, E&nbsp;Benetos, and M&nbsp;Sandler.
 Anomalous behaviour in loss-gradient based interpretability methods,
  Jul 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#subramanian2022anomalousmethods">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.48550/arxiv.2207.07769">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="terenzi2022comparisonactivity">54</a>]
</dt>
<dd>
A&nbsp;Terenzi, N&nbsp;Ortolani, I&nbsp;De&nbsp;Almeida&nbsp;Nolasco, E&nbsp;Benetos, and S&nbsp;Cecchi.
 Comparison of feature extraction methods for sound-based
  classification of honey bee activity.
 <em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>,
  30:112-122, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#terenzi2022comparisonactivity">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TASLP.2021.3133194">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="turchet2022theontology">55</a>]
</dt>
<dd>
L&nbsp;Turchet, P&nbsp;Bouquet, A&nbsp;Molinari, and G&nbsp;Fazekas.
 The smart musical instruments ontology.
 <em>Journal of Web Semantics</em>, 72, Apr 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#turchet2022theontology">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.websem.2021.100687">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="wang2022adaptiverecognition">56</a>]
</dt>
<dd>
C&nbsp;Wang, E&nbsp;Benetos, V&nbsp;Lostanlen, and E&nbsp;Chew.
 Adaptive scattering transforms for playing technique recognition.
 <em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>,
  30:1407-1421, Mar 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#wang2022adaptiverecognition">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TASLP.2022.3156785">DOI</a>&nbsp;| 
<a href="https://changhongw.github.io/">http</a>&nbsp;]

</dd>


<dt>
[<a name="wang2022jointcallrecognition">57</a>]
</dt>
<dd>
C&nbsp;Wang, E&nbsp;Benetos, E&nbsp;Versace, and S&nbsp;Wang.
 Joint scattering for automatic chick call recognition.
 In <em>30th European Signal Processing Conference</em>, pages 195-199.
  Belgrade, Serbia, Aug 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#wang2022jointcallrecognition">bib</a>&nbsp;]

</dd>


<dt>
[<a name="wu2022exploringinterfaces">58</a>]
</dt>
<dd>
Y&nbsp;Wu, N&nbsp;Bryan-Kinns, and J&nbsp;Zhi.
 Exploring visual stimuli as a support for novices’ creative
  engagement with digital musical interfaces.
 <em>Journal on Multimodal User Interfaces</em>, 16(3):343-356, Sep
  2022.
[&nbsp;<a href="pubs2022_raw_bib.html#wu2022exploringinterfaces">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s12193-022-00393-3">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zayasgarin2022dialogicexperience">59</a>]
</dt>
<dd>
E&nbsp;Zayas-Garin and A&nbsp;McPherson.
 Dialogic design of accessible digital musical instruments:
  Investigating performer experience.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zayasgarin2022dialogicexperience">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.2b8ce9a4">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhang2022qiaolevr">60</a>]
</dt>
<dd>
J&nbsp;Zhang and N&nbsp;Bryan-Kinns.
 Qiaole: Accessing traditional chinese musical instruments in vr.
 pages 357-362, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zhang2022qiaolevr">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/VRW55335.2022.00080">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhang2022integratingdisciplines">61</a>]
</dt>
<dd>
M&nbsp;Zhang, R&nbsp;Stewart, and N&nbsp;Bryan-Kinns.
 Integrating interactive technology concepts with material expertise
  in textile design disciplines.
 pages 1277-1287, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zhang2022integratingdisciplines">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3532106.3533535">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zhao2022violinistdistributions">62</a>]
</dt>
<dd>
Y&nbsp;Zhao, G&nbsp;Fazekas, and M&nbsp;Sandler.
 Violinist identification using note-level timbre feature
  distributions.
 volume 2022-May, pages 601-605, Jan 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zhao2022violinistdistributions">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/ICASSP43922.2022.9747606">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zheng2022squeezeprototypes">63</a>]
</dt>
<dd>
J&nbsp;Zheng and N&nbsp;Bryan-Kinns.
 Squeeze, twist, stretch: Exploring deformable digital musical
  interfaces design through non-functional prototypes.
 In <em>NIME 2022</em>, Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zheng2022squeezeprototypes">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.21428/92fbeb44.41da9da5">DOI</a>&nbsp;]

</dd>


<dt>
[<a name="zheng2022materialdesign">64</a>]
</dt>
<dd>
J&nbsp;Zheng, N&nbsp;Bryan-Kinns, and AP&nbsp;McPherson.
 Material matters: Exploring materiality in digital musical
  instruments design.
 In <em>Designing Interactive Systems Conference</em>, pages 976-986,
  Jun 2022.
[&nbsp;<a href="pubs2022_raw_bib.html#zheng2022materialdesign">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1145/3532106.3533523">DOI</a>&nbsp;]

</dd>
</dl><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.96.</em></p>
